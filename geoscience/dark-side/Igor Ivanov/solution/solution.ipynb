{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6a57e32",
   "metadata": {},
   "source": [
    "## Dark side of the volume\n",
    "*https://thinkonward.com/app/c/challenges/dark-side*  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820331d7",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Approach\n",
    "2. Hardware requirements and Time\n",
    "3. Software requirements and Installation\n",
    "4. Inference\n",
    "5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce163eb3",
   "metadata": {},
   "source": [
    "## 1. Approach\n",
    "\n",
    "#### 1.1 Summary\n",
    "For this challenge I trained an ensemble of 5 2D Unet models with large EfficientNet encoders (two B8, L2, L, XL) using [segmentation_model_pytorch](https://github.com/qubvel/segmentation_models.pytorch) framework. As training data I used volume slices over axis 0 and axis 1 (i.e. 1-channel images of size 300 x 1259). I used `smp.losses.DiceLoss()` and Adam optimizer. It took from 10 to 20 epochs to converge depending on the model. I used the `ReduceLROnPlateau` schedule and mixed precision training. I created a 10-fold split of data and trained only a single first fold for each model. Folds were created based on `GroupKFold` on the volume level i.e. validation is performed on completely unseen volumes. As a final ensemble I averaged predictions from 5 models, 2 axes, and 2 image orientations (original and up-down flip).\n",
    "\n",
    "#### 1.2 Details\n",
    "Given that images (slices) in our task have large resolution I was interested in researching the effect of encoder size. It turned out that scaling law worked very well in this case and all models from EfficientNet-B0 up to EfficientNet-B8 gave consistently better scores. Eventually B8 gave the best single model score of 0.9167. I trained EfficientNet-L2 which stands out for its size (0.5B) but the score of 0.9098 was not better than B8. Possibly more careful tuning of hyperparameters could push the score higher.  \n",
    "\n",
    "All models except one were trained on horizontally oriented images and one was trained on vertically oriented images. Model trained on vertical images gave comparable score. It is worth mentioning the same architecture (B8) trained on vertical images took less vRAM and trained faster compared to horizontal images.  \n",
    "\n",
    "I tried `smp.losses.DiceLoss()` and `smp.losses.JaccardLoss()` both giving almost the same scores for the same architecture. All final models were trained using Dice loss in multiclass setting i.e. `smp.losses.DiceLoss(mode='multiclass')` treating a binary task as a multiclass task with 2 classes. \n",
    "I compared two approaches regarding image selection. As a first one I trained on images where the mask has at least one positive pixel (value 1). As the second one I trained on all images including those where the mask is empty (all zeros). Second approach was significantly better.\n",
    "\n",
    "In the table below we can see Dice loss values and LB scores for each model.\n",
    "\n",
    "\n",
    "| Fold | Encoder ID                               | Dice loss | Leaderboard | Image orientation |\n",
    "|------|------------------------------------------|-----------|-------------|-------------------|\n",
    "| 0    | `tu-tf_efficientnet_l2.ns_jft_in1k`      | 0.0633    | 0.9098      | hor               |\n",
    "| 1    | `timm-efficientnet-b8`                   | 0.0606    | 0.9167      | hor               |\n",
    "| 2    | `tu-tf_efficientnet_b8.ap_in1k`          | 0.0587    | 0.9150      | ver               |\n",
    "| 3    | `tu-tf_efficientnetv2_l.in1k`            | 0.0621    | 0.9155      | hor               |\n",
    "| 4    | `tu-tf_efficientnetv2_xl.in21k_ft_in1k`  | 0.0576    | 0.9074      | hor               |\n",
    "|      |                                          |           |             |                   |\n",
    "|      |  `ensemble`                              |           | 0.9247      |                   |\n",
    "\n",
    "\n",
    "**Directory structure:**\n",
    "```\n",
    "solution\n",
    "|\n",
    "|-- models\n",
    "|\n",
    "|-- test\n",
    "|   |-- 2023-10-05_01b243eb\n",
    "|   |-- 2023-10-05_023d576f\n",
    "|   |-- ...\n",
    "|\n",
    "|-- train\n",
    "|   |-- 2023-10-05_0283ecc5\n",
    "|   |-- 2023-10-05_03b796af\n",
    "|   |-- ...\n",
    "|\n",
    "|-- data.py\n",
    "|-- infer.py\n",
    "|-- LICENSE.txt\n",
    "|-- requirements.txt\n",
    "|-- solution.ipynb\n",
    "|-- train.py\n",
    "|-- utils.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca8f6d8",
   "metadata": {},
   "source": [
    "## 2. Hardware requirements and Time\n",
    "\n",
    "Hardware:\n",
    "\n",
    "* 12x CPU\n",
    "* 32 GB RAM\n",
    "* 1x RTX-3090-24GB GPU\n",
    "* 500 GB SSD\n",
    "\n",
    "Time:\n",
    "\n",
    "* Training data creation: **4 hours**  \n",
    "* Training time:          **240 hours**  \n",
    "* Test data creation: **0.5 hour**\n",
    "* Inference time:     **3 hours**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aedae91",
   "metadata": {},
   "source": [
    "## 3. Software requirements and Installation\n",
    "\n",
    "* Ubuntu 22.04\n",
    "* Python: 3.10.12 (Conda)\n",
    "* CUDA 12.4\n",
    "\n",
    "**Dataset setup**  \n",
    "\n",
    "Solution package has empty `train` and `test` dirs. Please extract train and/or test data in corresponding locations so that we have default dir structure outlined in the section 1 above. Also you can just point the `--input_dir` parameter of the scripts to any data location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee5f6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd ~/solution\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4e25b0",
   "metadata": {},
   "source": [
    "## 4. Inference\n",
    "\n",
    "#### 4.1 Create test data\n",
    "\n",
    "Please replace `--input_dir` value with the path to the directory containing holdout volumes.  \n",
    "Same structure is expected i.e. each `.npy` file resides inside its own subdirectory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91334c25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python data.py \\\n",
    "--input_dir=test \\\n",
    "--output_dir_axis_0=test_img_axis_0 \\\n",
    "--output_dir_axis_1=test_img_axis_1 \\\n",
    "--has_label=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc45225",
   "metadata": {},
   "source": [
    "#### 4.2 Run inference \n",
    "\n",
    "Input dirs in the following command are the output dirs created with `data.py` script on the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b6413",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python infer.py \\\n",
    "--input_dir_axis_0=test_img_axis_0 \\\n",
    "--input_dir_axis_1=test_img_axis_1 \\\n",
    "--batch_size=16 \\\n",
    "--model_dir=models \\\n",
    "--submission_path=submission.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d83216c",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "#### 5.1 Create training data\n",
    "***Notes.*** \n",
    "1. If the mask has no positive pixels we don't save it as a `.png` file. Instead we will create it during training on the fly.\n",
    "2. I excluded 8 volumes which have completely empty masks from training data. They are listed at the bottom of `data.py`. It was just the initial heuristic which went to the final solution. These volumes could be used for training, although they most probably wouldn't have a significant effect.\n",
    "3. 12 volumes which have `2024` in their names need a transposition `np.transpose(volume, [1, 0, 2])` to match their corresponding masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f390a250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python data.py \\\n",
    "--input_dir=train \\\n",
    "--output_dir_axis_0=train_img_axis_0 \\\n",
    "--output_dir_axis_1=train_img_axis_1 \\\n",
    "--has_label=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23044173",
   "metadata": {},
   "source": [
    "#### 5.2 Run training\n",
    "\n",
    "***Notes.***\n",
    "\n",
    "1. All models were trained on images from both axes. If you are interested in an axis-specific model, just set\n",
    "   `--input_dir=train_img_axis_0` or `--input_dir=train_img_axis_1`.\n",
    "\n",
    "2. Training script saves `model-*.bin` file after each complete epoch and `ckpt-*.bin` file each hour. After training is complete there will be a single file called `model-*.bin` which is the final model. Each checkpoint contains all states (model, optimizer, etc.) so it is possible to continue training if there was an interruption. Please set the corresponding fold index (specified in checkpoint filename) and the last available checkpoint file. For example, to continue training we need to set: `--initial_fold=0` and `--ckpt=checkpoints/model-f0-e005-0.0775.bin`. All other parameters remain the same.\n",
    "\n",
    "3. Inference script uses model files containing weights only. So to export model after training we need to run a short snippet (set `file` variable with actual final model name):\n",
    "```\n",
    "import torch\n",
    "file = 'model.bin'\n",
    "torch.save(\n",
    "    torch.load(file, map_location=torch.device('cpu'))['state_dict'], \n",
    "    file.replace('.bin', '-ready.bin'))\n",
    "```\n",
    "\n",
    "4. To avoid saving intermediate checkpoints by time just set the argument `--save_seconds` with some large number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaac0e2",
   "metadata": {},
   "source": [
    "#### EfficientNet-L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1e59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py \\\n",
    "--input_dir=train_img_axis_* \\\n",
    "--output_dir=checkpoints_1 \\\n",
    "--encoder_name=tu-tf_efficientnet_l2.ns_jft_in1k \\\n",
    "--n_epochs=7 \\\n",
    "--batch_size=2 \\\n",
    "--accum=20 \\\n",
    "--lr=1e-4 \\\n",
    "--vertical=0 \\\n",
    "--aug=1 \\\n",
    "--save_seconds=3600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dc8a8d",
   "metadata": {},
   "source": [
    "#### EfficientNet-B8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7cee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py \\\n",
    "--input_dir=train_img_axis_* \\\n",
    "--output_dir=checkpoints_2 \\\n",
    "--encoder_name=timm-efficientnet-b8 \\\n",
    "--n_epochs=16 \\\n",
    "--batch_size=5 \\\n",
    "--accum=8 \\\n",
    "--lr=1e-3 \\\n",
    "--vertical=0 \\\n",
    "--aug=2 \\\n",
    "--save_seconds=3600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c7b41d",
   "metadata": {},
   "source": [
    "#### EfficientNet-B8 (vertical images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef02bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py \\\n",
    "--input_dir=train_img_axis_* \\\n",
    "--output_dir=checkpoints_3 \\\n",
    "--encoder_name=tu-tf_efficientnet_b8.ap_in1k \\\n",
    "--n_epochs=20 \\\n",
    "--batch_size=7 \\\n",
    "--accum=6 \\\n",
    "--lr=1e-3 \\\n",
    "--vertical=1 \\\n",
    "--aug=2 \\\n",
    "--save_seconds=3600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e72da01",
   "metadata": {},
   "source": [
    "#### EfficientNet-v2-L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd8b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py \\\n",
    "--input_dir=train_img_axis_* \\\n",
    "--output_dir=checkpoints_4 \\\n",
    "--encoder_name=tu-tf_efficientnetv2_l.in1k \\\n",
    "--n_epochs=9 \\\n",
    "--batch_size=10 \\\n",
    "--accum=4 \\\n",
    "--lr=1e-3 \\\n",
    "--vertical=0 \\\n",
    "--aug=2 \\\n",
    "--save_seconds=3600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a5f099",
   "metadata": {},
   "source": [
    "#### EfficientNet-v2-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcfbea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py \\\n",
    "--input_dir=train_img_axis_* \\\n",
    "--output_dir=checkpoints_5 \\\n",
    "--encoder_name=tu-tf_efficientnetv2_xl.in21k_ft_in1k \\\n",
    "--n_epochs=11 \\\n",
    "--batch_size=6 \\\n",
    "--accum=8 \\\n",
    "--lr=1e-3 \\\n",
    "--vertical=0 \\\n",
    "--aug=2 \\\n",
    "--save_seconds=3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dcea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
