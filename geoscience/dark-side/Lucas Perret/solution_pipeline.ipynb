{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59cc0658",
   "metadata": {},
   "source": [
    "# Fault Segmentation Prediction Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f098ef3",
   "metadata": {},
   "source": [
    "# General Explanation\n",
    "\n",
    "![UNet++ Architecture](image/unet_plus_plus_archi.png)\n",
    "\n",
    "*The above figure is from the [UNet++ paper](https://arxiv.org/pdf/1807.10165).*\n",
    "\n",
    "I used a **2D UNet++** model from the `segmentation_models_pytorch` library, with an **EfficientNet-B8** encoder pretrained on ImageNet.\n",
    "\n",
    "This approach yielded strong performance thanks to the following key factors:\n",
    "\n",
    "### 2.5D Architecture\n",
    "I predict the current frame using multiple frames at once: the 3 previous frames, the current frame, and the 3 following frames (7 frames total).  \n",
    "This strategy enhances spatial consistency and overall robustness of the model.\n",
    "\n",
    "### Multi-axis Training (x and y)\n",
    "For each fold, I trained the same model on two different axes:\n",
    "- The **x** axis, with frame shape (300, 1259)  \n",
    "- The **y** axis, also with frame shape (300, 1259)\n",
    "\n",
    "Predictions are then made along both axes, which enriches the model’s ability to detect faults.\n",
    "\n",
    "### Data Augmentation\n",
    "I applied transformations such as a 180° rotation and horizontal/vertical flips to help the model generalize better.\n",
    "\n",
    "### Model Ensembling\n",
    "I chose to split the dataset into 6 folds: 325 volumes for training and 65 for validation in each.  \n",
    "Due to time constraints, my final solution uses only the models from the first 3 folds, each trained for a maximum of 12 epochs.\n",
    "\n",
    "This combination of techniques — using **2D UNet++**, **2.5D** prediction, **multi-axis training**, **data augmentation**, and **model ensembling** — significantly contributed to achieving strong fault segmentation results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42942c71",
   "metadata": {},
   "source": [
    "# How to use this notebook ?\n",
    "\n",
    "This notebook demonstrates how to generate fault predictions for 3D volumes using one or multiple model checkpoints. \n",
    "We assume each checkpoint is specialized in predicting along one axis (or a combined \"xy\" axis which effectively means x and y). \n",
    "\n",
    "Below is an overview of the steps:\n",
    "\n",
    "0. **Convert 3D Volumes to 2D Slices**\n",
    "1. **Configuration**: Define the parameters for:\n",
    "   - Model checkpoints and corresponding axes.\n",
    "   - Input dataset path and optional volume filtering.\n",
    "   - Batch size, number of workers, etc.\n",
    "   - Output settings (where to save the final submission, probability volumes, thresholded volumes).\n",
    "   - Performance optimizations such as model compilation or forcing CPU usage.\n",
    "2. **Dataset Loading**: Load the dataset index (e.g., a parquet file) containing volume information.\n",
    "3. **Volume Filtering**: Optionally filter the dataset to process only specific volumes.\n",
    "4. **Prediction**: For each checkpoint and axis specification, generate the prediction volumes.\n",
    "5. **Ensembling**: Combine/average all prediction volumes.\n",
    "6. **Thresholding**: Apply a probability threshold to the ensembled predictions to generate a final binary mask.\n",
    "7. **Confidence-Based Zeroing**: We may choose to zero out an entire volume if its mean confidence \n",
    "   (across the predicted mask) is below a specified threshold, because we assume the volume might be \n",
    "   completely empty (no faults to predict).\n",
    "8. **Submission**: Create the final `.npz` file (and optionally save intermediate probability and thresholded volumes).\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711524ca",
   "metadata": {},
   "source": [
    "## 0. Convert 3D Volumes to 2D Slices\n",
    "Before diving into the prediction pipeline, we need to preprocess the 3D volumes by converting them into 2D slices. This step is crucial as our models are designed to work with 2D data.\n",
    "\n",
    "Function Overview\n",
    "The write_2d_slices function handles the conversion of 3D volumes into 2D slices. It takes the following parameters:\n",
    "\n",
    "root_dir: The directory containing your 3D volume data.\n",
    "output_dir: The directory where the 2D slices will be saved. This will later be used as the root_dir in the prediction configuration.\n",
    "mode: (Optional) If your root_dir includes multiple data parts (e.g., both training and testing data), you can specify the mode as \"test\" to convert only the test volumes.\n",
    "Usage\n",
    "First, ensure that the write_2d_slices function is correctly imported from your src/write_2d_slices.py module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302ad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directories\n",
    "ROOT_PATH_TO_ALL_TEST_PARTS = \"/data/datasets/dark-size-test\"  # Directory with 3D volumes\n",
    "PATH_TO_2D_SLICES_OUTPUT = \"/data/datasets/darkside-test-data-2d\"   # Directory to save 2D slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3d3049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the write_2d_slices function\n",
    "from src.write_2d_slices import write_2d_slices\n",
    "\n",
    "# (Optional) Specify the mode if your root_dir contains both train and test data parts\n",
    "CONVERSION_MODE = \"test\"  # Use \"test\" to convert only test volumes\n",
    "\n",
    "# Convert the 3D volumes to 2D slices\n",
    "write_2d_slices(\n",
    "    root_dir=ROOT_PATH_TO_ALL_TEST_PARTS,\n",
    "    output_dir=PATH_TO_2D_SLICES_OUTPUT,\n",
    "    axes=[\"x\", \"y\"],              # Specify the axes along which to slice\n",
    "    num_workers=25,          # Adjust based on your system's capabilities\n",
    "    mode=CONVERSION_MODE,     # Optional: specify the mode if needed\n",
    ")\n",
    "\n",
    "print(\"3D volumes have been successfully converted to 2D slices.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4db15f4",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "We'll start by importing the necessary libraries. \n",
    "The `predict_single_checkpoint`, `ensemble_volumes_and_save`, and `build_final_volumes_dir_name` functions \n",
    "are assumed to be defined in the `src.predict_utils` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Assuming you have these in a local python file at src/predict_utils.py\n",
    "from src.predict_utils import (\n",
    "    build_final_volumes_dir_name,\n",
    "    predict_single_checkpoint,\n",
    "    ensemble_volumes_and_save,\n",
    ")\n",
    "\n",
    "print(\"Imports done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed654147",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "In the cell below, we define a configuration dictionary `args` that simulates\n",
    "what would typically come from command-line arguments. \n",
    "The important keys in `args` are:\n",
    "\n",
    "- **checkpoints**: List of paths to your `.ckpt` model checkpoints.\n",
    "- **axes**: List of axes for each checkpoint (`'x'`, `'y'`, or `'xy'`). \n",
    "  - For example, if `axes[i] == 'xy'`, that means that particular checkpoint should be run on the `x` axis and the `y` axis.\n",
    "  - The length of `checkpoints` must match the length of `axes`.\n",
    "- **root_dir**: The directory where your 2D slices (and `dataset.parquet`) are stored.\n",
    "- **vol_filter**: (Optional) If not `None`, a list of volume IDs (`sample_id`) to process.\n",
    "- **batch_size**: Batch size for inference.\n",
    "- **num_workers**: Number of workers for the DataLoader.\n",
    "- **save_threshold**: Probability threshold to apply to the averaged predictions.\n",
    "- **min_mean_conf**: Minimum mean confidence required to keep the prediction (if below, the volume is zeroed).\n",
    "- **submission_path**: Path where the final `.npz` submission file will be saved.\n",
    "- **save_probas**: If `True`, save raw probability volumes in `predictions_probas/{model_name}/{axis}`.\n",
    "- **save_final**: If `True`, save final thresholded volumes in a combined directory.\n",
    "- **compile**: If `True`, compile the model for optimized performance (PyTorch 2.x feature).\n",
    "- **cpu**: If `True`, run inference on CPU (otherwise use CUDA if available).\n",
    "- **dtype**: The data type for inference (`float16`, `float32`, or `bf16`).\n",
    "- **force_prediction**: If `True`, re-predict even if volumes are already available.\n",
    "\n",
    "Feel free to modify the values below to match your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49835c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration dictionary\n",
    "args = {\n",
    "    \"checkpoints\": [\n",
    "        # Example: \"path/to/checkpoint_model_a.ckpt\",\n",
    "        #          \"path/to/checkpoint_model_b.ckpt\"\n",
    "        \"checkpoints/checkpoints_unetpp_timm-efficientnet-b8_nchans7_val_axisxy_20250102_152812/fold0-best-model-epoch=09-val_dice_3d=0.8959.ckpt\",\n",
    "        \"checkpoints/checkpoints_unetpp_timm-efficientnet-b8_nchans7_val_axisxy_20250102_152812/fold1-best-model-epoch=10-val_dice_3d=0.8988.ckpt\",\n",
    "        \"checkpoints/checkpoints_unetpp_timm-efficientnet-b8_nchans7_val_axisxy_20250102_152812/fold2-best-model-epoch=10-val_dice_3d=0.8870.ckpt\",\n",
    "    ],\n",
    "    \"axes\": [\n",
    "        # Must be 'x', 'y', or 'xy' and match the number of checkpoints.\n",
    "        # If a checkpoint is for 2 axes (say \"xy\"), that single checkpoint\n",
    "        # will be used to predict on axis 'x' and axis 'y'.\n",
    "        \"xy\",\n",
    "        \"xy\",\n",
    "        \"xy\",\n",
    "    ],\n",
    "    \"root_dir\": PATH_TO_2D_SLICES_OUTPUT,  # Contains dataset.parquet and 2D slices\n",
    "    \"vol_filter\": None,               # List of sample_ids to filter, or None for no filter\n",
    "    \"batch_size\": 8,\n",
    "    \"num_workers\": 8,\n",
    "    \"save_threshold\": 0.5,\n",
    "    \"min_mean_conf\": 0.1,  # If not None, volume is zeroed if mean confidence < this\n",
    "    \"submission_path\": \"submission.npz\",\n",
    "    \"save_probas\": True,   # always set it to True\n",
    "    \"save_final\": False,    # Whether to save thresholded volumes (just before encode them in create_submission)\n",
    "    \"compile\": False,\n",
    "    \"cpu\": False,\n",
    "    \"dtype\": \"bf16\",\n",
    "    \"force_prediction\": False, # Whether to re compute model predictions even if they already exists (with previous call with save_proba == True)\n",
    "}\n",
    "\n",
    "print(\"Configuration set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479212a3",
   "metadata": {},
   "source": [
    "## 3. Setting Up the Device\n",
    "\n",
    "We'll determine whether to use the CPU or GPU (CUDA) for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8253eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' if args[\"cpu\"] else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb718e",
   "metadata": {},
   "source": [
    "## 4. Basic Checks and Axis Counting\n",
    "\n",
    "We ensure that the number of checkpoints matches the number of axis specifications. \n",
    "We also count the total number of axes (so that if there's only one total axis, \n",
    "the `min_mean_conf` won't really have an effect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e659555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(args[\"checkpoints\"]) != len(args[\"axes\"]):\n",
    "    raise ValueError(\"Number of checkpoints must match number of axes specifications.\")\n",
    "\n",
    "# Count total axes across all specs\n",
    "total_axes = 0\n",
    "for axis_spec in args[\"axes\"]:\n",
    "    if axis_spec.lower() == 'xy':\n",
    "        total_axes += 2\n",
    "    else:\n",
    "        total_axes += 1\n",
    "\n",
    "# If there's only one model/axis combination but min_mean_conf is set, warn the user\n",
    "if len(args[\"checkpoints\"]) == 1 and total_axes == 1 and args[\"min_mean_conf\"] is not None:\n",
    "    print(\n",
    "        f\"Warning: You set a min_mean_conf ({args['min_mean_conf']}) but there's only one model and one axis. \"\n",
    "        \"Confidence-based filtering will be ignored.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c2b0c",
   "metadata": {},
   "source": [
    "## 5. Loading the Dataset\n",
    "\n",
    "We assume there's a `dataset.parquet` file in `args[\"root_dir\"]` which contains \n",
    "information about each sample (volume). We'll load it into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = os.path.join(args[\"root_dir\"], 'dataset.parquet')\n",
    "full_df = pd.read_parquet(df_path)\n",
    "print(\"Dataset index loaded.\")\n",
    "print(f\"Total samples in dataset: {len(full_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27b2940",
   "metadata": {},
   "source": [
    "## 6. Optional Volume Filtering (Only for fast debug, do not use these option in production)\n",
    "\n",
    "If `args[\"vol_filter\"]` is provided, we'll keep only the specified volume IDs. \n",
    "Otherwise, we'll process all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26854c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args[\"vol_filter\"]:\n",
    "    print(f\"Applying volume filter with {len(args['vol_filter'])} sample_id(s).\")\n",
    "    initial_count = len(full_df)\n",
    "    filtered_df = full_df[full_df['sample_id'].isin(args[\"vol_filter\"])].copy()\n",
    "    final_count = len(filtered_df)\n",
    "    missing_samples = set(args[\"vol_filter\"]) - set(filtered_df['sample_id'].unique())\n",
    "\n",
    "    print(f\"Number of samples after filtering: {final_count} (filtered out {initial_count - final_count} samples).\")\n",
    "    if missing_samples:\n",
    "        print(\n",
    "            \"Warning: The following sample_id(s) were not found in the dataset and will be ignored: \"\n",
    "            f\"{', '.join(missing_samples)}\"\n",
    "        )\n",
    "    full_df = filtered_df\n",
    "else:\n",
    "    print(\"No volume filter applied. Processing all samples.\")\n",
    "\n",
    "print(f\"Total samples to process: {len(full_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bab076d",
   "metadata": {},
   "source": [
    "## 7. Generating Predictions for Each Checkpoint & Axis\n",
    "\n",
    "For each entry in `args[\"checkpoints\"]` and its corresponding `args[\"axes\"]`, we call \n",
    "the `predict_single_checkpoint()` function, which handles slicing through the DataFrame, \n",
    "loading the model, and generating predictions.\n",
    "\n",
    "- If `axes[i] == 'xy'`, we will run predictions on both `x` and `y` axes using the same checkpoint.\n",
    "- We also pass:\n",
    "  - `batch_size`, `num_workers`\n",
    "  - `save_probas`: whether to save probability volumes\n",
    "  - `force_prediction`: whether to overwrite existing predictions\n",
    "  - `root_dir`: path to slices\n",
    "  - `dtype`, `compile_model`, `cpu`, `device`: for inference configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fec9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ckpt_path_str, axis_spec in zip(args[\"checkpoints\"], args[\"axes\"]):\n",
    "    checkpoint_path = Path(ckpt_path_str)\n",
    "    \n",
    "    # For 'xy', we do x, then y\n",
    "    if axis_spec.lower() == 'xy':\n",
    "        axis_list = ['x', 'y']\n",
    "    else:\n",
    "        axis_list = [axis_spec.lower()]\n",
    "    \n",
    "    for ax in axis_list:\n",
    "        print(f\"Predicting for Checkpoint: {checkpoint_path.name}, Axis: {ax}\")\n",
    "        predict_single_checkpoint(\n",
    "            checkpoint_path=checkpoint_path,\n",
    "            axis=ax,\n",
    "            full_df=full_df,\n",
    "            batch_size=args[\"batch_size\"],\n",
    "            num_workers=args[\"num_workers\"],\n",
    "            save_probas=args[\"save_probas\"],\n",
    "            force_prediction=args[\"force_prediction\"],\n",
    "            root_dir=args[\"root_dir\"],\n",
    "            dtype=args[\"dtype\"],\n",
    "            compile_model=args[\"compile\"],\n",
    "            cpu=args[\"cpu\"],\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "print(\"All checkpoints processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a5315",
   "metadata": {},
   "source": [
    "## 8. Gathering Predictions and Ensembling\n",
    "\n",
    "Now that we've generated predictions for each checkpoint and axis, we collect \n",
    "the corresponding directories in order to perform an ensembling step \n",
    "(e.g., averaging predictions).\n",
    "\n",
    "We'll look for sub-folders of the form: \n",
    "``predictions_probas/{model_name}/{axis}``.\n",
    "\n",
    "Afterwards, we can apply a final threshold (and optionally `min_mean_conf`) \n",
    "to the ensembled probability volumes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6013bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the prediction directories for ensembling\n",
    "prediction_dirs: List[Path] = []\n",
    "for ckpt_path_str, axis_spec in zip(args[\"checkpoints\"], args[\"axes\"]):\n",
    "    checkpoint_path = Path(ckpt_path_str)\n",
    "    model_name = checkpoint_path.stem\n",
    "\n",
    "    # If 'xy', we expect subaxes x and y\n",
    "    if axis_spec.lower() == 'xy':\n",
    "        sub_axes = ['x', 'y']\n",
    "    else:\n",
    "        sub_axes = [axis_spec.lower()]\n",
    "\n",
    "    for sub_ax in sub_axes:\n",
    "        pred_dir = checkpoint_path.parent / 'predictions_probas' / model_name / sub_ax\n",
    "        print(f\"Looking for predictions in {pred_dir}\")\n",
    "        if pred_dir.exists():\n",
    "            prediction_dirs.append(pred_dir)\n",
    "\n",
    "print(f\"Found {len(prediction_dirs)} relevant prediction directories for ensembling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682f009",
   "metadata": {},
   "source": [
    "## 9. Building Final Volumes and Creating Submission\n",
    "\n",
    "If `args[\"save_final\"]` is `True`, we'll build a special folder name that indicates \n",
    "which checkpoints/axes were used, then we'll call `ensemble_volumes_and_save()` \n",
    "to finalize the outputs and create the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a2840",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_volumes_dir = None\n",
    "if args[\"save_final\"]:\n",
    "    final_dir_name = build_final_volumes_dir_name(args[\"checkpoints\"], args[\"axes\"])\n",
    "    final_volumes_dir = Path(final_dir_name)\n",
    "    print(f\"Final volumes directory will be '{final_dir_name}'\")\n",
    "\n",
    "ensemble_volumes_and_save(\n",
    "    all_predictions=prediction_dirs,\n",
    "    dataset_index=full_df,\n",
    "    output_path=Path(args[\"submission_path\"]),\n",
    "    save_threshold=args[\"save_threshold\"],\n",
    "    device=device,\n",
    "    min_mean_conf=args[\"min_mean_conf\"],\n",
    "    save_final_volumes=args[\"save_final\"],\n",
    "    final_volumes_dir=final_volumes_dir\n",
    ")\n",
    "\n",
    "print(\"Ensembling and submission creation completed successfully.\")\n",
    "print(f\"Submission file: {args['submission_path']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
