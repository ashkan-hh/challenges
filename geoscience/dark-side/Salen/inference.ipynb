{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff1aaf0",
   "metadata": {},
   "source": [
    "### This file serves only as an explanation of the inference process, which takes about 12 hours due to the fact that it runs on a single card. Therefore, we provide a parallelization script parallel_infer.sh, After executing parallel_infer.sh, run the following cell to aggregate the final submission, which takes only about 3 hours.\n",
    "\n",
    "### This script assumes that the test set contains only the same 50 files as the public test set; if the number of files changes, manually edit the --shard parameter. For example, if there are 100 files, it should be changed to 0_25 25_50 50_75 75_100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1634fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from darkside_utils import create_submission\n",
    "\n",
    "all_test_vols = sorted(glob.glob('test_data/*/*.npy'))\n",
    "\n",
    "!rm sub.npz\n",
    "for filename in all_test_vols:\n",
    "    \n",
    "    sample_id = filename.split('/')[1]\n",
    "    ground_truth = np.load(filename.replace('test_data/', 'test_data/infered_results/'))\n",
    "\n",
    "    # creating a .npz file and populating it with sample IDs and their ground-truth fault coordinates\n",
    "    create_submission(\n",
    "        sample_id, ground_truth, \"sub.npz\", append=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfbe4d9",
   "metadata": {},
   "source": [
    "## Below is an explanation of the inference code, we recommend reading it first and preparing the data structures to be inferred as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74237ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import re\n",
    "import segmentation_models_pytorch_3d as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e77a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--world_size', default=4, type=int, help='Number of GPUs')\n",
    "parser.add_argument('--port', default=12345, type=int, help='DDP Port')\n",
    "parser.add_argument('--lr', default=4e-4, type=float, help='Initial learning rate')\n",
    "parser.add_argument('--eta_min', default=1e-7, type=float, help='Cosine minimum lr')\n",
    "parser.add_argument('--sub', default=94.64, type=float, help='Input subtract factor')\n",
    "parser.add_argument('--width', default=3, type=int, help='A Width Multiply Factor to Original SegResNet')\n",
    "parser.add_argument('--depth', default=1, type=int, help='A Depth Multiply Factor to Original SegResNet')\n",
    "parser.add_argument('--epochs', default=20, type=int, help='Number of epochs')\n",
    "#parser.add_argument('--fold', default=0, type=int, help='Number of kfold')\n",
    "parser.add_argument('--batch_size', default=1, type=int, help='Batch size')\n",
    "parser.add_argument('--patch_size', default='192_192_640', type=str, help='Patch size')\n",
    "parser.add_argument('--val_interv', default=1, type=int, help='Valid Interval')\n",
    "parser.add_argument('--log_dir', default='./runs', type=str, help='Directory for tensorboard logs')\n",
    "parser.add_argument('--noeval',default=False,action='store_true',help='Turn off Eval Process')\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2871f614",
   "metadata": {},
   "source": [
    "## Make sure pretrained.pth is in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c81db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 'cuda'\n",
    "\n",
    "model = smp.Unet(\n",
    "        encoder_name=\"efficientnet-b4\", # choose encoder, e.g. resnet34\n",
    "        in_channels=4,                  # model input channels (1 for gray-scale volumes, 3 for RGB, etc.)\n",
    "        encoder_weights=None,\n",
    "        classes=1,                      # model output channels (number of classes in your dataset)\n",
    "    ).to(rank)\n",
    "model.load_state_dict({k.replace('module.', ''): v for k, v in torch.load('pretrained.pth', map_location=rank).items()})\n",
    "model.eval()\n",
    "print('loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e522656",
   "metadata": {},
   "source": [
    "## Place the files to be tested in the test_data folder\n",
    "\n",
    "eg. \n",
    "```\n",
    "test_data/2023-10-05_32387297/seismicCubes_RFC_fullstack_2023.76123161.npy\n",
    "test_data/2023-10-05_1f452c99/seismicCubes_RFC_fullstack_2023.76188125.npy\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659598b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_vols = glob.glob('test_data/*/*.npy')\n",
    "len(all_test_vols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47cdd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (300, 300, 1259)\n",
    "stride=(24, 24, 80)\n",
    "patch_size=list(map(int, args.patch_size.split('_')))\n",
    "\n",
    "\n",
    "all_slices = []\n",
    "for x_min in list(range(0, shape[-3]-patch_size[0], stride[0]))+[shape[-3]-patch_size[0]]:\n",
    "    for y_min in list(range(0, shape[-2]-patch_size[1], stride[1]))+[shape[-2]-patch_size[1]]:\n",
    "        for z_min in list(range(0, shape[-1]-patch_size[2], stride[2]))+[shape[-1]-patch_size[2]]:\n",
    "            x_max = x_min + patch_size[0]\n",
    "            y_max = y_min + patch_size[1]\n",
    "            z_max = z_min + patch_size[2]\n",
    "\n",
    "            all_slices.append((x_min, x_max, y_min, y_max, z_min, z_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc15a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_arr = {i: np.load(i).astype(np.float16) for i in tqdm(all_test_vols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ca8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_overlap = {i: np.zeros(shape, dtype=np.int16) for i in tqdm(all_test_vols)}\n",
    "all_pred = {i: np.zeros(shape, dtype=np.float32) for i in tqdm(all_test_vols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d22fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tta(array, method='h', back=False): #Test-Time-Augmentation\n",
    "    \n",
    "    if method == 'h':\n",
    "        return np.flip(array, axis=0)\n",
    "    elif method == 'v':\n",
    "        return np.flip(array, axis=1)\n",
    "    elif method == 'hv':\n",
    "        return np.flip(np.flip(array, axis=1), axis=0)\n",
    "        \n",
    "    elif method == 'r':\n",
    "        if not back:\n",
    "            return np.rot90(array, axes=(0, 1))\n",
    "        else:\n",
    "            return np.rot90(array, axes=(0, 1), k=-1)\n",
    "\n",
    "    elif method == 'hr':\n",
    "        if not back:\n",
    "            array = np.flip(array, axis=0)\n",
    "            return np.rot90(array, axes=(0, 1))\n",
    "        else:\n",
    "            return np.flip(np.rot90(array, axes=(0, 1), k=-1), axis=0)\n",
    "\n",
    "    elif method == 'vr':\n",
    "        if not back:\n",
    "            array = np.flip(array, axis=1)\n",
    "            return np.rot90(array, axes=(0, 1))\n",
    "        else:\n",
    "            return np.flip(np.rot90(array, axes=(0, 1), k=-1), axis=1)\n",
    "\n",
    "    elif method == 'hvr':\n",
    "        if not back:\n",
    "            array = np.flip(np.flip(array, axis=1), axis=0)\n",
    "            return np.rot90(array, axes=(0, 1))\n",
    "        else:\n",
    "            array = np.flip(np.flip(array, axis=1), axis=0)\n",
    "            return np.rot90(array, axes=(0, 1), k=-1)\n",
    "\n",
    "    elif not method:\n",
    "        return array\n",
    "    \n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb3d6a",
   "metadata": {},
   "source": [
    "## Since the inference phase requires online computation of gradients and edge channels, we use dataset and dataloader to provide CPU parallel preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fdfa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferDataset(Dataset):\n",
    "    def __init__(self, all_arrays):\n",
    "\n",
    "        #self.all_arrays = all_arrays\n",
    "        self.all_arr_slc = []\n",
    "        for arr in all_arrays:\n",
    "            for aug in [None, 'h', 'v', 'hv', 'r', 'hr', 'vr', 'hvr']:\n",
    "            #for aug in [None, 'h', 'v', 'r']:\n",
    "                for slc in all_slices:\n",
    "                    self.all_arr_slc.append((arr, slc, aug))\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.all_arr_slc)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        name, (x_min, x_max, y_min, y_max, z_min, z_max), aug_ = self.all_arr_slc[idx]\n",
    "        input_array = all_arr[name]\n",
    "        seismic = input_array[x_min:x_max, y_min:y_max, z_min:z_max].copy()\n",
    "\n",
    "        seismic = tta(seismic, method=aug_, back=False)\n",
    "                \n",
    "        #Feature Engineering\n",
    "        grad = np.gradient(seismic, axis=2).astype('float')\n",
    "        edge_grad = (np.roll(grad, 1, axis=2) * grad <= 0).astype('float')\n",
    "        edge_raw = (np.roll(seismic, 1, axis=2) * seismic <= 0).astype('float')\n",
    "        \n",
    "        # rescale volume\n",
    "        seismic = seismic / 94.64\n",
    "        grad = grad / 39.04\n",
    "        \n",
    "        seismic = np.nan_to_num(np.stack([seismic, grad, edge_grad, edge_raw], axis=0), nan=0.0, posinf=0.0, neginf=0.0).clip(-1, 1)\n",
    "\n",
    "        seismic = torch.from_numpy(seismic).float()\n",
    "\n",
    "        return seismic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da94d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = InferDataset(all_test_vols)\n",
    "dl = DataLoader(ds, \n",
    "              batch_size=1, \n",
    "              num_workers=16, \n",
    "              pin_memory=True,\n",
    "              drop_last=False,\n",
    "              shuffle=False,\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdcd976",
   "metadata": {},
   "source": [
    "## Increase inference speed by about 2x with torch.compile API. Try commenting out this line if it's reporting errors due to environment configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b65658",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92043dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dl_idx, seismic in enumerate(tqdm(dl)):\n",
    "    name, (x_min, x_max, y_min, y_max, z_min, z_max), aug_ = ds.all_arr_slc[dl_idx]\n",
    "    with torch.no_grad():\n",
    "        pred = model(seismic.cuda()).squeeze().sigmoid().cpu().numpy()\n",
    "        \n",
    "    pred = tta(pred, method=aug_, back=True)\n",
    "\n",
    "    all_overlap[name][x_min:x_max, y_min:y_max, z_min:z_max] += 1\n",
    "    all_pred[name][x_min:x_max, y_min:y_max, z_min:z_max] += pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r test_data/infered_results/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffdc011",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, nam in enumerate(tqdm(all_test_vols)):\n",
    "\n",
    "    outputs = all_pred[nam] / all_overlap[nam]\n",
    "\n",
    "    save_dir = nam.replace('test_data/', 'test_data/infered_results/')\n",
    "    os.makedirs(os.path.dirname(save_dir), exist_ok=True)\n",
    "    np.save(save_dir, (outputs > 0.5).astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darkside_utils import create_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef265cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm sub.npz\n",
    "\n",
    "for filename in tqdm(all_test_vols):\n",
    "    \n",
    "    sample_id = filename.split('/')[1]\n",
    "    ground_truth = np.load(filename.replace('test_data/', 'test_data/infered_results/'))\n",
    "\n",
    "    # creating a .npz file and populating it with sample IDs and their ground-truth fault coordinates\n",
    "    create_submission(\n",
    "        sample_id, ground_truth, \"sub.npz\", append=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
