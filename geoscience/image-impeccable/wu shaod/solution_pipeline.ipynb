{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Solution Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Data Preprocessing / Feature Engineering\n",
    "In general, our solution does not use complex feature engineering.\n",
    "\n",
    "And, we use the following data processing methods to improve the stability of training and the final performance: \n",
    "1) Normalize the noisy seismic and denoised seismic on the entire volume to 0 - 255 without clip;\n",
    "2) Extract 2D slices from 3D volumes for training and test. And use the entire slice (1259x300) for training and testing;\n",
    "3) Use horizontal flip data augmentation during training and horizontal flip test time augmentation during testing;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Model description\n",
    "\n",
    "As for the model, we use [NAFNet-width64](https://arxiv.org/abs/2204.04676) as our final model, which is the UNet-based model, exceeds the SOTA methods before and is computationally efficient. \n",
    "\n",
    "And we use the [official pretrained model](https://drive.google.com/file/d/14Fht1QQJ2gMlk4N1ERCRuElg8JfjrWWR/view?usp=sharing) on [SIDD dataset](https://abdokamel.github.io/sidd/).\n",
    "\n",
    "\n",
    "The architecture of NAFNet is shown in the figure below. It is a Unet built by NAFNet block, which contains SCA (Simple Channel Attention) module and Simple Gate module.\n",
    "\n",
    "![NAFNet model schema](images/NAFNet.png \"UNET with 'resnet34' encoder\")\n",
    "\n",
    "The image is taken from [here](https://arxiv.org/abs/2204.04676).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We developed this solution based on the [official NAFNet code](https://github.com/megvii-research/NAFNet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Hardware and environment\n",
    "\n",
    "- PyTorch  2.1.2\n",
    "\n",
    "- Python  3.10 (ubuntu22.04)\n",
    "\n",
    "- Cuda  11.8\n",
    "\n",
    "- GPU  A40(48GB) * 2\n",
    "\n",
    "- CPU  30 vCPU AMD EPYC 7543 32-Core Processor\n",
    "\n",
    "\n",
    "With the current pipeline settings, the training process took about 40 hours (with 2 NVIDIA A40 GPU (2 * 48 GB) available).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Solution Reproduction Steps\n",
    "\n",
    "<font color=red>If you only want to reproduce my inference result, you only need to read section 2.1 and 2.6.</font>\n",
    "\n",
    "<font color=red>If you want to reproduce both my training and inference result, you need to read from section 2.1 step by step.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Environment Setup\n",
    "Please, run the following command to install all needed libraries and packages.\n",
    "\n",
    "<font color=red>Custom_NAFNet is modified by me according to the official code of NAFNet, and the training is based on this library</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt\n",
    "! cd ./src/Custom_NAFNet && python setup.py develop --no_cuda_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Download and unzip data\n",
    "\n",
    "Download the original competition data and unzip it.\n",
    "\n",
    "Run the shell script in the next cell to download the original competition train data and unzip it. You need to modify the following variable in the below cell:\n",
    "- ```SRC_TRAIN_DATA_ROOT```, <font color=red>represents the path to save the downloaded training data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "SRC_TRAIN_DATA_ROOT=\"./data/train_images/\"\n",
    "\n",
    "wget -P $SRC_TRAIN_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-train-data-part1.zip\n",
    "wget -P $SRC_TRAIN_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-train-data-part2.zip\n",
    "wget -P $SRC_TRAIN_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-train-data-part3.zip\n",
    "wget -P $SRC_TRAIN_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-train-data-part4.zip\n",
    "wget -P $SRC_TRAIN_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-train-data-part5.zip\n",
    "wget -P $SRC_TRAIN_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-train-data-part6.zip\n",
    "wget -P $SRC_TRAIN_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-train-data-part7.zip\n",
    "wget -P $SRC_TRAIN_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-train-data-part8.zip\n",
    "wget -P $SRC_TRAIN_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-train-data-part9.zip\n",
    "wget -P $SRC_TRAIN_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-train-data-part10.zip\n",
    "wget -P $SRC_TRAIN_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-train-data-part11.zip\n",
    "wget -P $SRC_TRAIN_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-train-data-part12.zip\n",
    "wget -P $SRC_TRAIN_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-train-data-part13.zip\n",
    "wget -P $SRC_TRAIN_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-train-data-part14.zip\n",
    "wget -P $SRC_TRAIN_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-train-data-part15.zip\n",
    "wget -P $SRC_TRAIN_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-train-data-part16.zip\n",
    "wget -P $SRC_TRAIN_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-train-data-part17.zip\n",
    "\n",
    "for file in $(find $SRC_TRAIN_DATA_ROOT/*.zip -type f); do\n",
    "    echo \"$file is a file\"\n",
    "    unzip -q $file -d $SRC_TRAIN_DATA_ROOT\n",
    "    rm $file\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Preprocess training data\n",
    "\n",
    "Because volume-based data normalization is time-consuming, the training data needs to be normalized to the range of 0-255 before training process, and in order to facilitate data reading, all data shapes are transposed to the same shape (1259, 300, 300) offline before training.\n",
    "\n",
    "Run the following code to get the normalized data. You need to modify the following three variables in the below code:\n",
    "1) ```SRC_TRAIN_DATA_ROOT```, represents the path of the original training data. This folder contains all the training data.\n",
    "2) ```DST_TRAIN_DATA_ROOT```, represents the storage path of the normalized and reshaped training data.\n",
    "3) ```PROCESS_THREAD_NUM```, represents the number of threads when multi-threading is performing data normalization. <font color=red>This variable can not be greater than multiprocessing.cpu_count().</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "import multiprocessing\n",
    "\n",
    "SRC_TRAIN_DATA_ROOT = r\"./data/train_images/\"\n",
    "DST_TRAIN_DATA_ROOT = r\"./data/train_images_new_shape/\"\n",
    "PROCESS_THREAD_NUM = 32\n",
    "\n",
    "os.makedirs(DST_TRAIN_DATA_ROOT, exist_ok=True)\n",
    "\n",
    "def rescale_volume(seismic, low=0, high=100):\n",
    "    \"\"\"\n",
    "    Rescaling 3D seismic volumes 0-255 range, clipping values between low and high percentiles\n",
    "    \"\"\"\n",
    "\n",
    "    minval = np.percentile(seismic, low)\n",
    "    maxval = np.percentile(seismic, high)\n",
    "\n",
    "    seismic = np.clip(seismic, minval, maxval)\n",
    "    seismic = ((seismic - minval) / (maxval - minval)) * 255\n",
    "\n",
    "    return seismic\n",
    "\n",
    "def process(test_id):\n",
    "    files = natsorted(os.listdir(f\"{SRC_TRAIN_DATA_ROOT}/{test_id}\"))\n",
    "    assert \"noise\" in files[1]\n",
    "    assert \"fullstack\" in files[0]\n",
    "    data = np.load(os.path.join(SRC_TRAIN_DATA_ROOT, test_id, files[1]), allow_pickle=True, mmap_mode=\"r+\")\n",
    "    label = np.load(os.path.join(SRC_TRAIN_DATA_ROOT, test_id,files[0]), allow_pickle=True, mmap_mode=\"r+\")\n",
    "\n",
    "    if data.shape != label.shape:\n",
    "        label = label.T\n",
    "    # trans to shape (1259, 300, 300)\n",
    "    if data.shape[1] == 1259:\n",
    "        data = data.transpose(1, 0, 2)\n",
    "        label = label.transpose(1, 0, 2)\n",
    "    elif data.shape[2] == 1259:\n",
    "        data = data.transpose(2, 0, 1)\n",
    "        label = label.transpose(2, 0, 1)\n",
    "\n",
    "    data = data.astype(np.float32)\n",
    "    label = label.astype(np.float32)\n",
    "\n",
    "    save_dir = os.path.join(DST_TRAIN_DATA_ROOT, test_id)\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    data = rescale_volume(data)\n",
    "    label = rescale_volume(label)\n",
    "\n",
    "    np.save(f\"{save_dir}/{files[1]}\", data)\n",
    "    np.save(f\"{save_dir}/{files[0]}\", label)\n",
    "\n",
    "\n",
    "with multiprocessing.Pool(processes = PROCESS_THREAD_NUM) as pool:\n",
    "    test_id_s = natsorted(os.listdir(SRC_TRAIN_DATA_ROOT))\n",
    "    print(test_id_s)\n",
    "    result = pool.map(process, test_id_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the above code, you should get a folder structure like this in directory ```./data/train_images_new_shape/```\n",
    "\n",
    "![train data dir](images/train_data_dir.png \"train data folder Structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Kfold training data\n",
    "\n",
    "We used local 5-fold cross validation during the training process. In this section, we divide the training data into 5-folds to obtain training and validation data for each fold.\n",
    "\n",
    "Run the following code to kfold the training data. You need to modify the following two variables in the below code:\n",
    "1) ```SRC_TRAIN_DATA_ROOT```, represents the path of the original training data. This folder contains all the training data.\n",
    "2) ```KFOLD_TXT_SAVE_ROOT```, represents the path to save the txt file of training and validation data in each fold, <font color=red>which will be used in the training section</font>\n",
    "\n",
    "<font color=red>Because the generated 5-fold division txt file is already provided in path ```./data/train_txt/```, you can skip the following code and directly use the txt file in folder ```./data/train_txt/``` for training.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "SRC_TRAIN_DATA_ROOT = r\"./data/train_images_new_shape/\"\n",
    "KFOLD_TXT_SAVE_ROOT = r\"./train_txt_reproduce/\"\n",
    "\n",
    "NUM_FOLD = 5\n",
    "RANDOM_SEED=123\n",
    "os.makedirs(KFOLD_TXT_SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "all_train_case = np.asarray(os.listdir(SRC_TRAIN_DATA_ROOT))\n",
    "kf = KFold(n_splits=NUM_FOLD, random_state=RANDOM_SEED, shuffle=True)\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(all_train_case)):\n",
    "    train_case = all_train_case[train_index]\n",
    "    valid_case = all_train_case[valid_index]\n",
    "\n",
    "    np.savetxt(f\"{KFOLD_TXT_SAVE_ROOT}/train_f{i}.txt\", train_case, fmt=\"%s\")\n",
    "    np.savetxt(f\"{KFOLD_TXT_SAVE_ROOT}/val_f{i}.txt\", valid_case, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Pipeline Training Configuration\n",
    "\n",
    "The training configuration file is located in ```./configs/CUSTOM_NAFNet-width64-train.yml```. <font color=red>In order to ensure the success of the training, the following variables in this file need to be modified.</font>\n",
    "1) ```num_gpu```, represents the number of GPUs to be used for training. We used 2 nvidia A40 GPU for training and write 2 here. <font color=red>If you use AWS SageMaker g5.12xlarge instance, you should write 4 here.</font>\n",
    "1) ```datasets.train.root_dir```, represents the root directory of the transposed training data, which should contain subfolders for the 249 training image pairs. <font color=red>It should be changed to the ```DST_TRAIN_DATA_ROOT``` where the transposed data was saved in Section 2.3.</font>\n",
    "2) ```datasets.train.txt_file```, represents the txt file used for training. To reproduce our results, you need to use the fold0 data divided in Section 2.4 for training. <font color=red>It should be changed to the path where the train_f0.txt file saved in section 2.4 is located.</font>\n",
    "3) ```datasets.val.root_dir```,  represents the root directory of the transposed training data,  should be the same as ```datasets.train.root_dir```.\n",
    "4) ```datasets.val.txt_file```, represents the txt file used for validation. <font color=red>It should be changed to the path where the val_f0.txt file saved in section 2.4 is located.</font>\n",
    "5) ```path.pretrain_network_g```, represents the path of the pretrained model. The model will be automatically loaded before training starts. It should be changed to <font color=red><u>./pretrained_model/NAFNet-SIDD-width64.pth</u>.</font>\n",
    "6) ```datasets.train.batch_size_per_gpu```, represents the mini batchsize of each GPU during training. When I use the A40 (48GB) GPU locally, I can set this variable to a maximum of 2. <font color=red>If you are using an AWS SageMaker g5.12xlarge instance, you can set this variable to 1.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Training step\n",
    "<font color=red>You can skip this step if you only want to inference with my pre-trained model, which is provided in </font>```./my_checkpoints/net_g_190000.pth```.\n",
    "\n",
    "Otherwise, run the following command to trigger the model training script. \n",
    "\n",
    "In the following command, the meaning of each variable is as follows:\n",
    "1) ```nproc_per_node```, represents the number of nodes you use for training. I set it to 2 when I training locally using 2xA40. <font color=red>If you are using an AWS SageMaker g5.12xlarge instance, you can set this variable to 4.</font>\n",
    "2) ```master_port```, communication Ports.\n",
    "3) ```./src/Custom_NAFNet/basicsr/train.py```, this is the main file of the training script and does not need to be modified.\n",
    "4) ```-opt```, this is the configuration file for the training process, including the model, data, training strategy, etc. <font color=red>You should modify it to the configuration file modified in section 2.5. The default is ```./configs/CUSTOM_NAFNet-width64-train.yml```.</font>\n",
    "\n",
    "NOTE:\n",
    "\n",
    "1) Using the same data and training configuration as mine, you can reproduce my training results.\n",
    "\n",
    "2) <font color=red>IF you do training process,  The training result will be saved to ```./experiments/final_solution_wushaodong``` directory. And you should select ```./experiments/final_solution_wushaodong/modles/net_g_190000.pth``` as your final training models, which can be used to reproduce my test results.</font>\n",
    "\n",
    "3) I spend nearly 40 hours to finish training by 2 x A40 locally. The training time can be used as a reference for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m torch.distributed.launch --nproc_per_node=4 --master_port=4321 ./src/Custom_NAFNet/basicsr/train.py -opt ./configs/CUSTOM_NAFNet-width64-train.yml --launcher pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat './src/Custom_NAFNet/experiments/*': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! scp -r ./src/Custom_NAFNet/experiments/* ./experiments/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Inference step\n",
    "To inference the model and form a predictions for test dataset please follow the instructions below. \n",
    "\n",
    "Firstly, download the test data, decompress it, and organize the test data into the format described below.\n",
    "\n",
    "![NAFNet model schema](images/test_data_dir.png)\n",
    "\n",
    "Run the shell script in the next cell to download the original competition test data and unzip it. You need to modify the following variable in the below cell:\n",
    "```SRC_TEST_DATA_ROOT```, <font color=red>represents the path to save the downloaded test data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "SRC_TEST_DATA_ROOT=\"./data/test_images/\"\n",
    "\n",
    "wget -P $SRC_TEST_DATA_ROOT https://xeek-public-287031953319-eb80.s3.amazonaws.com/image-impeccable/image-impeccable-test-data.zip\n",
    "\n",
    "for file in $(find $SRC_TEST_DATA_ROOT/*.zip -type f); do\n",
    "    echo \"$file is a file\"\n",
    "    unzip -q $file -d $SRC_TEST_DATA_ROOT\n",
    "    rm $file\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, you need to load the trained model and perform inference.\n",
    "\n",
    "In the following command, the meaning of each variable is as follows:\n",
    "\n",
    "1) ```./src/Custom_NAFNet/basicsr/inference.py```, this is the main file of the inference script and does not need to be modified.\n",
    "\n",
    "2) ```-opt```, this is the configuration file for the inference process, including the model, IO, pretrained model path, etc. <font color=red>The default file is ```./configs/CUSTOM_NAFNet-width64-test.yml```.</font>  And the following variables in this file need to be modified.</font>\n",
    "    - ```test_dir```, represents the root directory where the test data is located, which is ```SRC_TEST_DATA_ROOT``` in previous section.\n",
    "    - ```test_res_save_root```, represents the location where the inference results are stored. <font color=red><u>Note that what is stored here is the inference results of each volume, not the final submission format results.</u>.</font>\n",
    "    - ```path.pretrain_network_g```,  represents the path of the trained model. It can be changed to <font color=red><u>./my_checkpoints/net_g_190000.pth, which was trained by me locally</u>.</font> Also, it can be changed to the model path you trained in section 2.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ./src/Custom_NAFNet/basicsr/inference.py -opt ./configs/CUSTOM_NAFNet-width64-test.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use the following code to produce the final submission file for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "SUBMIT_FILE = \"./final_submission_wushaodong.npz\"\n",
    "INFER_RES_ROOT = \"/root/autodl-tmp/Image_Impeccable_Journey_to_Clarity/data/1_final_test_images_res_width64_in1out1_1259x300_bs4_f0_hfliptta_190000/\"\n",
    "TEST_CASES = ['2024-06-10_0d6402b1', '2024-06-10_1a4e5680', '2024-06-10_1b9a0096', \n",
    "            '2024-06-10_2bd82c05', '2024-06-10_3b118e17', '2024-06-10_43537d46', \n",
    "            '2024-06-10_662066f4', '2024-06-10_971ac6dd', '2024-06-10_9871c8c6', \n",
    "            '2024-06-10_b7c329be', '2024-06-10_bfd43f22', '2024-06-10_c952ed24', \n",
    "            '2024-06-10_cec3da7f', '2024-06-10_eb45f27e', '2024-06-11_f46c20fe']\n",
    "\n",
    "\n",
    "def rescale_volume(seismic, low=0, high=100):\n",
    "    \"\"\"\n",
    "    Rescaling 3D seismic volumes 0-255 range, clipping values between low and high percentiles\n",
    "    \"\"\"\n",
    "\n",
    "    minval = np.percentile(seismic, low)\n",
    "    maxval = np.percentile(seismic, high)\n",
    "\n",
    "    seismic = np.clip(seismic, minval, maxval)\n",
    "    seismic = ((seismic - minval) / (maxval - minval)) * 255\n",
    "\n",
    "    return seismic\n",
    "\n",
    "def create_submission(seismic_filenames: list, prediction: list, submission_path: str):\n",
    "    \"\"\"Function to create submission file out of all test predictions in one list\n",
    "\n",
    "    Parameters:\n",
    "        seismic_filenames: list of survey .npy filenames used for perdiction\n",
    "        prediction: list with 3D np.ndarrays of predicted missing parts\n",
    "        submission_path: path to save submission\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    submission = dict({})\n",
    "    for sample_name, sample_prediction in zip(seismic_filenames, prediction):\n",
    "        i_slices_index = (\n",
    "            np.array([0.25, 0.5, 0.75]) * sample_prediction.shape[0]\n",
    "        ).astype(int)\n",
    "        i_slices_names = [f\"{sample_name}_gt.npy-i_{n}\" for n in range(0, 3)]\n",
    "        i_slices = [sample_prediction[s, :, :].astype(np.uint8) for s in i_slices_index]\n",
    "        submission.update(dict(zip(i_slices_names, i_slices)))\n",
    "\n",
    "        x_slices_index = (\n",
    "            np.array([0.25, 0.5, 0.75]) * sample_prediction.shape[1]\n",
    "        ).astype(int)\n",
    "        x_slices_names = [f\"{sample_name}_gt.npy-x_{n}\" for n in range(0, 3)]\n",
    "        x_slices = [sample_prediction[:, s, :].astype(np.uint8) for s in x_slices_index]\n",
    "        submission.update(dict(zip(x_slices_names, x_slices)))\n",
    "\n",
    "    np.savez(submission_path, **submission)\n",
    "\n",
    "\n",
    "file_names_list = []\n",
    "pres = []\n",
    "for case in tqdm.tqdm(TEST_CASES):\n",
    "    data = np.load(f\"{INFER_RES_ROOT}/{case}/infe_res.npy\")\n",
    "    data1 = rescale_volume(data.copy().T)\n",
    "\n",
    "    file_names_list.append(case)\n",
    "    pres.append(data1.copy())\n",
    "\n",
    "create_submission(file_names_list, pres, SUBMIT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
