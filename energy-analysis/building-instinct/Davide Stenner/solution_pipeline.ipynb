{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution Overview\n",
    "\n",
    "My solution is based on LigthGBM trained on over 649 different feature based on Hourly consume to reduce memory consumption.\n",
    "\n",
    "The Cross validation scheme is a Stratified K fold for each different target and the LigthGBM was trained using the following metric:\n",
    "\n",
    "- Binary target: Binary Cross Entropy\n",
    "- Multi Class: Softmax\n",
    "\n",
    "For each target i selected the Number of Round which led to the best Cv-F1 Macro\n",
    "\n",
    "------\n",
    "\n",
    "Due to constrain time limit i was not able to fully reproduce each training on a dataset with 10.000 additional sample for each State/(Commercial/residential). The following target: in.geometry_floor_area_res, in.income_res, in.roof_material_res, in.vintage_res, in.weekday_operating_hours..hr_com was trained on a dataset with 5.000 additional sample.\n",
    "\n",
    "This makes the training procedure more complicated, as two trainings on two different datasets will be required to reproduce the submitted solution. This difficulty does not exist in the case of training on all targets in the dataset with 10,000 additional samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Dataset\n",
    "\n",
    "Two categories of additional dataset were used:\n",
    "\n",
    "### NRELP Eulp dataset:\n",
    "I downloaded each file from https://data.openei.org/s3_viewer?bucket=oedi-data-lake&prefix=nrel-pds-building-stock%2Fend-use-load-profiles-for-us-building-stock%2F\n",
    "\n",
    "I selected the following dataset release as the most recent:\n",
    "\n",
    "Feature dataset:\n",
    "- 2024/comstock_amy2018_release_1/timeseries_individual_buildings/by_state/upgrade=32/\n",
    "- 2022/resstock_amy2018_release_1.1/timeseries_individual_buildings/by_state/upgrade=10/\n",
    " \n",
    "Target Dataset (this file was saved by hand diorectly on ./data/data_dump):\n",
    "- 2024/comstock_amy2018_release_1/metadata/upgrade32.parquet -> renamed as metadata_com.parquet\n",
    "- 2022/resstock_amy2018_release_1.1/metadata/upgrade10.parquet -> renamed as metadata_res.parquet\n",
    "\n",
    "Each feature file was downloaded and downcasted/preprocessed using the powershell script (must be executed after concat_data.py):\n",
    "\n",
    "```\n",
    "# -1 as the number of home to scrape to scrape all\n",
    "command.ps1\n",
    "```\n",
    "Refer to Preprocessing Step for more information\n",
    "\n",
    "### Census dataset:\n",
    "QuickFacts for each State from https://www.census.gov/quickfacts/fact/table/{STATE}/PST045223.\n",
    "{STATE} was replaced by list of 5 state splitted by \",\" E.g.  \"AL,AK,AZ,AR,CA\" and executed for every USA state.\n",
    "Each file was saved to \"data\\other_dataset\\census\\QuickFacts Aug-23-2024_{num_file}.csv\"\n",
    "\n",
    "This files has information about geographical/anagrafical/economic for each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing / Feature Engineering\n",
    "\n",
    "The main part of my solution is based on the different feature i created:\n",
    "\n",
    "### Simple aggregation\n",
    "\n",
    "- average daily consumption over season, month, national holiday, state holiday.\n",
    "- average hour consumption over season, month, national holiday, state holiday and every TOU (time of use pricing).\n",
    "- average hour consumption over season, month and  weekend vs not weekend.\n",
    "- Every average hour consumption for each season (24 * 4 season).\n",
    "- Total consumption over season, month, week day, hour, overall.\n",
    "- Average hour consumption over season, month and the weekday.\n",
    "- Mean, Max, Min, Median hour consumption for each month.\n",
    "\n",
    "### Profile Aggregation\n",
    "\n",
    "- How many time a selected hour is a Min or a Max in a given day over all the year (is a %).\n",
    "- Average of the difference between hour consumption and mean, min, max hour consumption (24 feature).\n",
    "- How many time a selected weekday is a Min or a Max in a given week over all the year (is a %).\n",
    "- Average of the difference between daily consumption and mean, min, max week consumption (7 feature).\n",
    "\n",
    "### Range of work feature\n",
    "Given the difference between the next 4 hour consumption and the previous 4 hour consumption calculate:\n",
    "- 1 - The timestamp which has the max difference -> time of begin spike\n",
    "- 2 - The timestamp which has the min difference -> time of end spike\n",
    "\n",
    "This timestamps and the difference (range of work) are calculated by applying a Min to (1) and a Max to (2) this led to a conservative way to calculate the begin and end of the spike and also the difference.\n",
    "\n",
    "This calculation is done over mulltiple filter both for calculate the begin and the end.\n",
    "\n",
    "Begin filter:\n",
    "- Only between 3-13 in workday\n",
    "- Only between 3-13, weekend\n",
    "- Only on workday\n",
    "- Only on weekend\n",
    "\n",
    "End filter:\n",
    "- Only after 3 in workday\n",
    "- Only after 3 weekend\n",
    "- Only on workday\n",
    "- Only on weekend\n",
    "\n",
    "\n",
    "The total amount of memory required to download the dataset and create silver/gold datasets is 100gb+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Method\n",
    "\n",
    "I trained each target always using Stratified K-Fold on the selected target.\n",
    "\n",
    "- Building Stock Type was trained on the entire dataset.\n",
    "- Residential Targets: each residential target are trained only on the portion of residential data.\n",
    "- Commercial Targets: each commercial target are trained only on the portion of commercial data.\n",
    "\n",
    "\n",
    "The total amount of memory required for to run a complete experiment and save all required checkpoints is 50gb+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Insight\n",
    "\n",
    "### Building Stock Type\n",
    "\n",
    "For this singular target which is quite different from the others, the best feature are:\n",
    "\n",
    "- Average hour consumption over a given month and week day\n",
    "- How many time a selected weekday is a Min in a given week over all the year (is a %).\n",
    "\n",
    "For residential:\n",
    "\n",
    "- How many time a selected hour is a Min in a given day over all the year (is a %).\n",
    "- Average of the difference between daily consumption and mean week consumption.\n",
    "\n",
    "For commercial:\n",
    "- How many time a selected hour is a Min in a given day over all the year (is a %).\n",
    "- How many time a selected hour is a Max in a given day over all the year (is a %)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution Reproduction Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipeline Configuration\n",
    "\n",
    "Before running the pipeline, please navigate to the config/config.json file and specify the actual data paths to let the pipeline know about data location. \n",
    "\n",
    "- PATH_ORIGINAL_DATA: folder for starting dataset, default: \"data/original_dataset\",\n",
    "- PATH_SILVER_PARQUET_DATA: folder for silver dataset, default: \"data/silver_parquet_dataset\",\n",
    "- PATH_GOLD_PARQUET_DATA: folder for gold dataset, default: \"data/gold_parquet_dataset\",\n",
    "- PATH_MAPPER_DATA: folder for mapping file used in preprocessing dataset, default: \"data/mapper_dataset\",\n",
    "- PATH_OTHER_DATA: folder for other dataset, default: \"data/other_dataset\",\n",
    "- ORIGINAL_TRAIN_CHUNK_FOLDER: folder name where to put training parquet files, default: \"building-instinct-train-data\",\n",
    "- ORIGINAL_TEST_CHUNK_FOLDER: folder name where to put test parquet files, default: \"building-instinct-test-data\",\n",
    "- ORIGINAL_TRAIN_LABEL_FOLDER: folder name where to put training label parquet files, default: \"building-instinct-train-label\",\n",
    "\n",
    "Folder Structure:\n",
    "```\n",
    "data\n",
    "├─── gold_parquet_dataset\n",
    "│    └─── test_data.parquet\n",
    "│    └─── train_{target}_label.parquet\n",
    "└─── mapper_dataset\n",
    "│    └───mapper_category.json\n",
    "│    └───target_mapper.json.json\n",
    "└─── original_dataset\n",
    "│    └───building-instinct-test-data\n",
    "│    └───building-instinct-train-data\n",
    "│    └───building-instinct-train-label\n",
    "└─── other_dataset\n",
    "│    └─── census\n",
    "└─── silver_parquet_dataset\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "Please, run the following command to install all needed libraries and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Step\n",
    "You can skip this step if you want to start with the pretrained checkpoints provided in ./experiment/solution_lgb and go directly to Inference Step\n",
    "\n",
    "Otherwise, run the following command to preprocess everything.\n",
    "\n",
    "Run in the following order the following script before the training step (train and test parquet files must be placed under ORIGINAL_TRAIN_CHUNK_FOLDER, ORIGINAL_TRAIN_CHUNK_FOLDER folder):\n",
    "```\n",
    "#concat starting train/test dataset and create mapping json.\n",
    "python concat_data.py\n",
    "\n",
    "#will download every NRELP EULP files and preprocess it. This step will take couple of days depending on the connection.\n",
    "#when asked type -1 to download all\n",
    "command.ps1\n",
    "\n",
    "#create economics dataset.\n",
    "python script/create_economics_dataset.py\n",
    "```\n",
    "\n",
    "Required Memory: 50gb+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training step\n",
    "You can skip this step if you want to start with the pretrained checkpoints provided in ./experiment/solution_lgb and go directly to Inference Step.\n",
    "\n",
    "Otherwise, execute the following command.\n",
    "\n",
    "Two training must be run as i used two version of the additional dataset taken from NREL EULP dataset.\n",
    "The first one is trained using only 5_000 sample for each state both for residential/commercial form NREL EULP dataset.\n",
    "The second one is trained using 10_000 sample.\n",
    "\n",
    "I was not able to complete the training (time constraint) using the additional dataset with 10_000 additional sample for the following target : in.geometry_floor_area_res, in.income_res, in.roof_material_res, in.vintage_res, in.weekday_operating_hours..hr_com. For these target i used model trained only on 5_000 additional sample for each State.\n",
    "\n",
    "To reproduce the full solution:\n",
    "\n",
    "### Training with 5_000 sample\n",
    "\n",
    "Update the file ./config/params_lgb.json and set:\n",
    "\n",
    "- learning_rate: 0.05,\n",
    "- n_round: 2000\n",
    "- experiment_name: \"add_5000\"\n",
    "\n",
    "Training with 5_000 additional sample (estimated training time 7 days):\n",
    "```\n",
    "python create_additional_data.py --sample 5000\n",
    "python preprocess.py --add\n",
    "python train.py\n",
    "```\n",
    "\n",
    "### Training with 10_0000 sample\n",
    "\n",
    "Update the file ./config/params_lgb.json and set:\n",
    "\n",
    "- learning_rate: 0.15,\n",
    "- n_round: 800\n",
    "- experiment_name: \"add_10000\"\n",
    "\n",
    "Training with 10_000 additional sample (estimated training time 3-5 days):\n",
    "```\n",
    "python create_additional_data.py\n",
    "python preprocess.py --add\n",
    "python train.py\n",
    "```\n",
    "\n",
    "### Create Solution Folder\n",
    "As the final submission is defined by two different categories (due to time constraint) of model:\n",
    "\n",
    "- Model trained on 5_000 additional sample for in.geometry_floor_area_res, in.income_res, in.roof_material_res, in.vintage_res, in.weekday_operating_hours..hr_com.\n",
    "- Model trained on 10_000 for each other target.\n",
    "\n",
    "Create a folder called \"solution_lgb\" under ./experiment and place each subfolder (one for each target). The 4 target (in.geometry_floor_area_res, in.income_res, in.roof_material_res, in.vintage_res, in.weekday_operating_hours..hr_com) must come from add_5000, the other from add_10000.\n",
    "Update the file ./config/params_lgb.json experiment_name and set \"solution\" and go the Inference Step.\n",
    "\n",
    "### Additional Notes\n",
    "If the training for the bigger dataset (10_000) was completed. To reproduce the solution is just necessary to:\n",
    "\n",
    "Set in .config/params_lgb.json:\n",
    "- learning_rate: 0.05,\n",
    "- n_round: 2000\n",
    "- experiment_name: \"solution\"\n",
    "\n",
    "Run:\n",
    "```\n",
    "python create_additional_data.py\n",
    "python preprocess.py --add\n",
    "python train.py\n",
    "```\n",
    "This solution will score a little higher respect to my last submission.\n",
    "\n",
    "Double training is necessary to reproduce the current submitted solution which miss 4 target on the +10_000 dataset.\n",
    "\n",
    "\n",
    "Required Memory for gold dataset + model checkpoint: 70gb+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference step\n",
    "To inference the model and form a predictions for holdout dataset please follow the instructions below.\n",
    "\n",
    "Place every test parquet files under ORIGINAL_TEST_CHUNK_FOLDER.\n",
    "experiment_name must be set as \"solution\" in ./config/params_lgb.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this step concatenate all test file and create silver dataset\n",
    "!python script/create_test_data.py\n",
    "#this step create economics dataset -> this step is not necessary and is commented as i already provided the preprocessed file macro_economics_data.parquet\n",
    "# !python script/create_economics_dataset.py\n",
    "#execute preprocessing on test set\n",
    "!python preprocess.py --inference\n",
    "#execute inference and generate submission.parquet\n",
    "!python inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission will be saved to ./experiment/solution_lgb/submission.parquet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
