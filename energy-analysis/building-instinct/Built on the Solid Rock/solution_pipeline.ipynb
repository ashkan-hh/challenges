{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Solution Overview\n","\n","## The Environment\n","\n","- The Jupyter notebook runs in a Python 3.10.14 environment. The code runs on a CPU runtime type, lasting less than 15 hours (excluding hyperparameter search and feature selection).\n","\n","- The details of the operating system on which the notebook run is Linux-5.15.154+-x86_64-with-glibc2.31\n","\n","- The RAM usage recorded throughout the run of the notebook was less than 30gb.\n","\n","## The Code\n","\n","- This notebook takes input and uses only the data files provided in the competition resources.\n","\n","- At the end of the processes, it outputs 'scv_catselect_2xtsf.parquet' as the submission file.\n","\n","- The methodology used is to predict building features in two stages as required by the heirarchical nature of the task. Thus, an ensemble of [5] LightGBM classifiers (trained on different folds of the training data) is built to identify the building stock type and then depending on the identified building stock type, additional characteristics of each building are then predicted by a set of LightGBM classifier ensembles (5 classifiers per ensemble).\n","\n","- The General Overview of the Notebook is as follows:\n","\n","   1. Importing relevant methods from the following libraries:\n","\n","      - scipy==1.14.1\n","      - numpy==1.26.4\n","      - pandas==2.2.2\n","      - lightgbm==4.2.0\n","      - scikit-learn==1.2.2\n","      - category-encoders==2.6.3\n","      - tsfresh==0.20.3\n","\n","   2. Setting Paths\n","\n","      - Indicating the location of the input files to be used in the notebook.\n","\n","      - These paths are stored in the following variables:\n","          - `TEST_DIR` : path to the folder contain parquet files of the test data buildings.\n","          - `TRAIN_DIR` : path to the folder contain parquet files of the train data buildings\n","          - `SS_FILE` : path to the parquet file showing how a submission should be formatted.\n","          - `LABEL_FILE` : path to the parquet file containing attributes (meta data) of each building in the train dataset\n","\n","   3. Data Preprocessing and Feature Engineering\n","\n","      - Data is read, preprocessed and features engineered in four different phases.\n","\n","      - Phase 1: This phase extracts statistics of electricity consumption based on datetime features. This includes hour, day and month of peak consumption. Statistics such as difference between average weekend and average weekday consumption is calculated in addition to rolling window statistics.\n","\n","      - Phase 2: An automated feature engineering process applying tsfresh extraction to a quarter (full data was not used because of resource constraints) of the time series data of each train data building.\n","\n","      - Phase 3: This is the second manual feature engineering process that creates a set of 96 features each to represent consumption during weekends and consumption during weekdays. The aggregation is achieved by averaging consumption across all the 96 15-minute time blocks for each day (weekend days in one set and weekdays in the other set). This phase also calculates additional statistics based on these aggregates such as the number of outliers per set of 15-minute consumption values.\n","\n","      - Phase 4: This second automated feature engineering process applies the tsfresh extraction process to the two sets of timeseries data created in Phase 3 for each building.\n","\n","   4. Tackling Heirarchy 1 - Building Stock Type Classification\n","\n","      - A preprocessing pipeline is created to encode non-numeric features using a `TargetEncoder` object with the next step being the scaling of features using `MinMaxScaler`\n","\n","      - Feature selection using `select_features` method of `CatboostClassifier` with the `RecursiveByShapValues` algorithm. This feature selection is omitted from this notebook but the results are applied to reduce the huge feature dimension and improve model performance.\n","\n","      - Hyperparameter optimisation using `optuna` to select best hyperparameters of `LGBMClassifier` necessary for predicting building stock type. This step is also omitted from this notebook due to its time-consuming nature.\n","\n","      - Fitting 5 different `LGBMClassifier` models one each on each of 5 folds of the training data. These 5 models will be ensembled during inference by selecting the mode of their individual predictions.\n","\n","   5. Tackling Hierarchy 2 - Commercial Building Models\n","\n","      - The preprocessed data with engineered features is filtered to create a dataframe of commercial buildings only.\n","\n","      - This filtered dataframe is manipulated such that individual characteristics of commercial buildings to be predicted are transformed from being in individual target columns into a single target column along with a `target_type` column indicating the kind of characteristic being predicted. This is done using the `create_combo_task` function.\n","\n","      - A dictionary is created to store the index of the features relevant for predicting each kind of characteristic (`target_type`). These indices are obtained from a feature selection process using the `select_features` method of `CatboostClassifier` with the `RecursiveByShapValues` algorithm. This feature selection is omitted from this notebook but the results are applied to reduce the huge feature dimension and improve model performance.\n","\n","      - A dictionary is created to store the optimised hyperparameters of the `LGBMClassifier` model for predicting the values of each kind of characteristic (`target_type`). These hyperparameter values are obtained from an optuna hyperparameter search process which has been omitted from this notebook because it is time-intensive.\n","\n","      - Finally we iterate through each commercial building characteristic's data, preprocess it, select relevant features, and fit one `LGBMClassifier` model on each of 5 folds of the data.\n","\n","      - The model performance is evaluated and all the relevant objects are stored to be used in predicting on final test data.\n","\n","   6. Tackling Hierarchy 2 - Residential Building Models\n","\n","      - The preprocessed data with engineered features is filtered to create a dataframe of residential buildings only.\n","\n","      - This filtered dataframe is manipulated such that individual characteristics of residential buildings to be predicted are transformed from being in individual target columns into a single target column along with a `target_type` column indicating the kind of characteristic whose values are being predicted. This is done using the `create_combo_task` function.\n","\n","      - A dictionary is created to store the index of the features relevant for predicting each kind of characteristic (`target_type`). These indices are obtained from a feature selection process using the `select_features` method of `CatboostClassifier` with the `RecursiveByShapValues` algorithm. This feature selection is omitted from this notebook but the results are applied to reduce the huge feature dimension and improve model performance.\n","\n","      - A dictionary is created to store the optimised hyperparameters of the `LGBMClassifier` model for predicting the values of each kind of characteristic (`target_type`). These hyperparameter values are obtained from an optuna hyperparameter search process which has been omitted from this notebook because it is time-intensive.\n","\n","      - Finally we iterate through each residential building characteristic's data, preprocess it, select relevant features, and fit one `LGBMClassifier` model on each of 5 folds of the data.\n","\n","      - The model performance is evaluated and all the relevant objects are stored to be used in predicting on final test data\n","\n","   7. Predicting on Testset and Preparing Submission\n","\n","      - The data for the test set of buildings is read, preprocessed and features engineered in the same fashion of 4 phases like the training data.\n","\n","      - The 5 models developed for classifying the building stock type take turns in predicting the building stock type of the buildings in the test set with the modal prediction value being adopted as the final prediction.\n","\n","      - Based upon each building's predicted stock type, it is taken through the set of ensemble models for predicting the additional characteristics of either residential or commercial buildings.\n","\n","      - The prediction dataframe of the residential and commercial buildings are combined and manipulated to fit the format of the sample submission file.\n","\n","      - The formatted prediction dataframe is exported to the `scv_catselect_2xtsf.parquet` parquet file for submission.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Importing Relevant Modules and Specifying Directories"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T13:03:35.682104Z","iopub.status.busy":"2024-10-08T13:03:35.681575Z","iopub.status.idle":"2024-10-08T13:03:43.295898Z","shell.execute_reply":"2024-10-08T13:03:43.291290Z","shell.execute_reply.started":"2024-10-08T13:03:35.682058Z"},"trusted":true},"outputs":[],"source":["import os\n","from multiprocessing import Pool\n","from functools import partial\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n","from sklearn.metrics import f1_score\n","\n","from category_encoders import TargetEncoder\n","from lightgbm import LGBMClassifier\n","from scipy.stats import skew, kurtosis\n","from tsfresh.feature_extraction import extract_features, EfficientFCParameters"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T13:03:43.312142Z","iopub.status.busy":"2024-10-08T13:03:43.310970Z","iopub.status.idle":"2024-10-08T13:03:43.323043Z","shell.execute_reply":"2024-10-08T13:03:43.321019Z","shell.execute_reply.started":"2024-10-08T13:03:43.312072Z"},"trusted":true},"outputs":[],"source":["TEST_DIR = \"\"\n","TRAIN_DIR = \"\"\n","SS_FILE = \"\"\n","LABEL_FILE = \"\""]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T13:03:43.327671Z","iopub.status.busy":"2024-10-08T13:03:43.325341Z","iopub.status.idle":"2024-10-08T13:03:43.344435Z","shell.execute_reply":"2024-10-08T13:03:43.342928Z","shell.execute_reply.started":"2024-10-08T13:03:43.327569Z"},"trusted":true},"outputs":[],"source":["def list_files_recursive(root_dir: str, n: int = 5) -> list[str]:\n","    \"\"\"\n","    Recursively lists paths to parquet files within a given directory.\n","    \n","    Parameters\n","    ----------\n","    root_dir : str\n","        The path to the root directory where the search for parquet files will start.\n","        \n","    n : int, optional\n","        The maximum number of parquet file paths to return. Default is 5.\n","    \n","    Returns\n","    -------\n","    all_files : list[str]\n","        A list of parquet file paths found within the 'root_dir' directory.\n","        \n","    Notes\n","    -----\n","    The function will traverse all subdirectories of the given 'root_dir' and return\n","    the paths to at most 'n' parquet files.\n","    \"\"\"\n","    all_files = []\n","\n","    for dirpath, _, filenames in os.walk(root_dir):\n","        for ind, filename in enumerate(filenames):\n","            if ind < n and filename.endswith('.parquet'):\n","                all_files.append(os.path.join(dirpath, filename))\n","            elif ind >= n:\n","                break\n","\n","    return all_files"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preprocessing and Feature Engineering"]},{"cell_type":"markdown","metadata":{},"source":["## Manual Feature Engineering - Datetime and General Statistics"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T13:03:43.350143Z","iopub.status.busy":"2024-10-08T13:03:43.349159Z","iopub.status.idle":"2024-10-08T13:03:43.382916Z","shell.execute_reply":"2024-10-08T13:03:43.381468Z","shell.execute_reply.started":"2024-10-08T13:03:43.350082Z"},"trusted":true},"outputs":[],"source":["def extract_datetime_features(\n","    df, datetime_column=\"timestamp\", consumption_column=\"cons\"\n","):\n","    \"\"\"\n","    Extracts various time-based features from a datetime column, including:\n","    year, month, day, day of the week, is_weekend, quarter, hour, and cyclic time features\n","    (sin and cos transformations of time attributes). Additionally, it generates lag and\n","    rolling window features for the specified consumption column.\n","\n","    Parameters\n","    ----------\n","    df : pd.DataFrame\n","        The input DataFrame containing the datetime and consumption columns.\n","\n","    datetime_column : str, optional\n","        The name of the datetime column from which features are extracted, by default 'timestamp'.\n","\n","    consumption_column : str, optional\n","        The name of the column representing consumption values, used for lag and rolling window \n","        features, by default 'cons'.\n","\n","    Returns\n","    -------\n","    pd.DataFrame\n","        A DataFrame with the original columns and new engineered features based on \n","        the datetime column.\n","\n","    Notes\n","    -----\n","    - Cyclic transformations (sin, cos) are applied to month, day of the week, and hour to handle \n","    their circular nature.\n","    - Lags and rolling window features are generated for a specified consumption column.\n","    \"\"\"\n","\n","    # Ensure the datetime column is in datetime format\n","    df[datetime_column] = pd.to_datetime(df[datetime_column])\n","\n","    # Create a DataFrame to store the engineered features\n","    features_df = pd.DataFrame(index=df.index)\n","\n","    # Extract string prefix from datetime column name for feature naming\n","    prefix = datetime_column[:3]\n","\n","    # Date features\n","    features_df[f\"{prefix}_year\"] = df[datetime_column].dt.year\n","    features_df[f\"{prefix}_month\"] = df[datetime_column].dt.month\n","    features_df[f\"{prefix}_day\"] = df[datetime_column].dt.day\n","    features_df[f\"{prefix}_day_of_week\"] = df[datetime_column].dt.dayofweek\n","    features_df[f\"{prefix}_is_weekend\"] = (df[datetime_column].dt.dayofweek > 4).astype(\n","        int\n","    )\n","    features_df[f\"{prefix}_quarter\"] = df[datetime_column].dt.quarter\n","    features_df[f\"{prefix}_week_of_year\"] = df[datetime_column].dt.isocalendar().week\n","    features_df[f\"{prefix}_is_month_start\"] = df[\n","        datetime_column\n","    ].dt.is_month_start.astype(int)\n","    features_df[f\"{prefix}_is_month_end\"] = df[datetime_column].dt.is_month_end.astype(\n","        int\n","    )\n","    features_df[f\"{prefix}_is_quarter_start\"] = df[\n","        datetime_column\n","    ].dt.is_quarter_start.astype(int)\n","    features_df[f\"{prefix}_is_quarter_end\"] = df[\n","        datetime_column\n","    ].dt.is_quarter_end.astype(int)\n","    features_df[f\"{prefix}_is_year_start\"] = df[\n","        datetime_column\n","    ].dt.is_year_start.astype(int)\n","    features_df[f\"{prefix}_is_year_end\"] = df[datetime_column].dt.is_year_end.astype(\n","        int\n","    )\n","    features_df[f\"{prefix}_days_in_month\"] = df[\n","        datetime_column\n","    ].dt.days_in_month.astype(int)\n","\n","    # Time features\n","    features_df[f\"{prefix}_hour\"] = df[datetime_column].dt.hour\n","    features_df[f\"{prefix}_minute\"] = df[datetime_column].dt.minute\n","    features_df[f\"{prefix}_second\"] = df[datetime_column].dt.second\n","    features_df[f\"{prefix}_is_morning\"] = (\n","        (df[datetime_column].dt.hour >= 6) & (df[datetime_column].dt.hour < 12)\n","    ).astype(int)\n","    features_df[f\"{prefix}_is_afternoon\"] = (\n","        (df[datetime_column].dt.hour >= 12) & (df[datetime_column].dt.hour < 18)\n","    ).astype(int)\n","    features_df[f\"{prefix}_is_evening\"] = (\n","        (df[datetime_column].dt.hour >= 18) & (df[datetime_column].dt.hour < 24)\n","    ).astype(int)\n","    features_df[f\"{prefix}_is_night\"] = (\n","        (df[datetime_column].dt.hour >= 0) & (df[datetime_column].dt.hour < 6)\n","    ).astype(int)\n","\n","    # Cyclic time features\n","    features_df[f\"{prefix}_month_sin\"] = np.sin(\n","        2 * np.pi * features_df[f\"{prefix}_month\"] / 12\n","    )\n","    features_df[f\"{prefix}_month_cos\"] = np.cos(\n","        2 * np.pi * features_df[f\"{prefix}_month\"] / 12\n","    )\n","    features_df[f\"{prefix}_day_of_week_sin\"] = np.sin(\n","        2 * np.pi * features_df[f\"{prefix}_day_of_week\"] / 7\n","    )\n","    features_df[f\"{prefix}_day_of_week_cos\"] = np.cos(\n","        2 * np.pi * features_df[f\"{prefix}_day_of_week\"] / 7\n","    )\n","    features_df[f\"{prefix}_hour_sin\"] = np.sin(\n","        2 * np.pi * features_df[f\"{prefix}_hour\"] / 24\n","    )\n","    features_df[f\"{prefix}_hour_cos\"] = np.cos(\n","        2 * np.pi * features_df[f\"{prefix}_hour\"] / 24\n","    )\n","\n","    # Lag features (e.g., for 1 to 7-day lag)\n","    for lag in range(1, 8):\n","        features_df[f\"{prefix}_lag_{lag}\"] = df[consumption_column].shift(lag)\n","\n","    # Rolling window features (e.g., 7-day rolling window)\n","    window_size = 7\n","    features_df[f\"{prefix}_rolling_mean_{window_size}\"] = (\n","        df[consumption_column].rolling(window=window_size).mean()\n","    )\n","    features_df[f\"{prefix}_rolling_std_{window_size}\"] = (\n","        df[consumption_column].rolling(window=window_size).std()\n","    )\n","    features_df[f\"{prefix}_rolling_min_{window_size}\"] = (\n","        df[consumption_column].rolling(window=window_size).min()\n","    )\n","    features_df[f\"{prefix}_rolling_max_{window_size}\"] = (\n","        df[consumption_column].rolling(window=window_size).max()\n","    )\n","\n","    # Concatenate the new features with the original dataframe (excluding the datetime column)\n","    result_df = pd.concat([features_df, df.drop([datetime_column], axis=1)],\n","                           axis=1)\n","\n","    # Set the datetime column as the index\n","    result_df = result_df.set_index(df[datetime_column])\n","\n","    return result_df\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T13:03:43.385859Z","iopub.status.busy":"2024-10-08T13:03:43.385180Z","iopub.status.idle":"2024-10-08T13:03:43.403479Z","shell.execute_reply":"2024-10-08T13:03:43.402060Z","shell.execute_reply.started":"2024-10-08T13:03:43.385790Z"},"trusted":true},"outputs":[],"source":["def quantile_25(growth_vals: pd.Series) -> float:\n","    \"\"\"\n","    Calculate the 1st quartile (25th percentile) of a series of values.\n","\n","    Parameters\n","    ----------\n","    growth_vals : pd.Series\n","        A pandas Series containing numerical values for which the 1st quartile \n","        (25th percentile) is to be calculated.\n","\n","    Returns\n","    -------\n","    float\n","        The 25th percentile (1st quartile) of the input values.\n","    \"\"\"\n","    q25 = growth_vals.quantile(0.25)\n","    return q25\n","\n","\n","def quantile_75(growth_vals: pd.Series) -> float:\n","    \"\"\"\n","    Calculate the 3rd quartile (75th percentile) of a series of values.\n","\n","    Parameters\n","    ----------\n","    growth_vals : pd.Series\n","        A pandas Series containing numerical values for which the 3rd quartile \n","        (75th percentile) is to be calculated.\n","\n","    Returns\n","    -------\n","    float\n","        The 75th percentile (3rd quartile) of the input values.\n","    \"\"\"\n","    q75 = growth_vals.quantile(0.75)\n","    return q75\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T13:03:43.405852Z","iopub.status.busy":"2024-10-08T13:03:43.405299Z","iopub.status.idle":"2024-10-08T13:03:43.419938Z","shell.execute_reply":"2024-10-08T13:03:43.418450Z","shell.execute_reply.started":"2024-10-08T13:03:43.405795Z"},"trusted":true},"outputs":[],"source":["def get_flat(df, reference, prefix):\n","    \"\"\"\n","    Flatten a pandas DataFrame with a multi-level index and columns, and return a dictionary\n","    with keys generated from the column names and index values, prefixed as specified.\n","\n","    Parameters\n","    ----------\n","    df : pandas.DataFrame\n","        The input DataFrame with a multi-level index and columns to be flattened.\n","    reference : str\n","        A string used to filter out dictionary keys that start with this reference.\n","    prefix : str\n","        A string to prepend to each key in the resulting dictionary.\n","\n","    Returns\n","    -------\n","    dict\n","        A dictionary where keys are of the form 'prefix_columnname_index' and values are the\n","        corresponding DataFrame values. Keys that start with the `reference` string are excluded\n","        from the output.\n","\n","    Examples\n","    --------\n","    >>> import pandas as pd\n","    >>> df = pd.DataFrame({\n","    ...     ('A', 'X'): [1, 2],\n","    ...     ('A', 'Y'): [3, 4],\n","    ...     ('B', 'X'): [5, 6],\n","    ... }, index=['row1', 'row2'])\n","    >>> get_flat(df, 'A', 'prefix')\n","    {'prefix_B_X_row1': 5, 'prefix_B_X_row2': 6}\n","\n","    Notes\n","    -----\n","    The function is designed to work with DataFrames that have multi-level (hierarchical) \n","    column labels and a single-level index. It constructs new dictionary keys by concatenating \n","    the column names and index, and excludes any keys that begin with the `reference` string.\n","    \"\"\"\n","\n","    # Set the index name for the DataFrame to 'cons'\n","    df.index.name = \"cons\"\n","\n","    # Initialize an empty dictionary to store flattened key-value pairs\n","    flat_dict = {}\n","\n","    # Iterate over each index and column of the DataFrame\n","    for idx in df.index:\n","        for col in df.columns:\n","            # Flatten the multi-level column names into a single string\n","            col_name = \"_\".join([str(x) for x in col if x])\n","\n","            # Generate the key by combining the column name and index\n","            key = f\"{col_name}_{idx}\"\n","\n","            # Add the value from the DataFrame to the flat_dict\n","            flat_dict[key] = df.at[idx, col]\n","\n","    # Add prefix to the keys and filter out keys that start with the reference string\n","    flat_dict = {\n","        f\"{prefix}_{k}\": v for k, v in flat_dict.items() if not k.startswith(reference)\n","    }\n","\n","    return flat_dict\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T13:03:43.423046Z","iopub.status.busy":"2024-10-08T13:03:43.422044Z","iopub.status.idle":"2024-10-08T13:03:43.557841Z","shell.execute_reply":"2024-10-08T13:03:43.556041Z","shell.execute_reply.started":"2024-10-08T13:03:43.422996Z"},"trusted":true},"outputs":[],"source":["# reading data of building attributes and metadata\n","labels = pd.read_parquet(LABEL_FILE)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T13:03:43.562058Z","iopub.status.busy":"2024-10-08T13:03:43.560526Z","iopub.status.idle":"2024-10-08T13:03:43.597664Z","shell.execute_reply":"2024-10-08T13:03:43.596259Z","shell.execute_reply.started":"2024-10-08T13:03:43.561994Z"},"trusted":true},"outputs":[],"source":["def process_file(file):\n","    \"\"\"\n","    Process a parquet file containing building energy consumption data and extract various\n","    statistical and time-based features for analysis.\n","\n","    Parameters\n","    ----------\n","    file : str\n","        Path to the parquet file containing building energy consumption data.\n","\n","    Returns\n","    -------\n","    dict\n","        A dictionary containing various features such as consumption statistics, peak periods,\n","        seasonal means, and advanced rolling statistics.\n","\n","    Notes\n","    -----\n","    - The function reads the parquet file into a DataFrame and renames the main electricity\n","      consumption column to 'cons'.\n","    - Various statistics like mean, standard deviation, range, variance, etc. are calculated.\n","    - Peak consumption periods (hour, day, month) are identified and features are extracted\n","      based on these periods.\n","    - Additional rolling statistics and pivot tables are computed for different time periods.\n","    - The function returns a dictionary containing all the calculated features.\n","    \"\"\"\n","\n","    # Read the parquet file\n","    df = pd.read_parquet(file)\n","\n","    # Rename the main electricity consumption column to 'cons'\n","    value = \"out.electricity.total.energy_consumption\"\n","    df = df.rename(columns={value: \"cons\"})\n","    value = \"cons\"\n","\n","    # Extract building ID and calculate consumption statistics\n","    build_id = df.first_valid_index()  # building id\n","    cons_std = df[value].std()\n","    cons_mean = df[value].mean()\n","    cons_min = df[value].min()\n","    cons_max = df[value].max()\n","    cons_range = cons_max - cons_min\n","    cons_var = df[value].var()\n","    peak_to_avg_ratio = cons_max / cons_mean\n","\n","    # Extract datetime features\n","    df_datetime = extract_datetime_features(df)\n","    df_datetime.index = pd.to_datetime(df_datetime.index)\n","\n","    # Identify peak consumption hours, days, and months\n","    peak_hours = df_datetime[df_datetime[value] == cons_max].index.hour\n","    peak_hour = peak_hours[0] if len(peak_hours) > 0 else None\n","    peak_hourcount = len(peak_hours)\n","\n","    peak_days = df_datetime[df_datetime[value] == cons_max].index.dayofweek\n","    peak_day = peak_days[0] if len(peak_days) > 0 else None\n","    peak_daycount = len(peak_days)\n","\n","    peak_months = df_datetime[df_datetime[value] == cons_max].index.month\n","    peak_month = peak_months[0] if len(peak_months) > 0 else None\n","    peak_monthcount = len(peak_months)\n","\n","    # Determine if the peak day is a weekend (1 for weekend, 0 for weekday)\n","    peak_is_weekend = 1 if peak_day > 4 else 0\n","\n","    # Classify the peak consumption time into morning, afternoon, evening, or night\n","    if (peak_hour >= 6) and (peak_hour < 12):\n","        peak_timeofday = \"m\"\n","    elif (peak_hour >= 12) and (peak_hour < 18):\n","        peak_timeofday = \"a\"\n","    elif (peak_hour >= 18) and (peak_hour < 24):\n","        peak_timeofday = \"e\"\n","    elif (peak_hour >= 0) and (peak_hour < 6):\n","        peak_timeofday = \"n\"\n","\n","    # Calculate consumption differences, skewness, and kurtosis\n","    cons_diff_mean = df[value].diff().mean()\n","    cons_diff_std = df[value].diff().std()\n","    cons_skew = df[value].skew()\n","    cons_kurt = df[value].kurtosis()\n","\n","    # Calculate average consumption for weekdays and weekends\n","    weekday_mean = df_datetime[df_datetime[\"tim_is_weekend\"] == 0][value].mean()\n","    weekend_mean = df_datetime[df_datetime[\"tim_is_weekend\"] == 1][value].mean()\n","    weekend_diff = weekend_mean - weekday_mean\n","\n","    # Calculate seasonal mean consumption values\n","    spring_mean = df_datetime[\n","        (df_datetime[\"tim_month\"] >= 3) & (df_datetime[\"tim_month\"] <= 5)\n","    ][value].mean()\n","    summer_mean = df_datetime[\n","        (df_datetime[\"tim_month\"] >= 6) & (df_datetime[\"tim_month\"] <= 8)\n","    ][value].mean()\n","    autumn_mean = df_datetime[\n","        (df_datetime[\"tim_month\"] >= 9) & (df_datetime[\"tim_month\"] <= 11)\n","    ][value].mean()\n","    winter_mean = df_datetime[\n","        (df_datetime[\"tim_month\"] == 12) | (df_datetime[\"tim_month\"] <= 2)\n","    ][value].mean()\n","\n","    # Extract the building's state\n","    build_state = df.iloc[0, -1]  # state where building is found\n","\n","    # Initialize the overall dictionary to store all computed features\n","    overall_dict = {\n","        \"bldg_id\": build_id,\n","        \"cons_range\": cons_range,\n","        \"cons_std\": cons_std,\n","        \"cons_mean\": cons_mean,\n","        \"cons_min\": cons_min,\n","        \"cons_max\": cons_max,\n","        \"cons_var\": cons_var,\n","        \"peak_to_avg_ratio\": peak_to_avg_ratio,\n","        \"peak_hour\": peak_hour,\n","        \"peak_day\": peak_day,\n","        \"peak_is_weekend\": peak_is_weekend,\n","        \"peak_timeofday\": peak_timeofday,\n","        \"peak_month\": peak_month,\n","        \"peak_hourcount\": peak_hourcount,\n","        \"peak_daycount\": peak_daycount,\n","        \"peak_monthcount\": peak_monthcount,\n","        \"cons_diff_mean\": cons_diff_mean,\n","        \"cons_diff_std\": cons_diff_std,\n","        \"cons_skew\": cons_skew,\n","        \"cons_kurt\": cons_kurt,\n","        \"weekday_mean\": weekday_mean,\n","        \"weekend_mean\": weekend_mean,\n","        \"weekend_diff\": weekend_diff,\n","        \"spring_mean\": spring_mean,\n","        \"summer_mean\": summer_mean,\n","        \"autumn_mean\": autumn_mean,\n","        \"winter_mean\": winter_mean,\n","        \"build_state\": build_state,\n","    }\n","\n","    # Aggregations by time periods\n","    periods = [\"weekend\", \"afternoon\", \"morning\", \"evening\", \"night\"]\n","    time_conditions = {\n","        \"weekend\": \"tim_is_weekend\",\n","        \"afternoon\": \"tim_is_afternoon\",\n","        \"morning\": \"tim_is_morning\",\n","        \"evening\": \"tim_is_evening\",\n","        \"night\": \"tim_is_night\",\n","    }\n","\n","    for period in periods:\n","        condition = time_conditions[period]\n","        cons_by_period = pd.pivot_table(\n","            data=df_datetime,\n","            values=value,\n","            index=condition,\n","            aggfunc=[\"mean\", \"std\", quantile_25, quantile_75, \"max\", \"min\"],\n","        ).reset_index()\n","        cons_by_period_dict = get_flat(cons_by_period, condition, period)\n","        overall_dict.update(cons_by_period_dict)\n","\n","    # Advanced Rolling Statistics\n","    rolling_windows = [7, 30]  # 7-day and 30-day rolling windows\n","    for window in rolling_windows:\n","        df[f\"rolling_mean_{window}\"] = df[value].rolling(window=window).mean()\n","        df[f\"rolling_std_{window}\"] = df[value].rolling(window=window).std()\n","        df[f\"rolling_median_{window}\"] = df[value].rolling(window=window).median()\n","        df[f\"rolling_range_{window}\"] = (\n","            df[value].rolling(window=window).apply(lambda x: x.max() - x.min())\n","        )\n","        df[f\"rolling_var_{window}\"] = df[value].rolling(window=window).var()\n","        overall_dict.update(\n","            {\n","                f\"rolling_mean_{window}\": df[f\"rolling_mean_{window}\"].mean(),\n","                f\"rolling_std_{window}\": df[f\"rolling_std_{window}\"].mean(),\n","                f\"rolling_median_{window}\": df[f\"rolling_median_{window}\"].mean(),\n","                f\"rolling_range_{window}\": df[f\"rolling_range_{window}\"].mean(),\n","                f\"rolling_var_{window}\": df[f\"rolling_var_{window}\"].mean(),\n","            }\n","        )\n","\n","    return overall_dict\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T13:03:43.600810Z","iopub.status.busy":"2024-10-08T13:03:43.599841Z","iopub.status.idle":"2024-10-08T13:03:43.618702Z","shell.execute_reply":"2024-10-08T13:03:43.616561Z","shell.execute_reply.started":"2024-10-08T13:03:43.600763Z"},"trusted":true},"outputs":[],"source":["def prepare_data(folder, n_files=5, is_test=False, labels=labels):\n","    \"\"\"\n","    Prepare data by processing a folder of parquet files and extracting features from them.\n","\n","    Parameters\n","    ----------\n","    folder : str\n","        Path to the folder containing parquet files.\n","    n_files : int, optional\n","        Number of files to process, by default 5.\n","    is_test : bool, optional\n","        Indicates whether the function is being run on test data, by default False.\n","    labels : pandas.DataFrame\n","        DataFrame containing labels or metadata related to the building.\n","    \n","    Returns\n","    -------\n","    pandas.DataFrame\n","        A DataFrame containing extracted features. If `is_test` is False, the labels are merged\n","        based on the building ID.\n","    \"\"\"\n","\n","    # Retrieve list of files from the folder\n","    files = list_files_recursive(folder, n_files)\n","\n","    # Process files in parallel using multiprocessing Pool\n","    with Pool() as pool:\n","        process_file_partial = partial(process_file)\n","        dict_lists = pool.map(process_file_partial, files)\n","\n","    # Create a DataFrame from the processed features\n","    X = pd.DataFrame(dict_lists)\n","\n","    if is_test:\n","        return X\n","    else:\n","        # Merge the features with the labels\n","        return pd.merge(X, labels, on='bldg_id', how='left')\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T13:03:43.627397Z","iopub.status.busy":"2024-10-08T13:03:43.626476Z","iopub.status.idle":"2024-10-08T18:58:51.815436Z","shell.execute_reply":"2024-10-08T18:58:51.813704Z","shell.execute_reply.started":"2024-10-08T13:03:43.627348Z"},"trusted":true},"outputs":[],"source":["# lasts approx 6hours\n","train_data = prepare_data(TRAIN_DIR,n_files=7200)"]},{"cell_type":"markdown","metadata":{},"source":["## Ts_fresh Feature Engineering - Raw TimeSeries\n","In this portion of the feature engineering process, the ts_fresh library is used to\n","extract features from a quarter of each building's consumption timeseries data.\n","A quarter is used because of memory and time constraints"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T18:58:51.846357Z","iopub.status.busy":"2024-10-08T18:58:51.845744Z","iopub.status.idle":"2024-10-08T18:58:51.860425Z","shell.execute_reply":"2024-10-08T18:58:51.859013Z","shell.execute_reply.started":"2024-10-08T18:58:51.846301Z"},"trusted":true},"outputs":[],"source":["def ts_fresh_prep(folder: str, n_files: int = 3) -> pd.DataFrame:\n","    \"\"\"\n","    Prepares a combined time series dataframe from multiple building data files \n","    for feature extraction using ts_fresh.\n","\n","    This function reads time series data from a specified number of parquet files, \n","    processes the first quarter of each file's data, and structures the combined \n","    data in a format suitable for the ts_fresh feature extraction library.\n","\n","    Parameters\n","    ----------\n","    folder : str\n","        The path to the folder containing parquet files with time series data \n","        for different buildings.\n","    \n","    n_files : int, optional, default=3\n","        The number of building data files to consider for the processing.\n","        Only the first n_files in the folder will be used.\n","    \n","    Returns\n","    -------\n","    pd.DataFrame\n","        A dataframe containing time series data in ts_fresh-compatible format, \n","        with columns ['bldg_id', 'time', 'cons'].\n","        - 'bldg_id' : Building identifier for each entry.\n","        - 'time'    : Time step for the building (0-indexed per building).\n","        - 'cons'    : The total energy consumption for the respective time step.\n","    \"\"\"\n","\n","    # Get the list of files in the folder (recursively) based on the limit n_files\n","    files = list_files_recursive(folder, n_files)\n","\n","    # Read and process parquet files, renaming the energy consumption column\n","    dfs = [\n","        pd.read_parquet(file)\n","        .rename(columns={'out.electricity.total.energy_consumption': 'cons'})\n","        .reset_index(drop=False)  # Retain the original index as a separate column\n","        for file in files\n","    ]\n","\n","    # Select only the first quarter of the data from each building's dataframe\n","    dfs = [df.head(int(df.shape[0] / 4)) for df in dfs]\n","\n","    # Combine all dataframes into a single dataframe\n","    combined_df = pd.concat(dfs, ignore_index=True)\n","\n","    # Create a time column that indexes each row in order within each building's data\n","    combined_df['time'] = combined_df.groupby('bldg_id').cumcount()\n","\n","    # Select the columns required by ts_fresh: building id, time, and consumption\n","    tsf_df = combined_df[['bldg_id', 'time', 'cons']]\n","\n","    return tsf_df\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T18:58:51.862896Z","iopub.status.busy":"2024-10-08T18:58:51.862335Z","iopub.status.idle":"2024-10-08T19:00:39.711717Z","shell.execute_reply":"2024-10-08T19:00:39.710030Z","shell.execute_reply.started":"2024-10-08T18:58:51.862838Z"},"trusted":true},"outputs":[],"source":["# Also time-intensive\n","tsf_df = ts_fresh_prep(TRAIN_DIR,n_files=7200)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T19:45:31.918370Z","iopub.status.busy":"2024-10-08T19:45:31.917623Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Feature Extraction:  80%|████████  | 16/20 [4:41:06<39:19, 589.98s/it]   "]}],"source":["# Extract features \n","extracted_features = extract_features(tsf_df, column_id=\"bldg_id\", column_sort=\"time\",\n","                                     default_fc_parameters=EfficientFCParameters(),\n","                                        n_jobs=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.481219Z","iopub.status.idle":"2024-10-08T19:02:07.481761Z","shell.execute_reply":"2024-10-08T19:02:07.481528Z","shell.execute_reply.started":"2024-10-08T19:02:07.481504Z"},"trusted":true},"outputs":[],"source":["extracted_features['bldg_id'] = extracted_features.index"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.484679Z","iopub.status.idle":"2024-10-08T19:02:07.485367Z","shell.execute_reply":"2024-10-08T19:02:07.485057Z","shell.execute_reply.started":"2024-10-08T19:02:07.485025Z"},"trusted":true},"outputs":[],"source":["for col in extracted_features.columns:\n","    if extracted_features[col].isna().sum()>0:\n","        print(col,': ',extracted_features[col].isna().sum())\n","        extracted_features.drop(col,axis=1,inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.493537Z","iopub.status.idle":"2024-10-08T19:02:07.494151Z","shell.execute_reply":"2024-10-08T19:02:07.493914Z","shell.execute_reply.started":"2024-10-08T19:02:07.493888Z"},"trusted":true},"outputs":[],"source":["train_data_comp = pd.merge(train_data,extracted_features,how='left',on='bldg_id')\n","train_data_comp"]},{"cell_type":"markdown","metadata":{},"source":["## Manual Feature Engineering - 15min-Interval Condensation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.497174Z","iopub.status.idle":"2024-10-08T19:02:07.497754Z","shell.execute_reply":"2024-10-08T19:02:07.497489Z","shell.execute_reply.started":"2024-10-08T19:02:07.497463Z"},"trusted":true},"outputs":[],"source":["def get_15min_df(ndf):\n","    \"\"\"\n","    Aggregates electricity consumption data from the input time series DataFrame,\n","    computing average consumption in 15-minute intervals for both weekdays and weekends.\n","\n","    The output consists of two sets of 96 columns:\n","    - One set for the average consumption during weekdays (15-minute intervals across weekdays).\n","    - One set for the average consumption during weekends (15-minute intervals across weekends).\n","\n","    Parameters\n","    ----------\n","    ndf : pd.DataFrame\n","        DataFrame containing time series data for the electricity consumption of a single building.\n","        The DataFrame must contain the following columns:\n","        - 'timestamp' : datetime column indicating the time of the observation.\n","        - 'cons' : electricity consumption value at that time.\n","        - 'bldg_id' : identifier of the building.\n","\n","    Returns\n","    -------\n","    pd.DataFrame\n","        A DataFrame with one row:\n","        - 96 columns representing average weekday consumption per 15-minute interval.\n","        - 96 columns representing average weekend consumption per 15-minute interval.\n","        - A 'bldg_id' column indicating the building to which the data pertains.\n","\n","    Notes\n","    -----\n","    Each 15-minute interval is represented as a unique column. Weekdays are considered Monday\n","    through Friday, while weekends are Saturday and Sunday.\n","    \"\"\"\n","\n","    # Make a copy of the input DataFrame to avoid modifying the original data\n","    df = ndf.copy()\n","\n","    # Ensure 'timestamp' column is in datetime format\n","    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n","\n","    # Create a column for 15-minute intervals (0 to 95) based on the time of day\n","    df[\"15min_chunk\"] = (df[\"timestamp\"].dt.hour * 60 + df[\"timestamp\"].dt.minute) // 15\n","\n","    # Create a boolean column to distinguish between weekdays (False) and weekends (True)\n","    df[\"is_weekend\"] = (\n","        df[\"timestamp\"].dt.weekday >= 5\n","    )  # True for Saturday (5) and Sunday (6)\n","\n","    # Group data by 15-minute intervals and whether it's a weekend, then calculate mean consumption\n","    grouped = df.groupby([\"15min_chunk\", \"is_weekend\"])[\"cons\"].mean().reset_index()\n","\n","    # Initialize an empty dictionary to store results\n","    result = {}\n","\n","    # Iterate over all possible 15-minute chunks (0-95)\n","    for chunk in range(96):\n","        # Define column names for weekdays and weekends\n","        weekday_col = f\"{chunk}_15_weekday\"\n","        weekend_col = f\"{chunk}_15_weekend\"\n","\n","        # Extract mean consumption values for weekdays and weekends for the current chunk\n","        weekday_value = grouped[\n","            (grouped[\"15min_chunk\"] == chunk) & (grouped[\"is_weekend\"] == False)\n","        ][\"cons\"].values\n","        weekend_value = grouped[\n","            (grouped[\"15min_chunk\"] == chunk) & (grouped[\"is_weekend\"] == True)\n","        ][\"cons\"].values\n","\n","        # Handle cases where no data exists for a given chunk\n","        result[weekday_col] = weekday_value if len(weekday_value) > 0 else [None]\n","        result[weekend_col] = weekend_value if len(weekend_value) > 0 else [None]\n","\n","    # Convert result dictionary into a DataFrame\n","    result = pd.DataFrame(result)\n","\n","    # Add a column for the building ID\n","    result[\"bldg_id\"] = df[\"bldg_id\"][0]\n","\n","    return result\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.499082Z","iopub.status.idle":"2024-10-08T19:02:07.499703Z","shell.execute_reply":"2024-10-08T19:02:07.499442Z","shell.execute_reply.started":"2024-10-08T19:02:07.499415Z"},"trusted":true},"outputs":[],"source":["def calculate_iqr_outliers(series):\n","    \"\"\"\n","    Calculate the Interquartile Range (IQR) and use it to determine outlier thresholds.\n","    \n","    The IQR is the range between the first (25th percentile) and third quartile (75th percentile)\n","    values of the series. This function returns the lower and upper bounds for identifying\n","    potential outliers based on 0.5 * IQR.\n","\n","    Parameters:\n","    -----------\n","    series : pandas.Series\n","        A pandas Series object containing numerical data.\n","\n","    Returns:\n","    --------\n","    lower_bound : float\n","        The lower threshold for outlier detection (values below this are considered outliers).\n","    \n","    upper_bound : float\n","        The upper threshold for outlier detection (values above this are considered outliers).\n","    \"\"\"\n","\n","    # Calculate the 25th percentile (Q1) and 75th percentile (Q3) of the data\n","    q1 = series.quantile(0.25)\n","    q3 = series.quantile(0.75)\n","\n","    # Compute the Interquartile Range (IQR)\n","    iqr = q3 - q1\n","\n","    # Calculate the lower and upper bounds for outliers (using 0.5 * IQR)\n","    lower_bound = q1 - 0.5 * iqr\n","    upper_bound = q3 + 0.5 * iqr\n","\n","    return lower_bound, upper_bound\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.503125Z","iopub.status.idle":"2024-10-08T19:02:07.503698Z","shell.execute_reply":"2024-10-08T19:02:07.503440Z","shell.execute_reply.started":"2024-10-08T19:02:07.503414Z"},"trusted":true},"outputs":[],"source":["def prep_15min(folder, n_files=3):\n","    \"\"\"\n","    Aggregates electricity consumption data from parquet files and computes new features \n","    based on average consumption in 15-minute intervals for weekdays and weekends. The \n","    function also calculates summary statistics (e.g., mean, sum, variance) for these \n","    intervals and identifies outliers based on interquartile range (IQR).\n","    \n","    Parameters\n","    ----------\n","    folder : str\n","        Directory path where the parquet files containing the electricity consumption \n","        data are located.\n","    \n","    n_files : int, optional\n","        The number of parquet files (buildings) to process, by default 3.\n","    \n","    Returns\n","    -------\n","    pd.DataFrame\n","        DataFrame containing manually engineered features, including statistics and outlier counts \n","        for 15-minute interval consumption on weekdays and weekends.\n","    \n","    Notes\n","    -----\n","    - The parquet files should contain a column named 'out.electricity.total.energy_consumption', \n","      which represents the total electricity consumption.\n","    - The `get_15min_df` function should compute average consumption in 15-minute \n","      intervals for both weekdays and weekends.\n","    - The `calculate_iqr_outliers` function calculates lower and upper IQR thresholds \n","    to identify outliers.\n","    \"\"\"\n","\n","    # Get a list of files to process from the specified folder\n","    files = list_files_recursive(folder, n_files)\n","\n","    # Read each parquet file into a DataFrame, renaming the electricity consumption column\n","    dfs = [\n","        pd.read_parquet(file)\n","        .rename(columns={'out.electricity.total.energy_consumption': 'cons'})\n","        .reset_index(drop=False)\n","        for file in files\n","    ]\n","\n","    # Apply the get_15min_df function to resample each DataFrame into 15-minute intervals\n","    all_15min = [get_15min_df(df) for df in dfs]\n","\n","    # Concatenate all 15-minute DataFrames into one\n","    comb = pd.concat(all_15min).reset_index(drop=True)\n","\n","    # Select columns corresponding to weekday and weekend consumption\n","    weekdays = [col for col in comb.columns if col.endswith('_weekday')]\n","    weekends = [col for col in comb.columns if col.endswith('_weekend')]\n","\n","    # List of statistics to calculate for each interval\n","    statistics = ['sum', 'mean', 'max', 'min', 'std', 'var', 'median']\n","\n","    # Compute statistics for weekday columns and assign new feature names\n","    comb_weekdays_stats = comb[weekdays].agg(statistics, axis=1)\n","    comb_weekdays_stats.columns = [f\"weekdays_{col}\" for col in comb_weekdays_stats.columns]\n","\n","    # Compute statistics for weekend columns and assign new feature names\n","    comb_weekends_stats = comb[weekends].agg(statistics, axis=1)\n","    comb_weekends_stats.columns = [f\"weekends_{col}\" for col in comb_weekends_stats.columns]\n","\n","    # Concatenate new statistics with the original DataFrame\n","    comb = pd.concat([comb, comb_weekdays_stats, comb_weekends_stats], axis=1)\n","\n","    # Initialize columns to track outlier counts and IQR thresholds for weekdays and weekends\n","    comb['n_below_weekday'] = 0\n","    comb['n_above_weekday'] = 0\n","    comb['n_below_weekend'] = 0\n","    comb['n_above_weekend'] = 0\n","    comb['lower_weekday'] = 0.0\n","    comb['upper_weekday'] = 0.0\n","    comb['lower_weekend'] = 0.0\n","    comb['upper_weekend'] = 0.0\n","\n","    # Calculate outliers for each row using IQR-based thresholds\n","    for idx, row in comb.iterrows():\n","        # Get weekday and weekend values for the current row\n","        weekday_values = row[weekdays]\n","        weekend_values = row[weekends]\n","\n","        # Calculate IQR-based thresholds for weekday and weekend values\n","        lower_weekday, upper_weekday = calculate_iqr_outliers(weekday_values)\n","        lower_weekend, upper_weekend = calculate_iqr_outliers(weekend_values)\n","\n","        # Count how many weekday values fall outside the IQR range\n","        n_below_weekday = (weekday_values < lower_weekday).sum()\n","        n_above_weekday = (weekday_values > upper_weekday).sum()\n","\n","        # Count how many weekend values fall outside the IQR range\n","        n_below_weekend = (weekend_values < lower_weekend).sum()\n","        n_above_weekend = (weekend_values > upper_weekend).sum()\n","\n","        # Store the counts and thresholds in the DataFrame\n","        comb.at[idx, 'n_below_weekday'] = n_below_weekday\n","        comb.at[idx, 'n_above_weekday'] = n_above_weekday\n","        comb.at[idx, 'n_below_weekend'] = n_below_weekend\n","        comb.at[idx, 'n_above_weekend'] = n_above_weekend\n","        comb.at[idx, 'lower_weekday'] = lower_weekday\n","        comb.at[idx, 'upper_weekday'] = upper_weekday\n","        comb.at[idx, 'lower_weekend'] = lower_weekend\n","        comb.at[idx, 'upper_weekend'] = upper_weekend\n","\n","    return comb\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.504975Z","iopub.status.idle":"2024-10-08T19:02:07.505430Z","shell.execute_reply":"2024-10-08T19:02:07.505235Z","shell.execute_reply.started":"2024-10-08T19:02:07.505212Z"},"trusted":true},"outputs":[],"source":["df_15min = prep_15min(TRAIN_DIR,n_files=7200)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.514509Z","iopub.status.idle":"2024-10-08T19:02:07.515214Z","shell.execute_reply":"2024-10-08T19:02:07.514905Z","shell.execute_reply.started":"2024-10-08T19:02:07.514872Z"},"trusted":true},"outputs":[],"source":["train_data_comp = pd.merge(train_data_comp,df_15min,how='left',on='bldg_id')\n","train_data_comp"]},{"cell_type":"markdown","metadata":{},"source":["## Ts_fresh Feature Engineering - 15min-interval condensed TimeSeries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.517284Z","iopub.status.idle":"2024-10-08T19:02:07.517962Z","shell.execute_reply":"2024-10-08T19:02:07.517666Z","shell.execute_reply.started":"2024-10-08T19:02:07.517632Z"},"trusted":true},"outputs":[],"source":["def df15min_tsfprep(ndf: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Prepares a time series DataFrame for a feature extraction process using the ts_fresh\n","    library. This preparation is done by melting and pivoting columns related to\n","    '15_weekday' and '15_weekend' time intervals into separate rows and columns.\n","\n","    Parameters\n","    ----------\n","    ndf : pd.DataFrame\n","        Input DataFrame containing time series data, where the columns represent\n","        values at 15-minute intervals for both weekdays and weekends. The columns\n","        should be named with a numeric prefix and suffixes '_15_weekday' or '_15_weekend'.\n","\n","    Returns\n","    -------\n","    pd.DataFrame\n","        Transformed DataFrame where:\n","        - Rows are indexed by 'bldg_id' and 'time'.\n","        - There are two columns: '15_weekday' and '15_weekend', which hold the respective\n","          time series values for weekdays and weekends.\n","\n","    Notes\n","    -----\n","    The function works under the assumption that the input DataFrame has a structure\n","    where the time series data for weekdays and weekends are stored in columns with\n","    suffixes '_15_weekday' and '_15_weekend', respectively.\n","\n","    Example\n","    -------\n","    Input:\n","    +----------+--------+--------+--------+\n","    | bldg_id  | 0_15_weekday | 0_15_weekend | ... |\n","    +----------+--------+--------+--------+\n","    | 1        | 123    | 130    | ...    |\n","\n","    Output:\n","    +----------+--------+--------+--------+\n","    | bldg_id  | time   | 15_weekday | 15_weekend |\n","    +----------+--------+--------+--------+\n","    | 1        | 0      | 123    | 130    |\n","    \"\"\"\n","\n","    # Make a copy of the input DataFrame to avoid modifying the original\n","    df = ndf.copy()\n","\n","    # Melt the DataFrame to move '15_weekday' and '15_weekend' columns into rows\n","    melted_df = df.melt(\n","        id_vars=[\"bldg_id\"],\n","        value_vars=[\n","            col for col in df.columns if \"_15_weekday\" in col or \"_15_weekend\" in col\n","        ],\n","        var_name=\"time\",\n","        value_name=\"value\",\n","    )\n","\n","    # Create a new column 'day_type' to distinguish between weekday and weekend\n","    melted_df[\"day_type\"] = melted_df[\"time\"].apply(\n","        lambda x: \"15_weekday\" if \"weekday\" in x else \"15_weekend\"\n","    )\n","\n","    # Extract the time part from the column name (numeric prefix before '_15')\n","    melted_df[\"time\"] = melted_df[\"time\"].str.extract(r\"(\\d+)_15\")[0].astype(int)\n","\n","    # Pivot the DataFrame so '15_weekday' and '15_weekend' become columns\n","    reshaped_df = melted_df.pivot_table(\n","        index=[\"bldg_id\", \"time\"], columns=\"day_type\", values=\"value\"\n","    ).reset_index()\n","\n","    # Remove the column index name and rename columns for clarity\n","    reshaped_df.columns.name = None\n","\n","    # Ensure columns are properly named\n","    reshaped_df.rename(\n","        columns={\"15_weekday\": \"15_weekday\", \"15_weekend\": \"15_weekend\"}, inplace=True\n","    )\n","\n","    return reshaped_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.520322Z","iopub.status.idle":"2024-10-08T19:02:07.521004Z","shell.execute_reply":"2024-10-08T19:02:07.520713Z","shell.execute_reply.started":"2024-10-08T19:02:07.520679Z"},"trusted":true},"outputs":[],"source":["tsf_15min = df15min_tsfprep(df_15min)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.523531Z","iopub.status.idle":"2024-10-08T19:02:07.524192Z","shell.execute_reply":"2024-10-08T19:02:07.523901Z","shell.execute_reply.started":"2024-10-08T19:02:07.523869Z"},"trusted":true},"outputs":[],"source":["#Extract features\n","extracted_15min = extract_features(tsf_15min, column_id=\"bldg_id\", column_sort=\"time\",\n","                                        n_jobs=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.526425Z","iopub.status.idle":"2024-10-08T19:02:07.527082Z","shell.execute_reply":"2024-10-08T19:02:07.526801Z","shell.execute_reply.started":"2024-10-08T19:02:07.526769Z"},"trusted":true},"outputs":[],"source":["extracted_15min['bldg_id'] = extracted_15min.index"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.529346Z","iopub.status.idle":"2024-10-08T19:02:07.530027Z","shell.execute_reply":"2024-10-08T19:02:07.529730Z","shell.execute_reply.started":"2024-10-08T19:02:07.529696Z"},"trusted":true},"outputs":[],"source":["for col in extracted_15min.columns:\n","    if extracted_15min[col].isna().sum()>0:\n","        print(col,': ',extracted_15min[col].isna().sum())\n","        extracted_15min.drop(col,axis=1,inplace=True)\n","    elif extracted_15min[col].nunique()==1:\n","        print('constant', ' ',col)\n","        extracted_15min.drop(col,axis=1,inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.541533Z","iopub.status.idle":"2024-10-08T19:02:07.542075Z","shell.execute_reply":"2024-10-08T19:02:07.541864Z","shell.execute_reply.started":"2024-10-08T19:02:07.541840Z"},"trusted":true},"outputs":[],"source":["train_data_comp = pd.merge(train_data_comp,extracted_15min,how='left',on='bldg_id')\n","train_data_comp"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Developing Building Stock Type Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.544689Z","iopub.status.idle":"2024-10-08T19:02:07.545207Z","shell.execute_reply":"2024-10-08T19:02:07.544997Z","shell.execute_reply.started":"2024-10-08T19:02:07.544974Z"},"trusted":true},"outputs":[],"source":["X = train_data_comp[\n","    [\n","        col\n","        for col in train_data_comp.columns\n","        if not (col.endswith(\"_com\")) and not (col.endswith(\"_res\"))\n","    ]\n","]\n","X = X.drop(\"building_stock_type\", axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.546683Z","iopub.status.idle":"2024-10-08T19:02:07.547139Z","shell.execute_reply":"2024-10-08T19:02:07.546944Z","shell.execute_reply.started":"2024-10-08T19:02:07.546923Z"},"trusted":true},"outputs":[],"source":["stock_type_y = train_data_comp[\"building_stock_type\"].map(\n","    {\"residential\": 1, \"commercial\": 0}\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.550955Z","iopub.status.idle":"2024-10-08T19:02:07.551403Z","shell.execute_reply":"2024-10-08T19:02:07.551211Z","shell.execute_reply.started":"2024-10-08T19:02:07.551189Z"},"trusted":true},"outputs":[],"source":["# Identify and drop features with only one unique value. They are obviously not relevant\n","constant_cols = []\n","for col in X.columns:\n","    if X[col].nunique()==1:\n","        constant_cols.append(col)\n","        \n","X = X.drop(constant_cols,axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.553691Z","iopub.status.idle":"2024-10-08T19:02:07.554156Z","shell.execute_reply":"2024-10-08T19:02:07.553955Z","shell.execute_reply.started":"2024-10-08T19:02:07.553933Z"},"trusted":true},"outputs":[],"source":["# data preprocessing pipeling to encode non-numeric data and scale data\n","type_prep_pipe = Pipeline([\n","    ('tenc', ColumnTransformer([('Oencode', TargetEncoder(),\n","                                 ['peak_timeofday','build_state'])],\n","                               remainder='passthrough')),\n","    ('scaler', MinMaxScaler())])\n","\n","prepped_typeX = type_prep_pipe.fit_transform(X,stock_type_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.556120Z","iopub.status.idle":"2024-10-08T19:02:07.556698Z","shell.execute_reply":"2024-10-08T19:02:07.556459Z","shell.execute_reply.started":"2024-10-08T19:02:07.556435Z"},"trusted":true},"outputs":[],"source":["prepped_typeX.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.558678Z","iopub.status.idle":"2024-10-08T19:02:07.559150Z","shell.execute_reply":"2024-10-08T19:02:07.558957Z","shell.execute_reply.started":"2024-10-08T19:02:07.558933Z"},"trusted":true},"outputs":[],"source":["# indices of important features after feature selection process\n","true_indexes = [1024, 1026, 1029, 6, 1031, 5, 9, 11, 12, 2069, 21, 1559, 2071, 535, 23,\n","                25, 28, 1052, 541, 30, 33, 1570, 1574, 39, 40, 551, 42, 554, 1062, 45,\n","                2093, 1580, 560, 1584, 52, 1589, 54, 2104, 57, 568, 1083, 570, 63, 66,\n","                1091, 1092, 69, 75, 1099, 1615, 81, 82, 1106, 84, 1107, 87, 599, 1113,\n","                90, 1117, 1118, 95, 606, 1634, 99, 98, 101, 1635, 110, 112, 113, 1654,\n","                1145, 122, 124, 125, 126, 127, 128, 638, 1663, 131, 132, 645, 647, 659,\n","                149, 662, 1693, 160, 1696, 162, 1698, 678, 1702, 682, 1712, 690, 180,\n","                1716, 697, 186, 1216, 1220, 1747, 724, 1236, 737, 1290, 780, 1810, 789,\n","                803, 301, 813, 826, 314, 831, 834, 838, 844, 334, 847, 848, 337, 1879,\n","                861, 862, 353, 1386, 1389, 882, 1913, 393, 413, 416, 932, 1448, 944, 435,\n","                1460, 950, 958, 1471, 448, 447, 450, 1987, 460, 461, 974, 1487, 1495,\n","                990, 2018, 1509, 1510, 1511, 1512, 2023, 1515, 2028, 2031, 2034, 499,\n","                1012, 1014, 1015, 1016, 1022, 1023]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.561947Z","iopub.status.idle":"2024-10-08T19:02:07.562740Z","shell.execute_reply":"2024-10-08T19:02:07.562382Z","shell.execute_reply.started":"2024-10-08T19:02:07.562345Z"},"trusted":true},"outputs":[],"source":["finalX = prepped_typeX[:,true_indexes]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.566104Z","iopub.status.idle":"2024-10-08T19:02:07.566862Z","shell.execute_reply":"2024-10-08T19:02:07.566514Z","shell.execute_reply.started":"2024-10-08T19:02:07.566476Z"},"trusted":true},"outputs":[],"source":["finalX.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.569149Z","iopub.status.idle":"2024-10-08T19:02:07.569918Z","shell.execute_reply":"2024-10-08T19:02:07.569559Z","shell.execute_reply.started":"2024-10-08T19:02:07.569526Z"},"trusted":true},"outputs":[],"source":["# f1_score of 1.0 after hyperparameter tuning process with final parameters being:\n","cbest_params = {\n","    \"n_estimators\": 130,\n","    \"max_depth\": 3,\n","    \"num_leaves\": 159,\n","    \"learning_rate\": 0.2926672253860692,\n","    \"min_child_samples\": 38,\n","    \"min_child_weight\": 1.8156267400360182,\n","    \"subsample\": 0.40161872280656674,\n","    \"scale_pos_weight\": 6,\n","    \"colsample_bytree\": 0.9412860959511857,\n","    \"reg_alpha\": 0.4842521664025019,\n","    \"reg_lambda\": 3.664346685841027e-06,\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.571683Z","iopub.status.idle":"2024-10-08T19:02:07.572344Z","shell.execute_reply":"2024-10-08T19:02:07.572034Z","shell.execute_reply.started":"2024-10-08T19:02:07.572003Z"},"trusted":true},"outputs":[],"source":["pyc_df = pd.DataFrame(finalX)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:02:07.574388Z","iopub.status.idle":"2024-10-08T19:02:07.575050Z","shell.execute_reply":"2024-10-08T19:02:07.574759Z","shell.execute_reply.started":"2024-10-08T19:02:07.574726Z"},"trusted":true},"outputs":[],"source":["typekf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n","cmodels = []\n","oof_pred = []\n","for train_index, test_index in typekf.split(pyc_df, stock_type_y):\n","\n","    X_train, X_test = pyc_df.iloc[train_index], pyc_df.iloc[test_index]\n","    y_train, y_test = stock_type_y.iloc[train_index], stock_type_y.iloc[test_index]\n","    cmodel = Pipeline(\n","        [\n","            (\n","                \"model\",\n","                LGBMClassifier(\n","                    verbose=-1, device_type=\"cpu\", random_state=42, **cbest_params\n","                ),\n","            )\n","        ]\n","    )\n","\n","    cmodel.fit(X_train, y_train)\n","    preds = cmodel.predict(X_test)\n","    oof_pred.append(f1_score(y_test, preds))\n","    cmodels.append(cmodel)\n","\n","print(np.mean(oof_pred))\n"]},{"cell_type":"markdown","metadata":{},"source":["# Second Hierarchy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.826621Z","iopub.status.idle":"2024-10-08T12:15:00.827130Z","shell.execute_reply":"2024-10-08T12:15:00.826896Z","shell.execute_reply.started":"2024-10-08T12:15:00.826873Z"},"trusted":true},"outputs":[],"source":["def create_combo_task(df: pd.DataFrame, target_names: list) -> pd.DataFrame:\n","    \"\"\"\n","    Combine individual target columns into a single target column along with a `target_type` column.\n","\n","    This function takes a DataFrame and a list of target column names, and returns a new DataFrame \n","    where each row is duplicated for each target column. A new column `target_type` specifies the \n","    original target column from which the `target` value was taken.\n","\n","    Parameters\n","    ----------\n","    df : pd.DataFrame\n","        The input DataFrame containing features and multiple target columns.\n","    target_names : list of str\n","        List of column names in `df` representing different target columns to be combined.\n","\n","    Returns\n","    -------\n","    pd.DataFrame\n","        A DataFrame with two new columns:\n","        - `target_type`: Indicates the original target column name.\n","        - `target`: The target values from the corresponding `target_type` column, \n","        converted to strings.\n","\n","    Example\n","    -------\n","    >>> res_df = pd.DataFrame({\n","            'feature1': [0.1, 0.2],\n","            'feature2': [0.3, 0.4],\n","            'target1': [1, 0],\n","            'target2': [0, 1]\n","        })\n","    >>> target_columns = ['target1', 'target2']\n","    >>> combo_res_class_df = create_combo_task(res_df, target_columns)\n","    >>> print(combo_res_class_df)\n","       feature1  feature2 target_type target\n","    0       0.1       0.3     target1      1\n","    1       0.2       0.4     target1      0\n","    0       0.1       0.3     target2      0\n","    1       0.2       0.4     target2      1\n","    \"\"\"\n","\n","    comb_clas_dfs = []\n","\n","    for target in target_names:\n","        # Create a copy of the DataFrame without the target columns\n","        new_df = df.drop(columns=target_names)\n","        # Add a new column specifying the target type\n","        new_df[\"target_type\"] = target\n","        # Convert the target column values to strings and add them as 'target'\n","        new_df[\"target\"] = df[target].astype(str)\n","        comb_clas_dfs.append(new_df)\n","\n","    # Concatenate all dataframes vertically\n","    combined_df = pd.concat(comb_clas_dfs, axis=0)\n","\n","    return combined_df\n"]},{"cell_type":"markdown","metadata":{},"source":["# Commercial Targets"]},{"cell_type":"markdown","metadata":{},"source":["## Commercial Building Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.829662Z","iopub.status.idle":"2024-10-08T12:15:00.830346Z","shell.execute_reply":"2024-10-08T12:15:00.830056Z","shell.execute_reply.started":"2024-10-08T12:15:00.830005Z"},"trusted":true},"outputs":[],"source":["# filtering complete data to create data of commercial buildings only\n","comm_targets = [col for col in train_data_comp.columns if col.endswith('_com')]\n","comm_df = train_data_comp[~(train_data_comp[comm_targets[0]].isna())]\n","comm_df = comm_df[[col for col in comm_df if not col.endswith('_res')]]\n","\n","# dropping feature columns with only one unique value\n","comm_df.drop(constant_cols,axis=1,inplace=True)\n","\n","# applying create_combo_task function to condense the commercial building target types\n","# into one single target column\n","combo_com_clas_df = create_combo_task(comm_df,comm_targets)"]},{"cell_type":"markdown","metadata":{},"source":["## Commercial Building Feature Selection\n","Defining relevant indices of features necessary for predicting the target values of each commercial building target type.\n","These were identified as part of a feature selection process"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.832631Z","iopub.status.idle":"2024-10-08T12:15:00.833258Z","shell.execute_reply":"2024-10-08T12:15:00.832955Z","shell.execute_reply.started":"2024-10-08T12:15:00.832925Z"},"trusted":true},"outputs":[],"source":["# dictionary to store each target type as a key and the relevant feature indices as values\n","com_relevant_indices = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.835746Z","iopub.status.idle":"2024-10-08T12:15:00.836655Z","shell.execute_reply":"2024-10-08T12:15:00.836104Z","shell.execute_reply.started":"2024-10-08T12:15:00.836073Z"},"trusted":true},"outputs":[],"source":["com_relevant_indices['in.vintage_com'] = \\\n","[512, 1, 2050, 516, 1542, 2055, 530, 1047, 1565, 33, 546, 547, 1060, 1572, 1574, 551,\n"," 1575, 2083, 572, 63, 1088, 1600, 578, 1602, 75, 78, 1618, 611, 614, 616, 1130, 1133,\n"," 1135, 1138, 1654, 1655, 1145, 123, 1660, 637, 1661, 126, 640, 128, 638, 1666, 655, 1167,\n"," 1681, 660, 664, 670, 671, 679, 1196, 179, 185, 186, 699, 700, 1213, 188, 705, 1217, 707,\n"," 1221, 198, 1223, 200, 201, 1225, 1227, 716, 1738, 1226, 1222, 1737, 1746, 1748, 1749,\n"," 727, 216, 220, 1756, 734, 1757, 1758, 1254, 1775, 1776, 241, 1805, 272, 273, 1297, 1302,\n"," 1818, 1307, 1820, 1821, 1823, 1312, 1313, 1825, 803, 804, 1316, 1829, 1831, 809, 1834,\n"," 1835, 810, 1837, 1839, 1841, 1330, 1846, 311, 1848, 1849, 1855, 836, 1349, 1862, 1865,\n"," 337, 1361, 1367, 1880, 1368, 1371, 1884, 1372, 1888, 1889, 1891, 1380, 357, 1893, 1894,\n"," 1900, 366, 1903, 1904, 375, 1918, 1919, 1922, 386, 1923, 1925, 1414, 1929, 407, 409,\n"," 414, 1438, 1441, 1444, 1957, 1958, 1445, 1455, 1968, 1460, 1461, 1974, 1463, 1464,\n"," 1975, 1978, 1973, 1977, 445, 446, 1983, 1984, 1985, 1474, 455, 2001, 467, 2004, 2003,\n"," 470, 2013, 487, 2030, 500, 503, 506, 509]\n","\n","com_relevant_indices['in.comstock_building_type_group_com'] = \\\n","[512, 1546, 1550, 18, 1555, 19, 20, 1558, 23, 2071, 2072, 2076, 2078, 2079, 2080, 1060,\n"," 1573, 1064, 1066, 45, 51, 1076, 1592, 1604, 84, 1622, 101, 1131, 1647, 1135, 113, 1648,\n"," 116, 1146, 122, 123, 124, 126, 127, 128, 646, 1674, 659, 672, 1184, 685, 174, 186, 187,\n"," 188, 189, 190, 1726, 191, 1220, 1221, 1734, 711, 1224, 1228, 1746, 1748, 733, 735, 1778,\n"," 1780, 1278, 1288, 268, 269, 270, 1816, 1819, 802, 812, 1836, 1838, 832, 836, 1861, 329,\n"," 1867, 333, 1360, 849, 1363, 1884, 351, 1888, 370, 1914, 387, 1422, 1959, 1483, 1485,\n"," 2008, 473, 2011, 499, 500, 1530, 2044, 2046]\n","\n","com_relevant_indices['in.heating_fuel_com'] = \\\n","[512, 1, 0, 9, 10, 13, 1044, 23, 2073, 2079, 2080, 545, 1575, 1066, 1074, 2098, 1592,\n"," 1602, 84, 1622, 87, 110, 1651, 116, 1653, 123, 636, 637, 638, 128, 641, 642, 643, 650,\n"," 653, 655, 657, 660, 665, 675, 683, 688, 690, 179, 693, 182, 187, 188, 189, 190, 703,\n"," 192, 699, 706, 1212, 1220, 710, 198, 1224, 1737, 199, 202, 715, 1746, 1748, 1749, 726,\n"," 729, 1757, 736, 749, 241, 1285, 265, 266, 268, 269, 270, 271, 274, 1811, 1814, 287, 800,\n"," 801, 802, 804, 807, 1832, 810, 812, 1838, 1330, 823, 826, 315, 835, 836, 327, 1352,\n"," 1865, 1866, 331, 1867, 334, 345, 351, 354, 1892, 1894, 360, 376, 392, 415, 1957, 426,\n"," 1962, 1963, 1974, 1468, 446, 448, 463, 2013, 2016, 2017, 2030, 508]\n","\n","com_relevant_indices['in.wall_construction_type_com'] = \\\n","[1, 517, 2055, 2056, 9, 10, 11, 17, 536, 538, 539, 542, 1568, 2081, 1060, 2092, 2094,\n"," 562, 2102, 567, 2103, 1083, 572, 573, 1602, 1604, 75, 591, 611, 614, 622, 1135, 1140,\n"," 117, 120, 123, 635, 1661, 638, 1662, 640, 1666, 642, 132, 645, 646, 647, 1672, 653,\n"," 659, 669, 1184, 672, 674, 676, 677, 166, 1192, 682, 1195, 687, 177, 690, 180, 692, 186,\n"," 192, 1217, 705, 707, 706, 1221, 199, 1227, 716, 717, 203, 1234, 730, 220, 224, 231, 235,\n"," 238, 1775, 239, 754, 755, 756, 245, 757, 1278, 258, 1806, 272, 1296, 274, 275, 1819,\n"," 1308, 1313, 802, 291, 1316, 1317, 803, 807, 811, 812, 1323, 1326, 1327, 305, 1330, 1844,\n"," 1333, 1845, 311, 1846, 1851, 1852, 1858, 1349, 327, 1863, 1353, 331, 1870, 1872, 1361,\n"," 345, 346, 1371, 1372, 1369, 1374, 1887, 1376, 350, 1890, 1899, 366, 368, 1906, 375, \n"," 1402, 380, 1407, 384, 1922, 392, 393, 1932, 1424, 402, 1943, 409, 411, 1951, 1953, 1443,\n"," 1446, 1961, 1455, 1972, 439, 442, 1980, 1469, 1471, 448, 1991, 1992, 1999, 1489, 2002,\n"," 468, 1493, 470, 471, 473, 476, 477, 2014, 1505, 485, 494]\n","\n","com_relevant_indices['in.hvac_category_com'] = \\\n","[1, 9, 1546, 19, 20, 2069, 535, 536, 537, 1048, 538, 540, 542, 543, 2080, 2081, 1064,\n"," 554, 2093, 51, 2100, 567, 1592, 578, 580, 1120, 1130, 1133, 110, 117, 118, 119, 1653,\n"," 121, 1654, 124, 125, 637, 126, 640, 128, 642, 643, 644, 127, 662, 151, 164, 1191, 174,\n"," 177, 180, 181, 184, 186, 187, 699, 192, 1220, 1222, 198, 1224, 1226, 203, 1227, 1747,\n"," 1748, 726, 735, 1760, 755, 265, 267, 268, 269, 270, 272, 287, 1313, 802, 803, 804, 807,\n"," 808, 813, 814, 822, 824, 826, 323, 836, 327, 337, 340, 342, 387, 428, 440, 451, 1484,\n"," 1492, 2008, 2009, 2011, 2016, 2017]\n","\n","com_relevant_indices['in.ownership_type_com'] = \\\n","[512, 1, 527, 532, 533, 2071, 2073, 538, 1567, 2081, 547, 1060, 39, 1074, 2099, 51,\n"," 2101, 2100, 2103, 1592, 1594, 1598, 1087, 577, 1611, 1622, 105, 1645, 1646, 1648,\n"," 1649, 1652, 1141, 116, 1656, 1658, 1659, 123, 1147, 124, 127, 1663, 642, 1668, 1669,\n"," 646, 662, 1184, 674, 682, 1196, 686, 1200, 1204, 1208, 185, 697, 699, 1209, 189, 190,\n"," 191, 702, 1729, 188, 700, 709, 1228, 1741, 1229, 1231, 1233, 1745, 1746, 1751, 1752,\n"," 217, 218, 731, 1761, 1767, 1777, 1778, 1280, 1793, 1306, 795, 1820, 1310, 1826, 1320,\n"," 1833, 1321, 1834, 812, 1837, 1327, 1840, 1839, 1844, 311, 1849, 826, 1854, 1344, 1345,\n"," 1856, 323, 1861, 329, 1867, 1869, 337, 1874, 343, 344, 1370, 1883, 1888, 1385, 1392,\n"," 1398, 1406, 1408, 1921, 1412, 1934, 412, 1442, 1444, 421, 422, 1446, 1963, 1452, 1968,\n"," 1459, 1460, 1979, 443, 1980, 447, 1983, 1984, 1474, 1986, 1476, 1479, 1993, 1999,\n"," 1488, 472, 2008, 2010, 2013, 2016, 2017, 2027, 499]\n","\n","com_relevant_indices['in.number_of_stories_com'] = \\\n","[1, 2056, 2060, 2064, 529, 530, 19, 1555, 1554, 2067, 23, 1048, 535, 2074, 2079, 2080,\n"," 2098, 2099, 2103, 1592, 1594, 1615, 117, 637, 641, 643, 132, 644, 646, 1157, 1672, 652,\n"," 672, 688, 177, 690, 181, 183, 184, 185, 186, 696, 188, 189, 192, 705, 1219, 1220, 1224,\n"," 1225, 1226, 1227, 718, 1745, 1748, 724, 1753, 731, 1757, 1763, 1764, 741, 1254, 1766,\n"," 1266, 1780, 1781, 1782, 249, 254, 1278, 265, 267, 268, 1807, 271, 1810, 1811, 1299, 277,\n"," 1305, 1818, 1819, 283, 1830, 807, 808, 1321, 1837, 1846, 826, 831, 834, 835, 836, 1866,\n"," 1868, 1361, 1363, 349, 350, 1375, 353, 1384, 361, 1391, 1904, 1906, 887, 375, 1913, 378,\n"," 383, 385, 386, 387, 1926, 393, 395, 399, 411, 412, 1440, 1962, 1964, 1456, 1969, 1974,\n"," 1465, 1979, 444, 1980, 1473, 453, 1994, 1488, 2001, 1492, 472, 2013, 480, 1505, 499,\n"," 2038, 1533, 2046]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.839978Z","iopub.status.idle":"2024-10-08T12:15:00.840612Z","shell.execute_reply":"2024-10-08T12:15:00.840349Z","shell.execute_reply.started":"2024-10-08T12:15:00.840324Z"},"trusted":true},"outputs":[],"source":["com_relevant_indices['in.tstat_htg_sp_f..f_com'] = \\\n","[513, 2, 2052, 2056, 1546, 2067, 23, 2073, 2075, 2076, 2077, 2078, 2080, 2098, 1077,\n"," 1592, 87, 612, 100, 614, 1133, 1647, 1136, 1137, 1650, 116, 122, 1147, 124, 123, 1662,\n"," 1660, 126, 1659, 646, 648, 1675, 652, 1682, 659, 1684, 663, 1687, 1180, 672, 1700, 167,\n"," 1705, 1706, 172, 174, 1719, 185, 186, 188, 189, 190, 191, 704, 1219, 1733, 1734, 1221,\n"," 713, 1738, 1737, 715, 1227, 1228, 1740, 1746, 1747, 726, 1750, 1752, 1762, 1764, 1765,\n"," 1776, 1777, 1778, 1780, 1781, 1782, 253, 1792, 267, 268, 269, 1294, 270, 275, 1812,\n"," 1814, 1303, 1816, 279, 794, 1824, 1313, 1826, 1320, 1836, 1333, 822, 836, 1867, 1363,\n"," 1879, 347, 1886, 1889, 1896, 361, 364, 1903, 1392, 1908, 1912, 1913, 1914, 387, 393,\n"," 1421, 1435, 412, 1437, 1440, 1443, 1444, 1446, 1959, 1960, 1963, 1451, 1457, 1458, 433,\n"," 1460, 1459, 1462, 1973, 1978, 1981, 446, 1982, 1992, 1483, 1485, 2013, 2024, 2030, 499,\n"," 500, 504, 2046]\n","\n","com_relevant_indices['in.tstat_clg_sp_f..f_com'] = \\\n","[2052, 1542, 2056, 2064, 23, 2073, 2075, 2076, 2077, 2080, 1573, 1574, 560, 2097, 1076,\n"," 1592, 1087, 1604, 1611, 100, 1133, 1134, 1137, 116, 122, 123, 124, 1660, 643, 1156, 646,\n"," 1674, 1676, 1684, 1688, 678, 1705, 681, 683, 172, 685, 174, 1196, 1711, 692, 183, 696,\n"," 697, 698, 186, 700, 189, 190, 191, 1217, 1730, 1729, 706, 1221, 1222, 1734, 712, 1737,\n"," 1226, 1219, 1228, 1223, 1230, 1746, 1747, 1748, 729, 1754, 231, 1778, 1782, 1278, 1796,\n"," 1797, 1798, 268, 1294, 271, 273, 1811, 1816, 1306, 794, 1310, 1313, 1826, 803, 1827,\n"," 1835, 812, 1837, 1337, 826, 1341, 831, 832, 1861, 1862, 1351, 1867, 1871, 1874, 1876,\n"," 1881, 1882, 1889, 1377, 1383, 361, 363, 367, 1905, 1908, 1397, 1912, 1913, 1914, 378,\n"," 1405, 1406, 387, 1413, 1420, 1421, 398, 399, 396, 406, 411, 1437, 413, 1957, 1448, 1960,\n"," 1962, 1453, 434, 1467, 1470, 453, 1992, 1483, 1485, 1997, 1999, 2013, 1505, 2021, 2026,\n"," 2028, 508]\n","\n","com_relevant_indices['in.weekday_opening_time..hr_com'] = \\\n","[1, 1547, 1548, 1549, 1550, 524, 17, 19, 23, 2075, 2080, 1574, 1066, 1070, 1071, 1083,\n"," 574, 1123, 1127, 105, 1129, 1131, 120, 121, 122, 124, 1157, 1161, 649, 1162, 653, 1166,\n"," 654, 659, 662, 667, 1184, 1192, 1193, 1196, 684, 1200, 178, 1204, 181, 183, 185, 698,\n"," 1211, 1212, 1213, 190, 1215, 705, 711, 712, 1228, 1746, 1748, 727, 1751, 1753, 757, 1273,\n"," 1292, 1293, 271, 272, 274, 1816, 1305, 1818, 1312, 1313, 1314, 1315, 1826, 1317, 1316,\n"," 802, 1336, 1337, 1338, 1859, 336, 1360, 1362, 1363, 1361, 345, 351, 1383, 1384, 361,\n"," 1385, 388, 1436, 1448, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 438, 1464, 1467, 1470,\n"," 447, 1472, 1471, 1480, 1481, 1489, 2008, 1502, 1503, 500, 1533]\n","\n","com_relevant_indices['in.weekday_operating_hours..hr_com'] = \\\n","[517, 1542, 1551, 1553, 1554, 19, 1557, 1558, 1047, 1045, 1568, 1569, 1573, 1574, 1577,\n"," 1066, 1067, 1578, 1071, 1072, 1073, 1076, 1077, 1588, 2103, 1592, 1618, 597, 1129, 1131,\n"," 116, 117, 118, 119, 632, 125, 646, 650, 1682, 666, 1180, 1184, 674, 1192, 681, 1196,\n"," 177, 1201, 1204, 190, 1221, 1224, 202, 715, 1741, 1746, 1238, 1250, 1251, 1252, 1253,\n"," 1269, 246, 1281, 1282, 1292, 1807, 1300, 1304, 1817, 1821, 303, 1330, 822, 1848, 1850,\n"," 1344, 1857, 323, 1349, 1861, 331, 1357, 1873, 1362, 1881, 346, 1371, 1888, 1376, 1385,\n"," 1388, 365, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1908, 1398, 1400, 1401, 1402, 1405,\n"," 1406, 1407, 1408, 1409, 1410, 1411, 387, 1414, 1415, 1926, 1419, 1420, 1421, 1423, 1424,\n"," 1425, 1429, 1433, 1434, 1436, 432, 1972, 440, 1977, 1985, 1483, 1485, 467, 1492, 1491,\n"," 474, 1501, 477, 2023]"]},{"cell_type":"markdown","metadata":{},"source":["## Commercial Building Hyperparameter Tuning\n","Defining optimised hyperparameter values necessary for an improved performance of each model dedicated for each commercial building target type.\n","These were identified as part of an optuna optimisation process"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.842221Z","iopub.status.idle":"2024-10-08T12:15:00.842777Z","shell.execute_reply":"2024-10-08T12:15:00.842547Z","shell.execute_reply.started":"2024-10-08T12:15:00.842523Z"},"trusted":true},"outputs":[],"source":["# dictionary to store each target type as a key and the optimised hyperparameters as values\n","tuned_params_com = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.844504Z","iopub.status.idle":"2024-10-08T12:15:00.844964Z","shell.execute_reply":"2024-10-08T12:15:00.844761Z","shell.execute_reply.started":"2024-10-08T12:15:00.844740Z"},"trusted":true},"outputs":[],"source":["# 0.24855760678990307 f1: \n","tuned_params_com['in.vintage_com'] =\\\n","{'n_estimators': 929, 'max_depth': 30, 'num_leaves': 98,\n"," 'learning_rate': 0.09757152134768582, 'min_child_samples': 63,\n"," 'min_child_weight': 5.789632655738992, 'subsample': 0.7037137854216451,\n"," 'scale_pos_weight': 2, 'colsample_bytree': 0.7992042532947691,\n"," 'reg_alpha': 0.00010604613407883311, 'reg_lambda': 2.718366809743991e-07}\n","\n","# 0.9698519045448032 f1: \n","tuned_params_com['in.comstock_building_type_group_com'] =\\\n","{'n_estimators': 940, 'max_depth': 46, 'num_leaves': 81,\n"," 'learning_rate': 0.09672413197136034, 'min_child_samples': 91,\n"," 'min_child_weight': 0.004624180720460598, 'subsample': 0.48241105899368497,\n"," 'scale_pos_weight': 6, 'colsample_bytree': 0.6973108502666483,\n"," 'reg_alpha': 5.754194740383054e-06, 'reg_lambda': 4.785401761956007e-07}\n","\n","# 0.7889239792117071 f1\n","tuned_params_com['in.heating_fuel_com'] = \\\n","{'n_estimators': 999, 'max_depth': 40, 'num_leaves': 31,\n"," 'learning_rate': 0.07780397696933257, 'min_child_samples': 90,\n"," 'min_child_weight': 0.8545561924548544, 'subsample': 0.42951096078090645,\n"," 'scale_pos_weight': 2, 'colsample_bytree': 0.7134162448258592,\n"," 'reg_alpha': 7.466548533825271e-08, 'reg_lambda': 0.21541630101775278}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.846448Z","iopub.status.idle":"2024-10-08T12:15:00.846914Z","shell.execute_reply":"2024-10-08T12:15:00.846715Z","shell.execute_reply.started":"2024-10-08T12:15:00.846693Z"},"trusted":true},"outputs":[],"source":["# 0.9006977979183691  f1: \n","tuned_params_com['in.hvac_category_com'] =\\\n","{'n_estimators': 832, 'max_depth': 4, 'num_leaves': 156,\n"," 'learning_rate': 0.20961451964851766, 'min_child_samples': 81,\n"," 'min_child_weight': 0.0057852790807866265, 'subsample': 0.46176193007934135,\n"," 'scale_pos_weight': 6, 'colsample_bytree': 0.7308989879791561,\n"," 'reg_alpha': 1.293965596370794e-07, 'reg_lambda': 1.0701562102768924e-08}\n","\n","#0.5028913117127053 f1\n","tuned_params_com['in.wall_construction_type_com'] = \\\n","{'n_estimators': 956, 'max_depth': 20, 'num_leaves': 163,\n"," 'learning_rate': 0.16471541188369432, 'min_child_samples': 21,\n"," 'min_child_weight': 4.835751672994968, 'subsample': 0.5763943704418849,\n"," 'scale_pos_weight': 2, 'colsample_bytree': 0.7440354364609669,\n"," 'reg_alpha': 4.945273284359274e-07, 'reg_lambda': 0.00030261260038118315}\n","\n","#0.5994503822192405  f1: \n","tuned_params_com['in.ownership_type_com'] =\\\n","{'n_estimators': 786, 'max_depth': 1, 'num_leaves': 81,\n"," 'learning_rate': 0.13965667981953575, 'min_child_samples': 83,\n"," 'min_child_weight': 8.027579422119214, 'subsample': 0.5590589332530607,\n"," 'scale_pos_weight': 2, 'colsample_bytree': 0.6915794829646965,\n"," 'reg_alpha': 3.981238143791743e-06, 'reg_lambda': 0.0006286670702081804}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.848690Z","iopub.status.idle":"2024-10-08T12:15:00.849178Z","shell.execute_reply":"2024-10-08T12:15:00.848945Z","shell.execute_reply.started":"2024-10-08T12:15:00.848922Z"},"trusted":true},"outputs":[],"source":["#0.7225128425921922  f1: \n","tuned_params_com['in.number_of_stories_com'] =\\\n","{'n_estimators': 977, 'max_depth': 8, 'num_leaves': 123,\n"," 'learning_rate': 0.06315708717537268, 'min_child_samples': 61,\n"," 'min_child_weight': 3.4659332460428436, 'subsample': 0.9051859696480227,\n"," 'scale_pos_weight': 3, 'colsample_bytree': 0.7828327035765278,\n"," 'reg_alpha': 0.055681878622985126, 'reg_lambda': 5.30990530587543e-08}\n","\n","\n","#0.5514551239417378 f1\n","tuned_params_com['in.tstat_htg_sp_f..f_com'] = \\\n","{'n_estimators': 905, 'max_depth': 7, 'num_leaves': 203,\n"," 'learning_rate': 0.08990569979001954, 'min_child_samples': 62,\n"," 'min_child_weight': 8.371358945304257, 'subsample': 0.5108362241099114,\n"," 'scale_pos_weight': 2, 'colsample_bytree': 0.8565858005727937,\n"," 'reg_alpha': 1.9613936967546285e-05, 'reg_lambda': 1.6480916334949594e-08}\n","\n","\n","#0.518247047502408  f1: \n","tuned_params_com['in.tstat_clg_sp_f..f_com'] =\\\n","{'n_estimators': 291, 'max_depth': 4, 'num_leaves': 83,\n"," 'learning_rate': 0.25758042044965385, 'min_child_samples': 9,\n"," 'min_child_weight': 9.175461752314197, 'subsample': 0.4499538211206938,\n"," 'scale_pos_weight': 5, 'colsample_bytree': 0.823353726262743,\n"," 'reg_alpha': 0.0027062361196170865, 'reg_lambda': 0.2042564684055546}\n","\n","#0.26215467147714877 f1: \n","tuned_params_com['in.weekday_opening_time..hr_com'] =\\\n","{'n_estimators': 348, 'max_depth': 31, 'num_leaves': 138,\n"," 'learning_rate': 0.03888434262515391, 'min_child_samples': 45,\n"," 'min_child_weight': 1.1109450873970528, 'subsample': 0.8063239893252891,\n"," 'scale_pos_weight': 5, 'colsample_bytree': 0.5176944787284479,\n"," 'reg_alpha': 0.0005115978825365721, 'reg_lambda': 8.712227579337613e-06}\n","\n","#0.1995791303369906 0.2050631416306615 f1\n","tuned_params_com['in.weekday_operating_hours..hr_com'] = \\\n","{'n_estimators': 833, 'max_depth': 25, 'num_leaves': 229,\n"," 'learning_rate': 0.10743457607271996, 'min_child_samples': 79,\n"," 'min_child_weight': 0.032893547715672766, 'subsample': 0.9676303652176244,\n"," 'scale_pos_weight': 5, 'colsample_bytree': 0.5836586160140631,\n"," 'reg_alpha': 1.4559384884652598e-07, 'reg_lambda': 1.0991167895637697e-05}\n"]},{"cell_type":"markdown","metadata":{},"source":["## Developing Models for Commercial Buildings"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.851194Z","iopub.status.idle":"2024-10-08T12:15:00.851645Z","shell.execute_reply":"2024-10-08T12:15:00.851447Z","shell.execute_reply.started":"2024-10-08T12:15:00.851426Z"},"trusted":true},"outputs":[],"source":["def fit_com_models(df, min_n=6):\n","    \"\"\"\n","    Fits models for each commercial building target type by iterating through the data,\n","    preprocessing features, selecting relevant ones, and using optimized hyperparameters\n","    to train models. The models, encoders, and preprocessing pipelines are stored for\n","    future use in prediction on test data.\n","\n","    Parameters\n","    ----------\n","    df : pd.DataFrame\n","        DataFrame containing features and target values of the different commercial building target types.\n","\n","    min_n : int, optional (default=6)\n","        The minimum number of occurrences required for each target type to be included in the training process.\n","\n","    Returns\n","    -------\n","    models_dict : dict\n","        Dictionary storing the trained model pipelines for each commercial building target type.\n","\n","    enc_dict : dict\n","        Dictionary storing the label encoder objects for each commercial building target type.\n","\n","    prepro_dict : dict\n","        Dictionary storing the data preprocessing pipelines for each commercial building\n","        target type.\n","\n","    Notes\n","    -----\n","    - The function uses StratifiedKFold cross-validation with 5 splits.\n","    - The task is classified as 'binary' if the number of unique target values is 2, and \n","    'multiclass' otherwise.\n","    - Optimized hyperparameters for each target type are retrieved from a pre-defined dictionary \n","    `tuned_params_com`.\n","    \"\"\"\n","\n","    # Initialize lists and dictionaries to store results\n","    f1s = []\n","\n","    models_dict = {}\n","    enc_dict = {}\n","    prepro_dict = {}\n","\n","    # Iterate over each unique target type\n","    for t_type in df[\"target_type\"].unique():\n","        sub_df = df[df[\"target_type\"] == t_type]\n","\n","        # Filter out targets with less than `min_n` observations\n","        selected_classes = [\n","            targ\n","            for targ in sub_df[\"target\"].unique()\n","            if sub_df[sub_df[\"target\"] == targ].shape[0] > min_n\n","        ]\n","\n","        filt_df = sub_df[sub_df[\"target\"].isin(selected_classes)]\n","\n","        # Separate features and target\n","        X = filt_df.drop(\n","            [\"building_stock_type\", \"target\", \"target_type\"], axis=1\n","        ).reset_index(drop=True)\n","        y = filt_df[\"target\"]\n","\n","        # Encode target labels\n","        lenc = LabelEncoder()\n","        y_enc = pd.Series(lenc.fit_transform(y))\n","\n","        # Define preprocessing pipeline\n","        prep_pipe = Pipeline(\n","            [\n","                (\n","                    \"tenc\",\n","                    ColumnTransformer(\n","                        [\n","                            (\n","                                \"Oencode\",\n","                                TargetEncoder(),\n","                                [\"peak_timeofday\", \"build_state\"],\n","                            )\n","                        ],\n","                        remainder=\"passthrough\",\n","                    ),\n","                ),\n","                (\"scaler\", MinMaxScaler()),\n","            ]\n","        )\n","\n","        # Apply preprocessing pipeline to features\n","        prepped_X = prep_pipe.fit_transform(X, y_enc)\n","\n","        # Select relevant features for each commercial building type\n","        feature_filtered = prepped_X[:, com_relevant_indices[t_type]]\n","\n","        # Retrieve optimized hyperparameters for the target type\n","        com_params_cl = tuned_params_com.get(t_type)\n","        hyp_ccl = com_params_cl\n","\n","        # Define the task type and number of classes\n","        task = \"multiclass\" if y.nunique() > 2 else \"binary\"\n","        num_class = y.nunique() if y.nunique() > 2 else 1\n","\n","        prepped_df = pd.DataFrame(feature_filtered)\n","        kf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n","\n","        models = []\n","        oof_pred = []\n","\n","        # Stratified K-Fold cross-validation\n","        for train_index, test_index in kf.split(prepped_df, y_enc):\n","            X_train, X_test = prepped_df.iloc[train_index], prepped_df.iloc[test_index]\n","            y_train, y_test = y_enc.iloc[train_index], y_enc.iloc[test_index]\n","\n","            # Define and train the model pipeline\n","            model = Pipeline(\n","                [\n","                    (\n","                        \"model\",\n","                        LGBMClassifier(\n","                            verbose=-1,\n","                            device_type=\"cpu\",\n","                            objective=task,\n","                            num_class=num_class,\n","                            random_state=42,\n","                            **hyp_ccl,\n","                        ),\n","                    )\n","                ]\n","            )\n","\n","            model.fit(X_train, y_train)\n","            preds = model.predict(X_test)\n","            oof_pred.append(f1_score(y_test, preds, average=\"weighted\"))\n","            models.append(model)\n","\n","        # Calculate and print the F1 score for the target type\n","        f1 = np.mean(oof_pred)\n","        print(f\"{t_type} test f1: {f1}\")\n","\n","        # Store the trained model, encoder, and preprocessing pipeline\n","        models_dict[t_type] = models\n","        enc_dict[t_type] = lenc\n","        prepro_dict[t_type] = prep_pipe\n","\n","        f1s.append(f1)\n","\n","    # Print the average F1 score across all target types\n","    print(f\"Average F1: {np.mean(f1s)}\")\n","\n","    return models_dict, enc_dict, prepro_dict\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.854317Z","iopub.status.idle":"2024-10-08T12:15:00.854783Z","shell.execute_reply":"2024-10-08T12:15:00.854579Z","shell.execute_reply.started":"2024-10-08T12:15:00.854555Z"},"trusted":true},"outputs":[],"source":["comclas_models,comclas_encs,comclas_prepro = fit_com_models(combo_com_clas_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Residential Targets"]},{"cell_type":"markdown","metadata":{},"source":["## Residential Building Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.856505Z","iopub.status.idle":"2024-10-08T12:15:00.856973Z","shell.execute_reply":"2024-10-08T12:15:00.856770Z","shell.execute_reply.started":"2024-10-08T12:15:00.856748Z"},"trusted":true},"outputs":[],"source":["# filterning complete data to create data of residential buildings only\n","res_targets = [col for col in train_data_comp.columns if col.endswith('_res')]\n","res_df = train_data_comp[~(train_data_comp[res_targets[0]].isna())]\n","res_df = res_df[[col for col in res_df if not col.endswith('_com')]]\n","\n","# dropping feature columns with only one unique value\n","res_df.drop(constant_cols,axis=1,inplace=True)\n","\n","# applying create_combo_task function to condense the residential building target types\n","# into one single target column\n","combo_res_clas_df = create_combo_task(res_df,res_targets)"]},{"cell_type":"markdown","metadata":{},"source":["## Residential Building Feature Selection\n","Defining relevant indices of features necessary for predicting the target values of each residential building target type.\n","These were identified as part of a feature selection process"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.858647Z","iopub.status.idle":"2024-10-08T12:15:00.859126Z","shell.execute_reply":"2024-10-08T12:15:00.858888Z","shell.execute_reply.started":"2024-10-08T12:15:00.858867Z"},"trusted":true},"outputs":[],"source":["# dictionary to store each target type as a key and the relevant feature indices as values\n","res_relevant_indices = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.861582Z","iopub.status.idle":"2024-10-08T12:15:00.862455Z","shell.execute_reply":"2024-10-08T12:15:00.862077Z","shell.execute_reply.started":"2024-10-08T12:15:00.861964Z"},"trusted":true},"outputs":[],"source":["res_relevant_indices['in.geometry_building_type_recs_res'] = \\\n","[1, 1027, 517, 10, 525, 1550, 13, 25, 2078, 32, 2081, 2080, 35, 36, 1573, 547, 42, 44,\n"," 49, 51, 53, 54, 2101, 60, 64, 67, 68, 72, 78, 85, 87, 1117, 95, 1127, 1128, 617, 618,\n"," 624, 1656, 633, 121, 635, 123, 636, 638, 640, 641, 642, 131, 643, 135, 139, 662, 1176,\n"," 155, 674, 164, 166, 170, 173, 1201, 178, 1206, 185, 190, 1216, 1217, 1220, 1221, 1222,\n"," 1739, 1233, 726, 220, 221, 1758, 736, 225, 236, 753, 755, 756, 252, 1796, 267, 272, 278,\n"," 282, 794, 1819, 801, 814, 303, 307, 311, 319, 835, 836, 1861, 337, 338, 339, 849, 341,\n"," 342, 346, 349, 352, 870, 1387, 1905, 370, 1404, 1409, 1936, 1937, 1939, 1433, 1947,\n"," 924, 1953, 421, 428, 1965, 434, 436, 948, 950, 1461, 949, 1977, 954, 955, 956, 957,\n"," 958, 959, 1983, 446, 962, 965, 966, 454, 456, 1490, 1491, 2008, 1498, 488, 2026, 1021]\n","\n","res_relevant_indices['in.geometry_foundation_type_res'] = \\\n","[1, 514, 519, 521, 2058, 524, 1548, 1552, 17, 1554, 532, 25, 2077, 2078, 547, 548, 39,\n"," 1064, 558, 51, 1595, 574, 1615, 605, 608, 105, 617, 106, 626, 116, 117, 629, 1652, 122,\n"," 123, 634, 635, 636, 639, 640, 644, 646, 652, 653, 663, 1176, 666, 155, 1184, 177, 178,\n"," 179, 185, 187, 190, 191, 1214, 193, 1730, 708, 1220, 1221, 1222, 718, 720, 208, 1746,\n"," 1745, 217, 730, 731, 220, 221, 222, 223, 227, 233, 747, 751, 242, 755, 248, 253, 1792,\n"," 266, 275, 279, 1819, 802, 804, 1834, 810, 1837, 302, 1839, 307, 1332, 1848, 314, 317,\n"," 832, 833, 323, 1861, 329, 333, 339, 344, 345, 346, 353, 367, 370, 1399, 375, 379, 387,\n"," 393, 397, 406, 418, 421, 425, 428, 1969, 436, 437, 1462, 444, 445, 446, 1983, 457, 1481,\n"," 1996, 466, 472, 483, 484, 492, 497, 501, 2045, 511]\n","\n","res_relevant_indices['in.geometry_floor_area_res'] = \\\n","[1024, 1, 1025, 1539, 4, 1541, 2054, 1542, 1032, 2060, 1039, 528, 2064, 530, 19, 2066,\n"," 1045, 536, 25, 32, 35, 547, 1059, 39, 1578, 42, 44, 47, 2097, 49, 51, 53, 2102, 54,\n"," 2103, 63, 66, 72, 1611, 84, 85, 87, 607, 95, 1642, 1131, 1133, 110, 624, 116, 630, 631,\n"," 123, 635, 124, 1662, 642, 131, 155, 156, 165, 168, 178, 179, 692, 1202, 1206, 182, 184,\n"," 189, 190, 707, 1220, 1221, 1222, 1224, 1226, 1746, 727, 221, 1249, 241, 1782, 1277, 265,\n"," 1296, 1820, 287, 813, 315, 828, 829, 830, 831, 835, 836, 1362, 1875, 343, 1886, 360,\n"," 1897, 1389, 1390, 372, 375, 1915, 1408, 387, 1413, 1414, 1933, 1424, 1937, 1425, 1430,\n"," 1431, 1432, 1433, 1955, 1957, 428, 1967, 433, 1458, 436, 955, 958, 1470, 960, 961, 962,\n"," 964, 1476, 456, 1483, 1488, 2008, 2011, 992, 2017, 994, 995, 997, 1001, 1002, 492, 1004,\n"," 1005, 1007, 1008, 1009, 1010, 1013, 1535]\n","\n","res_relevant_indices['in.geometry_wall_type_res'] = \\\n","[1, 1027, 516, 1540, 522, 523, 1020, 525, 524, 1553, 529, 532, 533, 2075, 2078, 1060,\n"," 559, 2095, 560, 575, 577, 592, 83, 84, 596, 602, 608, 105, 117, 1141, 1656, 634, 123,\n"," 637, 638, 640, 642, 645, 1669, 1672, 650, 1676, 652, 656, 657, 660, 661, 662, 1176, 156,\n"," 673, 674, 1701, 1196, 175, 1200, 689, 178, 691, 1725, 190, 702, 193, 705, 1737, 1227,\n"," 1741, 721, 1234, 211, 725, 726, 1752, 1759, 1259, 751, 754, 755, 756, 246, 1273, 1277,\n"," 1279, 775, 801, 803, 1829, 805, 808, 1839, 303, 1844, 1845, 1334, 1847, 826, 315, 319,\n"," 836, 1861, 1353, 339, 1380, 360, 1384, 1898, 364, 1908, 375, 1917, 395, 1931, 1934, 402,\n"," 407, 1431, 1437, 1952, 1441, 1447, 423, 426, 427, 428, 1962, 1454, 1461, 1462, 439, 443,\n"," 444, 446, 1993, 458, 1490, 1491, 2004, 1501, 478, 1503, 2013, 1008, 503, 1529, 506, 508]\n","\n","res_relevant_indices['in.income_res'] = \\\n","[1024, 1, 1538, 1544, 525, 526, 17, 531, 2067, 19, 1556, 25, 1565, 1566, 30, 1059, 1060,\n"," 550, 552, 555, 2095, 2097, 51, 564, 1080, 1087, 576, 1096, 72, 75, 78, 1616, 592, 1618,\n"," 596, 602, 606, 610, 613, 106, 1133, 1134, 627, 116, 1139, 632, 633, 122, 634, 1144,\n"," 1660, 639, 1666, 131, 132, 644, 1667, 658, 661, 155, 156, 677, 178, 185, 188, 190,\n"," 1216, 192, 706, 709, 710, 1223, 712, 713, 1228, 1741, 1229, 720, 1233, 1235, 1748, 725,\n"," 1245, 735, 1775, 1778, 1779, 1266, 1269, 1271, 1278, 1289, 1294, 1807, 1811, 277, 283,\n"," 1308, 1826, 295, 1323, 301, 1837, 815, 1328, 817, 818, 1843, 1334, 1337, 826, 1852, 1342,\n"," 831, 1858, 323, 1861, 1351, 1354, 332, 333, 1356, 1876, 1365, 1367, 1372, 1884, 349, 353,\n"," 1890, 1377, 1889, 1894, 1895, 359, 1897, 365, 1901, 1402, 1917, 381, 1409, 1922, 1923,\n"," 1412, 1411, 1926, 391, 392, 395, 1931, 1419, 1936, 1425, 402, 1938, 400, 401, 406, 1430,\n"," 1431, 404, 1946, 1436, 1948, 414, 416, 1956, 424, 1451, 1458, 1460, 1972, 1461, 439,\n"," 1974, 1978, 444, 1469, 1468, 1983, 1982, 1985, 963, 1477, 454, 1990, 1991, 458, 460,\n"," 974, 1998, 463, 1488, 1491, 2005, 2010, 2011, 477, 2015, 992, 999, 488, 496, 498, 506,\n"," 503, 1017, 1018, 507, 1535]\n","\n","res_relevant_indices['in.roof_material_res'] = \\\n","[1, 517, 520, 1032, 525, 1039, 528, 17, 1558, 25, 2075, 541, 545, 547, 51, 1588, 54,\n"," 60, 585, 588, 87, 600, 1131, 620, 110, 1137, 1652, 1653, 1656, 634, 123, 635, 636,\n"," 637, 640, 1665, 643, 645, 135, 648, 651, 654, 669, 672, 674, 165, 678, 1193, 682, 172,\n"," 173, 687, 1200, 1202, 179, 1722, 187, 190, 704, 708, 1224, 716, 1741, 1230, 719, 720,\n"," 728, 730, 731, 222, 736, 1250, 1261, 755, 756, 767, 257, 258, 1799, 269, 1294, 1293,\n"," 1807, 273, 271, 1295, 1301, 279, 1818, 795, 1823, 803, 1831, 1835, 814, 303, 1329, 307,\n"," 1844, 822, 311, 1848, 1337, 1850, 823, 829, 1341, 319, 1856, 836, 1861, 1862, 327, 1354,\n"," 331, 1869, 337, 1362, 339, 340, 855, 1880, 346, 358, 361, 878, 879, 374, 375, 1405, 1925,\n"," 392, 1420, 1421, 398, 399, 1936, 1426, 1939, 1428, 1941, 1940, 1432, 1946, 411, 1438,\n"," 1440, 420, 1449, 1961, 939, 428, 429, 431, 1457, 434, 1460, 436, 1462, 1974, 1981, 446,\n"," 447, 1986, 966, 456, 1993, 1480, 459, 1997, 975, 1488, 1489, 1490, 2002, 473, 2010, 1505,\n"," 995, 488, 490, 1517, 496, 501, 506, 507]\n","\n","res_relevant_indices['in.heating_fuel_res'] = \\\n","[1, 1527, 516, 522, 527, 17, 1554, 1555, 1556, 2071, 25, 2079, 2080, 1059, 548, 2093, 51,\n"," 2100, 2099, 578, 582, 75, 593, 598, 87, 601, 91, 104, 617, 618, 1131, 110, 117, 118,\n"," 1655, 119, 122, 123, 1659, 124, 637, 638, 1664, 640, 642, 131, 643, 639, 646, 1158, 1162,\n"," 652, 1166, 656, 657, 658, 674, 678, 174, 688, 179, 693, 696, 697, 190, 191, 702, 193,\n"," 198, 714, 202, 716, 1741, 719, 1232, 721, 1235, 724, 213, 726, 728, 1755, 731, 734, 736,\n"," 755, 1779, 243, 1278, 255, 259, 267, 268, 1300, 794, 1820, 1822, 290, 802, 807, 303, 307,\n"," 311, 832, 321, 323, 835, 329, 337, 1873, 340, 1876, 349, 1885, 351, 1374, 358, 360, 367,\n"," 371, 387, 1924, 389, 398, 400, 402, 1943, 1946, 412, 416, 418, 419, 421, 423, 428, 1965,\n"," 1966, 1456, 439, 1978, 444, 445, 447, 449, 1991, 461, 1498, 1500, 476, 1503, 1504, 2017,\n"," 481, 493, 495, 503, 1532]\n","\n","res_relevant_indices['in.tenure_res'] = \\\n","[0, 1, 3, 1542, 1548, 525, 1551, 17, 19, 532, 23, 536, 25, 30, 547, 38, 49, 51, 54, 1079,\n"," 60, 573, 66, 75, 591, 594, 84, 90, 93, 95, 1127, 1128, 104, 1133, 110, 624, 1650, 1651,\n"," 118, 632, 120, 634, 121, 122, 1659, 123, 1664, 641, 131, 139, 652, 660, 661, 155, 673,\n"," 1186, 1190, 166, 168, 172, 173, 685, 1719, 183, 1220, 1224, 716, 1741, 208, 721, 726,\n"," 1240, 222, 1758, 1759, 1253, 1775, 1269, 757, 760, 1791, 1279, 1282, 260, 261, 263, 265,\n"," 267, 1807, 273, 1810, 1297, 277, 278, 279, 1813, 801, 1314, 1829, 1321, 303, 1330, 307,\n"," 822, 310, 825, 826, 1850, 831, 323, 836, 835, 1356, 337, 340, 1367, 1370, 1883, 1885,\n"," 861, 1887, 1889, 1380, 358, 359, 1385, 1898, 366, 1391, 370, 375, 1405, 383, 384, 1409,\n"," 1926, 391, 1928, 393, 394, 1931, 1421, 1422, 1936, 1428, 1429, 1943, 1433, 1946, 410,\n"," 928, 420, 1961, 1449, 430, 1966, 944, 942, 1972, 437, 950, 1975, 1462, 1467, 447, 959,\n"," 1983, 962, 451, 963, 965, 1479, 967, 1995, 1998, 1495, 1497, 2010, 1505, 2020, 491, 492]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.864183Z","iopub.status.idle":"2024-10-08T12:15:00.864699Z","shell.execute_reply":"2024-10-08T12:15:00.864465Z","shell.execute_reply.started":"2024-10-08T12:15:00.864441Z"},"trusted":true},"outputs":[],"source":["res_relevant_indices['in.vacancy_status_res'] = \\\n","[6, 18, 1554, 19, 24, 2075, 2076, 2079, 33, 2082, 36, 2084, 2085, 39, 42, 45, 51, 1589,\n"," 54, 57, 1594, 63, 576, 577, 1089, 69, 72, 75, 78, 81, 87, 89, 91, 92, 94, 96, 97, 619,\n"," 622, 1139, 116, 1144, 1664, 131, 132, 1672, 1681, 155, 1180, 174, 177, 178, 693, 183,\n"," 185, 186, 187, 189, 191, 1227, 724, 1752, 221, 734, 736, 1251, 1252, 749, 1776, 1777,\n"," 1778, 757, 1784, 1785, 1792, 1796, 1798, 263, 264, 266, 267, 273, 794, 795, 796, 1311,\n"," 287, 802, 806, 807, 1836, 1331, 1845, 1340, 828, 830, 831, 833, 834, 835, 836, 1865,\n"," 1365, 1890, 358, 365, 378, 411, 1436, 1961, 1980, 447, 995, 2019, 1000, 1001, 2025, 1523]\n","\n","res_relevant_indices['in.vintage_res'] = \\\n","[1, 2053, 517, 518, 10, 11, 526, 527, 1556, 536, 2075, 1565, 1567, 1570, 547, 2089,\n"," 555, 560, 2097, 1074, 564, 567, 1080, 56, 569, 1600, 577, 72, 1608, 587, 591, 596,\n"," 602, 613, 620, 1133, 1134, 623, 624, 1135, 116, 117, 1656, 634, 123, 1147, 637, 1662,\n"," 640, 643, 133, 647, 652, 656, 665, 676, 682, 687, 177, 689, 692, 694, 697, 185, 190,\n"," 191, 1729, 1217, 709, 1222, 711, 1227, 1743, 723, 1748, 1242, 1755, 225, 1765, 231,\n"," 1258, 237, 755, 1267, 246, 1274, 1791, 773, 264, 777, 266, 267, 273, 275, 1305, 794,\n"," 795, 1821, 1824, 1826, 1314, 295, 1832, 1834, 1839, 1327, 818, 306, 307, 1842, 311,\n"," 1336, 1338, 827, 1851, 318, 1345, 323, 327, 1864, 1865, 1353, 329, 333, 1361, 338,\n"," 339, 1364, 340, 1366, 345, 346, 347, 1376, 353, 354, 355, 356, 1889, 1896, 1901, 1392,\n"," 1395, 1396, 1397, 379, 1403, 1406, 1410, 1923, 1925, 389, 392, 1417, 396, 1935, 1425,\n"," 1426, 1429, 1943, 407, 1945, 1434, 1951, 432, 1460, 438, 1980, 445, 447, 448, 1985, 450,\n"," 453, 454, 455, 1992, 970, 460, 465, 466, 467, 2004, 1492, 1496, 477, 2018, 483, 996,\n"," 2021, 994, 488, 2029, 494, 507, 1531, 508]\n","\n","res_relevant_indices['in.bedrooms_res'] = \\\n","[1, 2050, 1538, 516, 1539, 1034, 10, 525, 2063, 2066, 19, 1558, 25, 30, 1568, 36, 1574,\n"," 45, 46, 49, 52, 54, 2103, 1080, 1084, 67, 72, 83, 604, 617, 1643, 1133, 110, 622, 624,\n"," 1137, 1136, 116, 635, 123, 1659, 124, 132, 648, 655, 1172, 661, 155, 1182, 1184, 674,\n"," 165, 173, 177, 178, 179, 182, 185, 186, 187, 189, 190, 707, 711, 1224, 716, 1741, 1743,\n"," 1746, 731, 1759, 1249, 1262, 754, 1778, 253, 1790, 767, 1285, 265, 266, 267, 268, 1294,\n"," 275, 278, 280, 287, 288, 801, 290, 294, 806, 298, 1836, 1837, 813, 814, 303, 1328, 815,\n"," 307, 817, 1327, 1332, 824, 826, 315, 1850, 830, 319, 834, 323, 1860, 1863, 327, 343,\n"," 1883, 1885, 352, 1890, 357, 1898, 1387, 367, 1397, 1407, 1408, 388, 1414, 1928, 1933,\n"," 1937, 1426, 1430, 1432, 409, 1946, 1433, 415, 1953, 1442, 1450, 1453, 944, 1970, 947,\n"," 436, 951, 954, 444, 958, 959, 962, 963, 1475, 967, 1483, 1491, 2011, 2015, 995, 2020,\n"," 998, 1005, 1007, 1523, 1012]\n","\n","res_relevant_indices['in.cooling_setpoint_res'] = \\\n","[1, 2, 517, 2055, 521, 11, 528, 1552, 2067, 537, 25, 2077, 1569, 547, 2083, 1573, 1574,\n"," 551, 2089, 1071, 51, 564, 2100, 2099, 2103, 1080, 1087, 75, 1615, 81, 594, 596, 84,\n"," 1626, 608, 611, 100, 615, 1136, 116, 630, 122, 123, 1146, 635, 637, 638, 640, 1156,\n"," 649, 653, 662, 1176, 1695, 672, 673, 1709, 686, 689, 691, 692, 693, 180, 695, 697, 186,\n"," 187, 190, 191, 1729, 1730, 1225, 210, 213, 728, 729, 730, 220, 1759, 1775, 756, 1783,\n"," 1272, 1791, 258, 260, 1284, 268, 1807, 271, 277, 794, 287, 1312, 1313, 1314, 1316, 1829,\n"," 805, 1319, 1321, 1833, 1837, 1838, 1841, 1329, 307, 1331, 1843, 1848, 825, 826, 1853,\n"," 832, 833, 835, 1866, 1870, 1872, 337, 1360, 339, 347, 1884, 1373, 1371, 351, 352, 1377,\n"," 355, 1894, 1383, 363, 1900, 368, 1392, 1396, 1399, 1913, 379, 380, 1405, 1411, 1924,\n"," 1928, 1929, 1417, 1930, 1933, 1421, 1424, 406, 1433, 410, 1947, 412, 1437, 415, 1439,\n"," 419, 1958, 1960, 1962, 1963, 1964, 428, 430, 437, 1465, 1467, 1468, 445, 1980, 446,\n"," 448, 449, 450, 455, 1998, 477, 480, 481, 488, 496, 500, 508, 510]\n","\n","res_relevant_indices['in.heating_setpoint_res'] = \\\n","[1, 514, 518, 1542, 1554, 1558, 537, 2076, 2078, 2079, 33, 2082, 1059, 36, 1060, 39, 552,\n"," 45, 1071, 51, 564, 572, 574, 63, 1600, 577, 1090, 1601, 66, 1602, 583, 72, 586, 588,\n"," 592, 594, 1618, 595, 598, 599, 600, 89, 607, 612, 613, 614, 1133, 1134, 116, 118, 1654,\n"," 634, 123, 122, 1147, 1662, 639, 131, 1681, 155, 1192, 681, 177, 693, 183, 185, 186, 187,\n"," 189, 190, 1213, 1219, 1224, 1738, 205, 728, 1752, 1754, 1755, 1251, 1253, 1254, 1776,\n"," 753, 1778, 1792, 1793, 264, 265, 267, 1803, 269, 271, 1807, 277, 1819, 796, 287, 802,\n"," 803, 1826, 1828, 299, 1326, 1838, 307, 1848, 825, 1340, 829, 830, 831, 832, 834, 835,\n"," 836, 1861, 327, 1352, 1865, 1355, 1870, 1871, 337, 345, 1883, 1887, 353, 355, 1898, 1389,\n"," 369, 1410, 387, 1925, 1933, 1422, 401, 1943, 1435, 415, 1449, 1450, 427, 428, 426, 1458,\n"," 1983, 1985, 453, 454, 974, 1490, 2002, 1492, 469, 1494, 474, 987, 476, 479, 482, 486,\n"," 2023, 491, 2029, 494, 499, 508]"]},{"cell_type":"markdown","metadata":{},"source":["## Residential Building Hyperparameter Tuning\n","Defining optimised hyperparameter values necessary for an improved performance of each model dedicated for each residential building target type.\n","These were identified as part of an optuna optimisation process"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.867057Z","iopub.status.idle":"2024-10-08T12:15:00.867701Z","shell.execute_reply":"2024-10-08T12:15:00.867417Z","shell.execute_reply.started":"2024-10-08T12:15:00.867384Z"},"trusted":true},"outputs":[],"source":["tuned_params_res = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.869720Z","iopub.status.idle":"2024-10-08T12:15:00.870353Z","shell.execute_reply":"2024-10-08T12:15:00.870077Z","shell.execute_reply.started":"2024-10-08T12:15:00.870047Z"},"trusted":true},"outputs":[],"source":["# 0.6958745369005935 f1: \n","tuned_params_res['in.geometry_building_type_recs_res'] =\\\n","{'n_estimators': 934, 'max_depth': 3, 'num_leaves': 123,\n"," 'learning_rate': 0.2945480376671004, 'min_child_samples': 34,\n"," 'min_child_weight': 6.167802479295942, 'subsample': 0.8909410473215246,\n"," 'scale_pos_weight': 4, 'colsample_bytree': 0.7543807422433793,\n"," 'reg_alpha': 0.00011906921825675507, 'reg_lambda': 4.8485395544580945e-08}\n","\n","# 0.5188565122370529 f1: \n","tuned_params_res['in.geometry_foundation_type_res'] =\\\n","{'n_estimators': 922, 'max_depth': 28, 'num_leaves': 103,\n"," 'learning_rate': 0.09849562522357214, 'min_child_samples': 89,\n"," 'min_child_weight': 6.187944536726171, 'subsample': 0.8149723490572682,\n"," 'scale_pos_weight': 4, 'colsample_bytree': 0.4426110188002679,\n"," 'reg_alpha': 0.00010014981523703602, 'reg_lambda': 0.00023407513126556}\n","\n","# 0.3480546863006969 f1\n","tuned_params_res['in.geometry_floor_area_res'] = \\\n","{'n_estimators': 910, 'max_depth': 25, 'num_leaves': 102,\n"," 'learning_rate': 0.07279111345034658, 'min_child_samples': 63,\n"," 'min_child_weight': 3.2526461228236947, 'subsample': 0.9480222295341681,\n"," 'scale_pos_weight': 4, 'colsample_bytree': 0.9628635623120934,\n"," 'reg_alpha': 0.00027927416239330717, 'reg_lambda': 0.004003317689316492}\n","\n","# 0.7699896053788151 f1: \n","tuned_params_res['in.geometry_wall_type_res'] =\\\n","{'n_estimators': 600, 'max_depth': 3, 'num_leaves': 104,\n"," 'learning_rate': 0.24648477843229688, 'min_child_samples': 65,\n"," 'min_child_weight': 1.6775679111282833, 'subsample': 0.4329242857212138,\n"," 'scale_pos_weight': 5, 'colsample_bytree': 0.4003413875429718,\n"," 'reg_alpha': 0.01216804184830573, 'reg_lambda': 4.694397179386468e-05}\n","\n","# 0.10280264644713603 f1: \n","tuned_params_res['in.income_res'] =\\\n","{'n_estimators': 684, 'max_depth': 13, 'num_leaves': 117,\n"," 'learning_rate': 0.14516046183367184, 'min_child_samples': 19,\n"," 'min_child_weight': 7.227114697099523, 'subsample': 0.9226131920085651,\n"," 'scale_pos_weight': 6, 'colsample_bytree': 0.6485099688165997,\n"," 'reg_alpha': 3.024497277471494e-05, 'reg_lambda': 0.00023040655178930457}\n","\n","# 0.5404408272513648 f1\n","tuned_params_res['in.roof_material_res'] = \\\n","{'n_estimators': 803, 'max_depth': 24, 'num_leaves': 145,\n"," 'learning_rate': 0.016910825931978325, 'min_child_samples': 40,\n"," 'min_child_weight': 7.771112155902316, 'subsample': 0.44775552808959174,\n"," 'scale_pos_weight': 3, 'colsample_bytree': 0.6769161871481143,\n"," 'reg_alpha': 2.6435425761672062e-08, 'reg_lambda': 0.008235783409096475}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.871700Z","iopub.status.idle":"2024-10-08T12:15:00.872318Z","shell.execute_reply":"2024-10-08T12:15:00.872045Z","shell.execute_reply.started":"2024-10-08T12:15:00.871990Z"},"trusted":true},"outputs":[],"source":["# 0.6888482971159382  f1: \n","tuned_params_res['in.heating_fuel_res'] =\\\n","{'n_estimators': 335, 'max_depth': 7, 'num_leaves': 51,\n"," 'learning_rate': 0.12963879325098038, 'min_child_samples': 33,\n"," 'min_child_weight': 6.257192439904964, 'subsample': 0.5425612366413691,\n"," 'scale_pos_weight': 6, 'colsample_bytree': 0.7958368532059044,\n"," 'reg_alpha': 5.97882134647585e-08, 'reg_lambda': 3.173451776830115e-05}\n","\n","#0.7702281848379628 f1: \n","tuned_params_res['in.tenure_res'] =\\\n","{'n_estimators': 930, 'max_depth': 27, 'num_leaves': 183,\n"," 'learning_rate': 0.05958169486618951, 'min_child_samples': 67,\n"," 'min_child_weight': 2.0933971144187558, 'subsample': 0.5822419223973381,\n"," 'scale_pos_weight': 2, 'colsample_bytree': 0.5297643605000292,\n"," 'reg_alpha': 3.0331132949376417e-05, 'reg_lambda': 1.3852796948184832e-05}\n","\n","# 0.9976882365296991 f1: \n","tuned_params_res['in.vacancy_status_res'] =\\\n","{'n_estimators': 767, 'max_depth': 1, 'num_leaves': 174,\n"," 'learning_rate': 0.16599487617300912, 'min_child_samples': 90,\n"," 'min_child_weight': 0.7710019776202889, 'subsample': 0.5317999966923885,\n"," 'scale_pos_weight': 4, 'colsample_bytree': 0.4505082905425865,\n"," 'reg_alpha': 3.792576185268018e-05, 'reg_lambda': 0.007801565273536475}\n","\n","# 0.2048327110306317 f1\n","tuned_params_res['in.vintage_res'] = \\\n","{'n_estimators': 740, 'max_depth': 35, 'num_leaves': 35,\n"," 'learning_rate': 0.09298959300141071, 'min_child_samples': 92,\n"," 'min_child_weight': 8.47911871022102, 'subsample': 0.40431044203566197,\n"," 'scale_pos_weight': 5, 'colsample_bytree': 0.6864178335889475,\n"," 'reg_alpha': 0.43398145576873853, 'reg_lambda': 6.576364002484857e-07}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.874345Z","iopub.status.idle":"2024-10-08T12:15:00.874962Z","shell.execute_reply":"2024-10-08T12:15:00.874680Z","shell.execute_reply.started":"2024-10-08T12:15:00.874647Z"},"trusted":true},"outputs":[],"source":["# 0.5075012873030607 f1: \n","tuned_params_res['in.bedrooms_res'] =\\\n","{'n_estimators': 735, 'max_depth': 34, 'num_leaves': 241,\n"," 'learning_rate': 0.060871086872348566, 'min_child_samples': 69,\n"," 'min_child_weight': 0.4737265426136331, 'subsample': 0.8986333166383806,\n"," 'scale_pos_weight': 5, 'colsample_bytree': 0.8935561882173697,\n"," 'reg_alpha': 0.00041647170063019743, 'reg_lambda': 1.4111607171914601e-05}\n","\n","# 0.2523593670534726  f1: \n","tuned_params_res['in.cooling_setpoint_res'] =\\\n","{'n_estimators': 480, 'max_depth': 11, 'num_leaves': 97,\n"," 'learning_rate': 0.08270776904857623, 'min_child_samples': 48,\n"," 'min_child_weight': 5.010895136096351, 'subsample': 0.5761480486298616,\n"," 'scale_pos_weight': 3, 'colsample_bytree': 0.9262180465285766,\n"," 'reg_alpha': 2.9482770022566384e-07, 'reg_lambda': 0.005365434491707699}\n","\n","#0.33351148735202124 f1: \n","tuned_params_res['in.heating_setpoint_res'] =\\\n","{'n_estimators': 941, 'max_depth': 1, 'num_leaves': 124,\n"," 'learning_rate': 0.2776918947250579, 'min_child_samples': 62,\n"," 'min_child_weight': 7.94405862197077, 'subsample': 0.9164496651494268,\n"," 'scale_pos_weight': 2, 'colsample_bytree': 0.9702796578314632,\n"," 'reg_alpha': 0.005468991713660568, 'reg_lambda': 0.007772734581335458}"]},{"cell_type":"markdown","metadata":{},"source":["## Developing Models for Residential Buildings"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.878392Z","iopub.status.idle":"2024-10-08T12:15:00.879038Z","shell.execute_reply":"2024-10-08T12:15:00.878735Z","shell.execute_reply.started":"2024-10-08T12:15:00.878703Z"},"trusted":true},"outputs":[],"source":["def fit_residential_models(df, min_occurrences=6):\n","    \"\"\"\n","    Trains classification models for each residential building target type in the dataset.\n","\n","    This function iterates through each unique target type in the dataset, performs data \n","    preprocessing and feature selection, trains the model using pre-optimized hyperparameters, \n","    evaluates the model using stratified k-fold cross-validation, and stores the trained models \n","    and preprocessing pipelines.\n","\n","    Parameters\n","    ----------\n","    df : pd.DataFrame\n","        A dataframe containing features and target values for different residential building types.\n","        It should have at least the following columns: 'building_stock_type', 'target', and \n","        'target_type'.\n","\n","    min_occurrences : int, optional, default=6\n","        The minimum number of occurrences required for each target class to be included in the model\n","        training process. Classes with fewer occurrences are excluded.\n","\n","    Returns\n","    -------\n","    models_dict : dict\n","        A dictionary where the keys are the residential building target types, and the \n","        values are lists of trained model pipelines (one model per fold of cross-validation).\n","\n","    enc_dict : dict\n","        A dictionary where the keys are the residential building target types, and the values are\n","        `LabelEncoder` objects used to encode the target labels.\n","\n","    prepro_dict : dict\n","        A dictionary where the keys are the residential building target types, and the values are\n","        preprocessing pipelines used to prepare the data before model training.\n","\n","    Notes\n","    -----\n","    - The model uses LightGBM (LGBMClassifier) with optimized hyperparameters for each target type.\n","    - Categorical features such as 'peak_timeofday' and 'build_state' are encoded using \n","    target encoding.\n","    - The models are evaluated using weighted F1-score for cross-validation.\n","    \"\"\"\n","\n","    # Initialize dictionaries to store models, encoders, and preprocessing pipelines\n","    models_dict = {}\n","    enc_dict = {}\n","    prepro_dict = {}\n","    f1_scores = []\n","\n","    for target_type in df[\"target_type\"].unique():\n","        # Subset the dataframe for the current target type\n","        sub_df = df[df[\"target_type\"] == target_type]\n","\n","        # Select target classes that have more than `min_occurrences`\n","        selected_classes = [\n","            targ\n","            for targ in sub_df[\"target\"].unique()\n","            if sub_df[sub_df[\"target\"] == targ].shape[0] > min_occurrences\n","        ]\n","        filtered_df = sub_df[sub_df[\"target\"].isin(selected_classes)]\n","\n","        # Separate features (X) and target (y)\n","        X = filtered_df.drop(\n","            [\"building_stock_type\", \"target\", \"target_type\"], axis=1\n","        ).reset_index(drop=True)\n","        y = filtered_df[\"target\"]\n","\n","        # Encode target labels\n","        label_encoder = LabelEncoder()\n","        y_encoded = pd.Series(label_encoder.fit_transform(y))\n","\n","        # Data preprocessing pipeline: TargetEncoder for selected categorical features,\n","        # followed by MinMaxScaler\n","        preprocessing_pipeline = Pipeline(\n","            [\n","                (\n","                    \"target_encoding\",\n","                    ColumnTransformer(\n","                        [\n","                            (\n","                                \"target_encoder\",\n","                                TargetEncoder(),\n","                                [\"peak_timeofday\", \"build_state\"],\n","                            )\n","                        ],\n","                        remainder=\"passthrough\",\n","                    ),\n","                ),\n","                (\"scaling\", MinMaxScaler()),\n","            ]\n","        )\n","\n","        # Fit and transform the features using the preprocessing pipeline\n","        prepped_X = preprocessing_pipeline.fit_transform(X, y_encoded)\n","\n","        # Filter features based on relevance (using pre-defined feature indices)\n","        relevant_features = prepped_X[:, res_relevant_indices[target_type]]\n","\n","        # Load hyperparameters for the current target type, or use default parameters\n","        target_hyperparams = tuned_params_res.get(target_type)\n","        model_params = target_hyperparams\n","\n","        # Determine the task type (binary or multiclass classification)\n","        if y.nunique() > 2:\n","            task_type = \"multiclass\"\n","            num_classes = y.nunique()\n","        else:\n","            task_type = \"binary\"\n","            num_classes = 1\n","\n","        # Cross-validation setup: StratifiedKFold\n","        stratified_kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n","        models_per_fold = []\n","        oof_predictions = []\n","\n","        # Train and evaluate models using cross-validation\n","        for train_idx, test_idx in stratified_kfold.split(relevant_features, y_encoded):\n","            X_train, X_test = relevant_features[train_idx], relevant_features[test_idx]\n","            y_train, y_test = y_encoded.iloc[train_idx], y_encoded.iloc[test_idx]\n","\n","            model_pipeline = Pipeline(\n","                [\n","                    (\n","                        \"classifier\",\n","                        LGBMClassifier(\n","                            verbose=-1,\n","                            device_type=\"cpu\",\n","                            objective=task_type,\n","                            num_class=num_classes,\n","                            random_state=42,\n","                            **model_params,\n","                        ),\n","                    )\n","                ]\n","            )\n","\n","            model_pipeline.fit(X_train, y_train)\n","            y_pred = model_pipeline.predict(X_test)\n","            weighted_f1 = f1_score(y_test, y_pred, average=\"weighted\")\n","            oof_predictions.append(weighted_f1)\n","            models_per_fold.append(model_pipeline)\n","\n","        # Calculate and print the average F1 score for this target type\n","        mean_f1 = np.mean(oof_predictions)\n","        print(f\"{target_type} test F1: {mean_f1}\")\n","\n","        # Store the models, encoders, and preprocessing pipelines in their respective dictionaries\n","        models_dict[target_type] = models_per_fold\n","        enc_dict[target_type] = label_encoder\n","        prepro_dict[target_type] = preprocessing_pipeline\n","\n","        f1_scores.append(mean_f1)\n","\n","    # Print and return the average F1 score across all target types\n","    print(f\"Average F1 score: {np.mean(f1_scores)}\")\n","    return models_dict, enc_dict, prepro_dict\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.880256Z","iopub.status.idle":"2024-10-08T12:15:00.880862Z","shell.execute_reply":"2024-10-08T12:15:00.880584Z","shell.execute_reply.started":"2024-10-08T12:15:00.880554Z"},"trusted":true},"outputs":[],"source":["resclas_models,resclas_encs,resclas_prepro = fit_residential_models(combo_res_clas_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Onward Test"]},{"cell_type":"markdown","metadata":{},"source":["## Reading and Manipulating Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.883216Z","iopub.status.idle":"2024-10-08T12:15:00.883824Z","shell.execute_reply":"2024-10-08T12:15:00.883550Z","shell.execute_reply.started":"2024-10-08T12:15:00.883520Z"},"trusted":true},"outputs":[],"source":["def create_test_combo(df, target_names):\n","    \"\"\"\n","    Combine all features into a format for one-time prediction on multiple target types.\n","\n","    This function creates copies of the input DataFrame `df`, appends a new column called \n","    `target_type` for each target in `target_names`, and then concatenates the resulting \n","    DataFrames into a single DataFrame.\n","\n","    Parameters\n","    ----------\n","    df : pandas.DataFrame\n","        The input DataFrame containing features used for prediction.\n","\n","    target_names : list of str\n","        A list of target names to append as a new column 'target_type' in the DataFrame.\n","\n","    Returns\n","    -------\n","    pandas.DataFrame\n","        A concatenated DataFrame with an added 'target_type' column for each target in \n","        `target_names`.\n","\n","    Examples\n","    --------\n","    >>> res_df = pd.DataFrame({'feature1': [1, 2], 'feature2': [3, 4]})\n","    >>> res_targets = ['target1', 'target2']\n","    >>> combo_res_class_df = create_test_combo(res_df, res_targets)\n","    >>> print(combo_res_class_df)\n","       feature1  feature2 target_type\n","    0         1         3     target1\n","    1         2         4     target1\n","    0         1         3     target2\n","    1         2         4     target2\n","\n","    \"\"\"\n","    # Initialize a list to store copies of DataFrames with appended target types\n","    comb_clas_dfs = []\n","\n","    # Loop through each target in target_names\n","    for target in target_names:\n","        # Create a copy of the input DataFrame to avoid modifying the original\n","        new_df = df.copy()\n","\n","        # Add the 'target_type' column with the current target\n","        new_df[\"target_type\"] = target\n","\n","        # Append the modified DataFrame to the list\n","        comb_clas_dfs.append(new_df)\n","\n","    # Concatenate all the DataFrames into one along the row axis (axis=0)\n","    combined_df = pd.concat(comb_clas_dfs, axis=0)\n","\n","    return combined_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.885334Z","iopub.status.idle":"2024-10-08T12:15:00.885990Z","shell.execute_reply":"2024-10-08T12:15:00.885661Z","shell.execute_reply.started":"2024-10-08T12:15:00.885631Z"},"trusted":true},"outputs":[],"source":["def expand_targets_wide(df):\n","    \"\"\"\n","    Expands the input DataFrame by pivoting it to create multiple columns, \n","    each corresponding to a unique target type, instead of a single column for all targets.\n","\n","    Parameters\n","    ----------\n","    df : pandas.DataFrame\n","        The input DataFrame, which contains at least the following columns:\n","        - 'bldg_id': Identifier for each building.\n","        - 'target_type': The type/category of the target (used as columns in the output).\n","        - 'target': The actual target values (used as the values in the output).\n","\n","    Returns\n","    -------\n","    reshaped_df : pandas.DataFrame\n","        A reshaped DataFrame where each unique 'target_type' becomes a separate column, \n","        and 'bldg_id' is used as the index. The columns represent different target types, \n","        and the values in those columns represent the corresponding target values.\n","\n","    Example\n","    -------\n","    >>> reshaped = expand_targets_wide(btest_comm)\n","    \"\"\"\n","    reshaped_df = df.pivot(index='bldg_id', columns='target_type', values='target').reset_index()\n","\n","    # Remove the name for the columns index (pivot table metadata)\n","    reshaped_df.columns.name = None\n","\n","    return reshaped_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.887901Z","iopub.status.idle":"2024-10-08T12:15:00.888545Z","shell.execute_reply":"2024-10-08T12:15:00.888222Z","shell.execute_reply.started":"2024-10-08T12:15:00.888188Z"},"trusted":true},"outputs":[],"source":["# although test data is 1440, using n_files of 1440 seemed to produce a dataframe\n","# short of 10 buildings so 1450 is used when reading test building files\n","# throughout the notebook\n","test_data = prepare_data(TEST_DIR,n_files=1450,is_test=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.920135Z","iopub.status.idle":"2024-10-08T12:15:00.920672Z","shell.execute_reply":"2024-10-08T12:15:00.920428Z","shell.execute_reply.started":"2024-10-08T12:15:00.920404Z"},"trusted":true},"outputs":[],"source":["tsf_df = ts_fresh_prep(TEST_DIR,n_files=1450)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.923377Z","iopub.status.idle":"2024-10-08T12:15:00.924079Z","shell.execute_reply":"2024-10-08T12:15:00.923739Z","shell.execute_reply.started":"2024-10-08T12:15:00.923707Z"},"trusted":true},"outputs":[],"source":["# Extract features separately\n","extracted_features_test = extract_features(tsf_df, column_id=\"bldg_id\", column_sort=\"time\",\n","                                     default_fc_parameters=EfficientFCParameters(),\n","                                        n_jobs=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.926569Z","iopub.status.idle":"2024-10-08T12:15:00.927260Z","shell.execute_reply":"2024-10-08T12:15:00.926939Z","shell.execute_reply.started":"2024-10-08T12:15:00.926906Z"},"trusted":true},"outputs":[],"source":["extracted_features_test['bldg_id'] = extracted_features_test.index"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.929851Z","iopub.status.idle":"2024-10-08T12:15:00.930677Z","shell.execute_reply":"2024-10-08T12:15:00.930441Z","shell.execute_reply.started":"2024-10-08T12:15:00.930400Z"},"trusted":true},"outputs":[],"source":["extracted_features_test = extracted_features_test[extracted_features.columns]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.940453Z","iopub.status.idle":"2024-10-08T12:15:00.942069Z","shell.execute_reply":"2024-10-08T12:15:00.941642Z","shell.execute_reply.started":"2024-10-08T12:15:00.941571Z"},"trusted":true},"outputs":[],"source":["test_data = pd.merge(test_data,extracted_features_test,how='left',on='bldg_id')\n","test_data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.944966Z","iopub.status.idle":"2024-10-08T12:15:00.946003Z","shell.execute_reply":"2024-10-08T12:15:00.945702Z","shell.execute_reply.started":"2024-10-08T12:15:00.945668Z"},"trusted":true},"outputs":[],"source":["df_15min_test = prep_15min(TEST_DIR,n_files=1450)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.947582Z","iopub.status.idle":"2024-10-08T12:15:00.948616Z","shell.execute_reply":"2024-10-08T12:15:00.948295Z","shell.execute_reply.started":"2024-10-08T12:15:00.948259Z"},"trusted":true},"outputs":[],"source":["test_data = pd.merge(test_data,df_15min_test,how='left',on='bldg_id')\n","test_data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.950884Z","iopub.status.idle":"2024-10-08T12:15:00.951900Z","shell.execute_reply":"2024-10-08T12:15:00.951609Z","shell.execute_reply.started":"2024-10-08T12:15:00.951558Z"},"trusted":true},"outputs":[],"source":["tsf_15min_test = df15min_tsfprep(df_15min_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.953656Z","iopub.status.idle":"2024-10-08T12:15:00.954641Z","shell.execute_reply":"2024-10-08T12:15:00.954346Z","shell.execute_reply.started":"2024-10-08T12:15:00.954314Z"},"trusted":true},"outputs":[],"source":["#Extract features\n","extracted_15min_test = extract_features(tsf_15min_test, column_id=\"bldg_id\", \n","                                        column_sort=\"time\", n_jobs=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.956408Z","iopub.status.idle":"2024-10-08T12:15:00.957402Z","shell.execute_reply":"2024-10-08T12:15:00.957117Z","shell.execute_reply.started":"2024-10-08T12:15:00.957084Z"},"trusted":true},"outputs":[],"source":["extracted_15min_test['bldg_id'] = extracted_15min_test.index\n","\n","extracted_15min_test = extracted_15min_test[extracted_15min.columns]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.959159Z","iopub.status.idle":"2024-10-08T12:15:00.960141Z","shell.execute_reply":"2024-10-08T12:15:00.959833Z","shell.execute_reply.started":"2024-10-08T12:15:00.959801Z"},"trusted":true},"outputs":[],"source":["test_data = pd.merge(test_data,extracted_15min_test,how='left',on='bldg_id')\n","test_data"]},{"cell_type":"markdown","metadata":{},"source":["## Predicting Building Stock Type of Test Data Buildings"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.961984Z","iopub.status.idle":"2024-10-08T12:15:00.962992Z","shell.execute_reply":"2024-10-08T12:15:00.962709Z","shell.execute_reply.started":"2024-10-08T12:15:00.962676Z"},"trusted":true},"outputs":[],"source":["prepped_test = type_prep_pipe.transform(test_data[X.columns])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.964802Z","iopub.status.idle":"2024-10-08T12:15:00.965764Z","shell.execute_reply":"2024-10-08T12:15:00.965473Z","shell.execute_reply.started":"2024-10-08T12:15:00.965440Z"},"trusted":true},"outputs":[],"source":["finaltest = prepped_test[:,true_indexes]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.967649Z","iopub.status.idle":"2024-10-08T12:15:00.968765Z","shell.execute_reply":"2024-10-08T12:15:00.968267Z","shell.execute_reply.started":"2024-10-08T12:15:00.968214Z"},"trusted":true},"outputs":[],"source":["# predict building stock type\n","stock_type_preds = []\n","for cmodel in cmodels:\n","    prediction = cmodel.predict(finaltest)\n","    stock_type_preds.append(prediction)\n","    \n","predictions = pd.DataFrame(stock_type_preds)\n","predictions = predictions.mode()\n","predictions = predictions.T\n","\n","test_data['building_stock_type'] = predictions\n","test_data['building_stock_type'] = test_data['building_stock_type'].map(\n","    {1:'residential',0:'commercial'}\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Predicting Commercial Building Targets of Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.971301Z","iopub.status.idle":"2024-10-08T12:15:00.972006Z","shell.execute_reply":"2024-10-08T12:15:00.971711Z","shell.execute_reply.started":"2024-10-08T12:15:00.971677Z"},"trusted":true},"outputs":[],"source":["# preparing commercial building data targets\n","test_commercial = test_data[test_data['building_stock_type']=='commercial'].copy()\n","test_commercial.drop(constant_cols,axis=1,inplace=True)\n","btest_comclas = create_test_combo(test_commercial,comm_targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.974306Z","iopub.status.idle":"2024-10-08T12:15:00.974992Z","shell.execute_reply":"2024-10-08T12:15:00.974696Z","shell.execute_reply.started":"2024-10-08T12:15:00.974665Z"},"trusted":true},"outputs":[],"source":["# predicting target values of each commercial building target type\n","btest_comclas_preds = []\n","for t_type in btest_comclas['target_type'].unique():\n","    temp_df = btest_comclas[btest_comclas['target_type']==t_type].copy()\n","    \n","    filtered_copy = temp_df.copy()\n","    temp_df_prep = comclas_prepro[t_type].transform(filtered_copy)\n","    filtered_tempdf = temp_df_prep[:,com_relevant_indices[t_type]]\n","    \n","    trained_models = comclas_models[t_type]\n","    target_preds = []\n","    for tmodel in trained_models:\n","        prediction = tmodel.predict(filtered_tempdf)\n","        target_preds.append(prediction)\n","\n","    predictions = pd.DataFrame(target_preds)\n","    predictions = predictions.mode()\n","    predictions = predictions.T\n","    \n","    \n","    temp_df.reset_index(drop=True,inplace=True)\n","    temp_df['target'] = predictions[0]\n","    temp_df['target'] = temp_df['target'].astype('int')\n","\n","    temp_df['target'] = comclas_encs[t_type].inverse_transform(temp_df['target'])\n","    btest_comclas_preds.append(temp_df)\n","    \n","btest_comclas = pd.concat(btest_comclas_preds,axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.977905Z","iopub.status.idle":"2024-10-08T12:15:00.978564Z","shell.execute_reply":"2024-10-08T12:15:00.978275Z","shell.execute_reply.started":"2024-10-08T12:15:00.978241Z"},"trusted":true},"outputs":[],"source":["# Viewing the Predictions\n","btest_comclas.iloc[:,-3:]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.980439Z","iopub.status.idle":"2024-10-08T12:15:00.982336Z","shell.execute_reply":"2024-10-08T12:15:00.981999Z","shell.execute_reply.started":"2024-10-08T12:15:00.981965Z"},"trusted":true},"outputs":[],"source":["# Expanding Prediction DataFrame into Expanded format where each target type has its own\n","# column\n","btest_comm = btest_comclas\n","btest_comm = expand_targets_wide(btest_comm)\n","btest_comm['building_stock_type'] = 'commercial'"]},{"cell_type":"markdown","metadata":{},"source":["## Predicting Residential Building Targets of Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.984263Z","iopub.status.idle":"2024-10-08T12:15:00.984918Z","shell.execute_reply":"2024-10-08T12:15:00.984623Z","shell.execute_reply.started":"2024-10-08T12:15:00.984589Z"},"trusted":true},"outputs":[],"source":["# predicting res targets\n","test_residential = test_data[test_data['building_stock_type']=='residential'].copy()\n","test_residential.drop(constant_cols,axis=1,inplace=True)\n","btest_resclas = create_test_combo(test_residential,res_targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.986876Z","iopub.status.idle":"2024-10-08T12:15:00.987517Z","shell.execute_reply":"2024-10-08T12:15:00.987238Z","shell.execute_reply.started":"2024-10-08T12:15:00.987208Z"},"trusted":true},"outputs":[],"source":["btest_resclas_preds = []\n","for t_type in btest_resclas['target_type'].unique():\n","    temp_df = btest_resclas[btest_resclas['target_type']==t_type].copy()\n","    \n","    filtered_copy = temp_df.copy()\n","    preptemp_df = resclas_prepro[t_type].transform(filtered_copy)\n","    filtered_tempdf = preptemp_df[:,res_relevant_indices[t_type]]\n","    \n","    trained_models = resclas_models[t_type]\n","    target_preds = []\n","    for tmodel in trained_models:\n","        prediction = tmodel.predict(filtered_tempdf)\n","        target_preds.append(prediction)\n","\n","    predictions = pd.DataFrame(target_preds)\n","    predictions = predictions.mode()\n","    predictions = predictions.T\n","    \n","    \n","    temp_df.reset_index(drop=True,inplace=True)\n","    temp_df['target'] = predictions[0]\n","    temp_df['target'] = temp_df['target'].astype('int')\n","    \n","    temp_df['target'] = resclas_encs[t_type].inverse_transform(temp_df['target'])\n","    btest_resclas_preds.append(temp_df)\n","btest_resclas = pd.concat(btest_resclas_preds,axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.991711Z","iopub.status.idle":"2024-10-08T12:15:00.992589Z","shell.execute_reply":"2024-10-08T12:15:00.992242Z","shell.execute_reply.started":"2024-10-08T12:15:00.992210Z"},"trusted":true},"outputs":[],"source":["btest_res = btest_resclas\n","\n","btest_res = expand_targets_wide(btest_res)\n","btest_res['building_stock_type'] = 'residential'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.994868Z","iopub.status.idle":"2024-10-08T12:15:00.995926Z","shell.execute_reply":"2024-10-08T12:15:00.995563Z","shell.execute_reply.started":"2024-10-08T12:15:00.995527Z"},"trusted":true},"outputs":[],"source":["total_testpred = pd.concat([btest_res,btest_comm],axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:00.997557Z","iopub.status.idle":"2024-10-08T12:15:00.998314Z","shell.execute_reply":"2024-10-08T12:15:00.997996Z","shell.execute_reply.started":"2024-10-08T12:15:00.997974Z"},"trusted":true},"outputs":[],"source":["total_testpred = total_testpred.fillna(value=str(None))\n","total_testpred.set_index('bldg_id',inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Applying Sample Submissioin Formatting."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:01.000985Z","iopub.status.idle":"2024-10-08T12:15:01.001945Z","shell.execute_reply":"2024-10-08T12:15:01.001650Z","shell.execute_reply.started":"2024-10-08T12:15:01.001618Z"},"trusted":true},"outputs":[],"source":["ss = pd.read_parquet(SS_FILE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:01.003410Z","iopub.status.idle":"2024-10-08T12:15:01.004559Z","shell.execute_reply":"2024-10-08T12:15:01.004238Z","shell.execute_reply.started":"2024-10-08T12:15:01.004204Z"},"trusted":true},"outputs":[],"source":["# ordering columns to fit sample submission format\n","total_testpred = total_testpred[ss.columns]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:01.006724Z","iopub.status.idle":"2024-10-08T12:15:01.007381Z","shell.execute_reply":"2024-10-08T12:15:01.007093Z","shell.execute_reply.started":"2024-10-08T12:15:01.007061Z"},"trusted":true},"outputs":[],"source":["for col in total_testpred.columns:\n","    total_testpred[col] = total_testpred[col].astype('str')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:01.008996Z","iopub.status.idle":"2024-10-08T12:15:01.009639Z","shell.execute_reply":"2024-10-08T12:15:01.009345Z","shell.execute_reply.started":"2024-10-08T12:15:01.009314Z"},"trusted":true},"outputs":[],"source":["for col in total_testpred.columns:\n","    total_testpred[col] = np.where(total_testpred[col]=='None',None,\n","                                   total_testpred[col])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:01.012240Z","iopub.status.idle":"2024-10-08T12:15:01.012727Z","shell.execute_reply":"2024-10-08T12:15:01.012520Z","shell.execute_reply.started":"2024-10-08T12:15:01.012497Z"},"trusted":true},"outputs":[],"source":["# Modifying the predicted values for certain columns where '.0' is supposed to be \n","# exluded in the decimal place of numeric values.\n","columns_to_modify = [\n","    'in.number_of_stories_com',\n","    'in.tstat_clg_sp_f..f_com',\n","    'in.tstat_htg_sp_f..f_com',\n","    'in.weekday_opening_time..hr_com',\n","    'in.weekday_operating_hours..hr_com',\n","    'in.bedrooms_res'\n","]\n","\n","for col in columns_to_modify:\n","    if col in total_testpred.columns:\n","        total_testpred[col] = total_testpred[col].str.replace('.0', '', regex=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:01.014140Z","iopub.status.idle":"2024-10-08T12:15:01.014933Z","shell.execute_reply":"2024-10-08T12:15:01.014565Z","shell.execute_reply.started":"2024-10-08T12:15:01.014506Z"},"trusted":true},"outputs":[],"source":["# exporting to submission file\n","total_testpred.to_parquet('scv_catselect_2xtsf.parquet')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T12:15:01.016709Z","iopub.status.idle":"2024-10-08T12:15:01.017278Z","shell.execute_reply":"2024-10-08T12:15:01.017066Z","shell.execute_reply.started":"2024-10-08T12:15:01.017033Z"},"trusted":true},"outputs":[],"source":["# Viewing the results\n","seeing = pd.read_parquet('scv_catselect_2xtsf.parquet')\n","seeing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5388214,"sourceId":8953392,"sourceType":"datasetVersion"},{"datasetId":5408092,"sourceId":8980997,"sourceType":"datasetVersion"},{"datasetId":5423903,"sourceId":9003442,"sourceType":"datasetVersion"},{"datasetId":5693947,"sourceId":9418360,"sourceType":"datasetVersion"},{"datasetId":5760752,"sourceId":9501348,"sourceType":"datasetVersion"},{"datasetId":5764176,"sourceId":9501414,"sourceType":"datasetVersion"}],"dockerImageVersionId":30776,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
