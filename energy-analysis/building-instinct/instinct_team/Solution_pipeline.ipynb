{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88451a0e",
   "metadata": {},
   "source": [
    "# Solution Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a567f",
   "metadata": {},
   "source": [
    "The solution is based on the ensemble of 2 CNN network architectures(tf_efficientnetv2_s_in21k and \n",
    "seresnext26t_32x4d) over 2d-1channel representations of energy consumption data. Data augmentation is applied during training and the simple ensemble is used, averaging the results of the different architecture models, trained on the differently preprocessed data.\n",
    "\n",
    "<img src=\"assets/model.png\" alt=\"Model Architecture Diagram\" width=\"600\"/>\n",
    "The solution is divided into two stages:\n",
    "\n",
    "1. **Classifying the building type** as either commercial or residential.\n",
    "2. **Performing multilabel classification** based on the result from stage 1.\n",
    "\n",
    "For stage 2, the model training followed a two-step process:\n",
    "\n",
    "- **Pretraining** models on an external dataset from the .\n",
    "- **Fine-tuning** the pretrained models on the challenge dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64462e4-2fcc-4874-a4a5-e41e3734c8e3",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "Linux system (tested on Ubuntu 22.04), python 3.9 with conda environment, tested on 3090 with the necessary CUDA drivers installed to run pytorch.\n",
    "\n",
    "Install dependencies, using python 3.9 with conda\n",
    "```\n",
    "$ conda create -n envs python=3.9\n",
    "$ conda activate envs\n",
    "$ pip install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "$ pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ff230",
   "metadata": {},
   "source": [
    "## Folder Structure\n",
    "```\n",
    "project/\n",
    "├── data/ \n",
    "│   ├── train : Contains the train files.\n",
    "|   ├── test : Contains the test files.\n",
    "|   ├── labels \n",
    "|       └──train_label.parquet\n",
    "├── src/\n",
    "│   ├── prepare_data.py \n",
    "│   │   └── Creates multilabel folds for training.\n",
    "│   ├── train_stage1.py \n",
    "│   │   └── Trains a classifier for building type (commercial/residential).\n",
    "│   ├── train_stage2.py \n",
    "│   │   └── Pretrains a multilabel classifier for a specific building type on external data.\n",
    "│   ├── train_stage3.py \n",
    "│   │   └── Fine-tunes the pretrained models on the challenge dataset.\n",
    "│   └── infer.py \n",
    "│       └── Performs inference on the challenge data and generates the submission file.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd69a019-fb1c-438e-8f45-06a9889e68fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c30dd5-5fa8-4ba7-b003-74b3fec4cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca90bf7-58e3-4936-aa6c-cf27feba0c71",
   "metadata": {},
   "source": [
    "## Pipeline Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efde55e-e97b-4d71-99e4-da1c3d9e8d8e",
   "metadata": {},
   "source": [
    "### 1. Encode Target Variables\n",
    "Before starting the training process, we need to encode the target variables using `LabelEncoder`. The encoded objects are saved in pickle format, allowing them to be easily reused in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee0afb-eff8-4ea5-8709-892a4b2011f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir=\"data\"\n",
    "filepath_labels =f\"{input_dir}/labels/train_label.parquet\" #path to the train label file\n",
    "df_targets = pd.read_parquet(filepath_labels, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da2d6d-7ee3-4026-9c76-6e13a3ed87a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{input_dir}/Label_encoded\",exist_ok=True)\n",
    "#Iterate through each target column and apply label encoding\n",
    "for col in df_targets.columns:\n",
    "    le = LabelEncoder()\n",
    "    df_targets[col] = le.fit_transform(df_targets[col])\n",
    "    # Save the encoder to a file\n",
    "    with open(f'{input_dir}/Label_encoded/{col}_label_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(le, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184c1686-08a6-4b9a-91c0-76846eb579ac",
   "metadata": {},
   "source": [
    "### 2. Download External Data\n",
    "### 2.a. Download external data for commercial buildings \n",
    "This section reproduces the downloading process of external data for commercial buildings from ***Open Energy Data Initiative (OEDI)***. The download is done in parallel  and ends at bldg_id = 47477 which is used in the final submission.\n",
    "\n",
    "We first download the  metadata file from s3 path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1acd58b-03fd-401b-b1de-e7250504d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_com_base_s3 = \"s3://oedi-data-lake/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2024/\"\n",
    "meta_com_rep_s3 = \"comstock_amy2018_release_1/metadata_and_annual_results/national/csv/\"\n",
    "meta_com_f = \"baseline_metadata_only.csv\"\n",
    "s3_path = meta_com_base_s3 + meta_com_rep_s3 + meta_com_f\n",
    "local_meta_path = f\"{input_dir}/external/baseline_metadata_com.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c5c1b-5c9a-423b-b668-4a36573aa34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"aws s3 cp {s3_path} {local_meta_path} --no-sign-request\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a6570-5d8a-4bcc-94e4-e6eb6ff8c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ext = pd.read_csv(local_meta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94970394-ac71-4ed5-803a-20a792016458",
   "metadata": {},
   "source": [
    "We select the subdata of all the external data to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db78648-2568-43ac-b383-6973fe84f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_cols =['in.comstock_building_type_group',\n",
    " 'in.heating_fuel',\n",
    " 'in.hvac_category',\n",
    " 'in.number_of_stories',\n",
    " 'in.ownership_type',\n",
    " 'in.vintage',\n",
    " 'in.wall_construction_type',\n",
    " 'in.tstat_clg_sp_f..f',\n",
    " 'in.tstat_htg_sp_f..f',\n",
    " 'in.weekday_opening_time..hr',\n",
    " 'in.weekday_operating_hours..hr']\n",
    "res_cols=['in.bedrooms',\n",
    " 'in.cooling_setpoint',\n",
    " 'in.heating_setpoint',\n",
    " 'in.geometry_building_type_recs',\n",
    " 'in.geometry_floor_area',\n",
    " 'in.geometry_foundation_type',\n",
    " 'in.geometry_wall_type',\n",
    " 'in.heating_fuel',\n",
    " 'in.income',\n",
    " 'in.roof_material',\n",
    " 'in.tenure',\n",
    " 'in.vacancy_status',\n",
    " 'in.vintage']\n",
    "cons_cols=['bldg_id','in.state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66dbdc-68b5-4a2d-b8ed-bdefcbcf1a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAST_ROW_BLDG_ID = 47477 #define the last row do include for the submitted version\n",
    "all_cols = cons_cols + com_cols\n",
    "dfcom  = df_ext[all_cols]\n",
    "last_idx = dfcom.loc[dfcom.bldg_id==LAST_ROW_BLDG_ID].index[0]\n",
    "dfcom = dfcom.iloc[:last_idx+1]\n",
    "dfcom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d0f9b4-0bfd-4465-9700-262c2e600e58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to download a single file from S3\n",
    "def download_s3_file(row,base_rep_s3=\"comstock_amy2018_release_1\",target_type=\"com\"):\n",
    "    \"\"\"\n",
    "    Downloads the external data for a given building ID.\n",
    "    \n",
    "    Args:\n",
    "        row (int): The building row data.\n",
    "        base_rep_s3 (str): base repository name in s3 link\n",
    "        target_type (str) : the target type (com/res)\n",
    "\n",
    "    Returns:\n",
    "        None: Data is downloaded and saved locally.\n",
    "    \"\"\"\n",
    "    state = row['in.state']\n",
    "    bldg_id = row['bldg_id']\n",
    "    state_path = f\"state={state}/\"\n",
    "    file_name = f\"{bldg_id}-0.parquet\"\n",
    "    base_s3 = \"s3://oedi-data-lake/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2024/\"\n",
    "    rep_s3 = f\"{base_rep_s3}/timeseries_individual_buildings/by_state/upgrade=0/\"\n",
    "    s3_path = os.path.join(base_s3, rep_s3, state_path, file_name)\n",
    "    \n",
    "   \n",
    "    local_dir = f\"{input_dir}/external/features/{target_type}/{state}\"\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    local_file = os.path.join(local_dir, file_name)\n",
    "    # AWS S3 copy command\n",
    "    os.system(f\"aws s3 cp {s3_path} {local_file} --no-sign-request\")\n",
    "\n",
    "# Parallelize the download\n",
    "def parallel_download(df,base_rep_s3=\"comstock_amy2018_release_1\",target_type=\"com\", max_workers=10):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(download_s3_file, row,base_rep_s3,target_type) for idx, row in df.iterrows()]\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6821fe-fea5-499e-b585-ecf4584e0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_download(dfcom,base_rep_s3=\"comstock_amy2018_release_1\",target_type=\"com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fa20a5-3959-4649-bfb1-1657bfcd64f8",
   "metadata": {},
   "source": [
    "### 2.b. Download external data for residential buildings\n",
    "This section reproduces the downloading process of external data for residential buildings from ***Open Energy Data Initiative (OEDI)***. The download is done in parallel  and ends at bldg_id =  36600 which is used in the final submission.\n",
    "\n",
    "We first download the  metadata file from s3 path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f19b8-e58e-4178-b44e-23324d06c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_res_base_s3 = \"s3://oedi-data-lake/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2024/\"\n",
    "meta_res_rep_s3 = \"resstock_amy2018_release_2/metadata_and_annual_results/national/csv/\"\n",
    "meta_res_f = \"baseline_metadata_only.csv\"\n",
    "s3_path = meta_res_base_s3 + meta_res_rep_s3 + meta_res_f\n",
    "local_meta_path = f\"{input_dir}/external/baseline_metadata_res.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2394aa-b776-428b-a82a-c830f89b496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"aws s3 cp {s3_path} {local_meta_path} --no-sign-request\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76649848-24fd-4c3e-ae15-05c477d07c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ext = pd.read_csv(local_meta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac928722-b631-42f7-86e3-aa787ef034d8",
   "metadata": {},
   "source": [
    "We select the subdata of all the external data to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83646fe-71d4-4285-8962-d394954b603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAST_ROW_BLDG_ID = 36600 #define the last row do include\n",
    "all_cols = cons_cols + res_cols\n",
    "dfres = df_ext[all_cols]\n",
    "last_idx = dfres.loc[dfres.bldg_id==LAST_ROW_BLDG_ID].index[0]\n",
    "dfres = dfres.iloc[:last_idx+1]\n",
    "dfres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a290ae9-a0ad-436e-90ed-1cc871019f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_download(dfres,base_rep_s3=\"resstock_amy2018_release_2\",target_type=\"res\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594140b-69f7-49f7-a40a-e26edda35724",
   "metadata": {},
   "source": [
    "### 2.c. Prepare External Data\n",
    "In this section, we process the external dataset by removing rows with target values that do not appear in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2892146a-e686-4b5e-8377-7202218cccd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filepath_com_labels = f'{input_dir}/external/baseline_metadata_com.csv'#path to the train label file\n",
    "dfcom = pd.read_csv(filepath_com_labels)\n",
    "filepath_res_labels = f'{input_dir}/external/baseline_metadata_res.csv'#path to the train label file\n",
    "dfres = pd.read_csv(filepath_res_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6139b-5d35-4882-a32f-3a9891e31ca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resfiles = glob(f\"{input_dir}/external/features/res/*/*-0.parquet\")\n",
    "comfiles = glob(f\"{input_dir}/external/features/com/*/*-0.parquet\")\n",
    "bldg_com=[int(os.path.basename(f).replace(\"-0.parquet\",\"\")) for f in comfiles]\n",
    "bldg_res=[int(os.path.basename(f).replace(\"-0.parquet\",\"\")) for f in resfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bd778b-9bdb-45c2-9b32-21ba9cdfcb96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfc = dfcom[dfcom['bldg_id'].isin(bldg_com)]\n",
    "dfr=dfres[dfres['bldg_id'].isin(bldg_res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b4f5f-06b0-411f-b7dd-6d1bbcf96bb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfc=dfc[cons_cols+com_cols].rename({col:col+\"_com\" for col in com_cols},axis=1)\n",
    "dfc.insert(0, 'building_stock_type', \"commercial\")\n",
    "dfc[\"in.weekday_opening_time..hr_com\"]=dfc[\"in.weekday_opening_time..hr_com\"].astype(int)\n",
    "dfc[\"in.weekday_operating_hours..hr_com\"]=dfc[\"in.weekday_opening_time..hr_com\"].astype(int)\n",
    "for col in com_cols:\n",
    "    with open(f'{input_dir}/Label_encoded/{col}_com_label_encoder.pkl', 'rb') as f:\n",
    "        le = pickle.load(f)\n",
    "    dfc=dfc[dfc[col+\"_com\"].astype(str).isin(list(le.classes_))]\n",
    "dfc=dfc.astype(object)\n",
    "dfc.to_parquet(f\"{input_dir}/labels/external_com.parquet\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec4048-c966-4ef1-9a7b-fd22c52d21d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfr=dfr[cons_cols+res_cols].rename({col:col+\"_res\" for col in res_cols},axis=1)\n",
    "dfr.insert(0, 'building_stock_type',\"residential\")\n",
    "for col in res_cols:\n",
    "    with open(f'{input_dir}/Label_encoded/{col}_res_label_encoder.pkl', 'rb') as f:\n",
    "        le = pickle.load(f)\n",
    "    dfr=dfr[dfr[col+\"_res\"].astype(str).isin(list(le.classes_))]\n",
    "dfr=dfr.astype(object)\n",
    "dfr.to_parquet(f\"{input_dir}/labels/external_res.parquet\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9e3c6",
   "metadata": {},
   "source": [
    "### 3. Training models\n",
    "<img src=\"assets/training_pipeline.png\" alt=\"Training Model Diagram\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd4c2c-30ac-411e-bf5d-c90ec5de444d",
   "metadata": {},
   "source": [
    "### 3.a Create Multilabel Folds\n",
    "- Inside the `src` folder run the command `python prepare_data.py` to create Multilabel Stratified KFold (with K=10)  for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f979f4-b0d9-49ec-a332-334322aa3da1",
   "metadata": {},
   "source": [
    "### 3.b. Trainning\n",
    "- Inside the `src` folder run the shell script `train_effnet.sh` to train all the pipeline for the  `tf_efficientnetv2_s_in21k` backbone.\n",
    "- Inside the `src` folder run the shell script `train_seresnext.sh` to train all the pipeline for the  `seresnext26t_32x4d` backbone.\n",
    "\n",
    "\n",
    "### 4. Inference and Submission\n",
    "- Inside the `src` folder run `infer.py` to perform inference and generate the submission file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b90991",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
