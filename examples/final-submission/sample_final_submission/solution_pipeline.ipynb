{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98bb7530",
   "metadata": {},
   "source": [
    "# Solution Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c84f48",
   "metadata": {},
   "source": [
    "### Data Preprocessing / Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff9a14",
   "metadata": {},
   "source": [
    "In general, our approach does not involve any additional feature engineering steps. However, to fit the model requirements and prevent model overfitting we resort to a few traditional data preprocessing and augmentation techniques by applying image resizing, normalization, blur, horizontal flips etc. \n",
    "\n",
    "Training dataset has been constructed by slicing original seismic volumes along one of the dimensions. Therefore, the model is designed to be trained on 2-D data. After inference (for test/holdout data), the 2-D predictions are stucked up back into 3-D volumes. \n",
    "\n",
    "*CAVEAT: such approach is not optimal, as seismic layers are 3 dimensional objects, so the 2-D model won't be able to decently segment 3-D objects. This approach serves as a sample pipeline only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aaa61b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model description\n",
    "\n",
    "As the main model, we use [UNET architecture](https://arxiv.org/abs/1505.04597) with 'resnet34' encoder pretrained on ['imagenet' dataset](https://www.image-net.org/about.php)  as such configuration proved to be a good starting point for further model training on domain specific data. \n",
    "\n",
    "![UNET model schema](images/UNET.png \"UNET with 'resnet34' encoder\")\n",
    "\n",
    "The image is taken from [here](https://www.researchgate.net/publication/350858002_Deeply_Supervised_UNet_for_Semantic_Segmentation_to_Assist_Dermatopathological_Assessment_of_Basal_Cell_Carcinoma).\n",
    "\n",
    "\n",
    "To automate and manage the model pipeline conveniently we use [Pytorch Lighning](https://lightning.ai/docs/pytorch/stable/) framework. \n",
    "\n",
    "\n",
    "With the current pipeline settings, the training process took about 10 hours (with 1 GPU (24 GB) available)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa46a21",
   "metadata": {},
   "source": [
    "# Solution Reproduction Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde4804",
   "metadata": {},
   "source": [
    "## 1. Pipeline Configuration\n",
    "\n",
    "Before running the pipeline, please navigate to the ```configs/config.yaml``` file and specify the actual data paths to let the pipeline know about data location. No other changes in the config file are required. \n",
    "\n",
    "CONFIGURATION CAVEAT:\n",
    "\n",
    "* If the evaluation instance has more than 1 GPU, feel free to adjust the ```gpus``` parameter file to speed up the training process;\n",
    "\n",
    "* If you encounter GPU memory problems, feel free to decrease ```batch_size``` parameter;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecaebf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Environment Setup\n",
    "Please, run the following command to install all needed libraries and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9fd434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f32b49",
   "metadata": {},
   "source": [
    "## 3. Training step\n",
    "\n",
    "Download the model weights from [Hugging Face](https://huggingface.co/thinkonward/challenges/tree/final-submission) before you get started. After you have downloaded them and put the `./checkpoints` directory in the root directory (`sample_final_submission`) proceed with the following instructions.\n",
    "\n",
    "You can skip this step if you want to start with the pretrained checkpoints provided in ```./checkpoints/best```.\n",
    "Otherwise, uncomment and run the following command to trigger the model training script. \n",
    "\n",
    "The checkpoints will be saved to ```./checkpoints``` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e33d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python src/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93865808",
   "metadata": {},
   "source": [
    "## 4. Inference step\n",
    "To inference the model and form a predictions for holdout dataset please follow the instructions below. \n",
    "\n",
    "*INFERENCE CONFIGURATION CAVEAT:  \n",
    "* Feel free to manage the input/output directories used for inference through ```inference_input_data_dir``` and ```inference_output_data_dir``` parameters in ```configs/config.yaml```;\n",
    "* In order to change model checkpoints used for inference adjust the ```CHECKPOINT_WEIGHTS``` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d15f6357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from glob import glob\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.train import SegmentationModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9b94f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import config\n",
    "config_path = \"configs/config.yaml\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4595b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SegmentationModule(config, mode=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4770351c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path to model checkpoint\n",
    "CHECKPOINT_WEIGHTS = \"checkpoints/best/last.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ab0a481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model weights\n",
    "state_dict = torch.load(CHECKPOINT_WEIGHTS)[\"state_dict\"]\n",
    "fixed_state_dict = {\n",
    "    key.replace(\"model.\", \"\"): value for key, value in state_dict.items()\n",
    "}\n",
    "\n",
    "model.load_state_dict(fixed_state_dict)\n",
    "model = model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a479edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data_dir = config.get(\"data_paths\")[\"inference_input_data_dir\"]\n",
    "# get paths to all inference volumes\n",
    "inf_cube_paths = glob(os.path.join(inference_data_dir, \"test_vol_*.npy\"))\n",
    "inf_cube_paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a0bf41f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set of transformations applied to the inference data\n",
    "inference_transform = A.Compose(\n",
    "    [\n",
    "        A.Lambda(image=lambda img, **kwargs: img.astype(np.float32) / 255.0),\n",
    "        A.Resize(128, 320),\n",
    "        A.Normalize(mean=(0.485,), std=(0.229,), max_pixel_value=1.0),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80fa5edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:10<00:21, 10.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/predictions/sub_vol_1.npy has been created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:21<00:10, 10.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/predictions/sub_vol_2.npy has been created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:31<00:00, 10.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/predictions/sub_vol_3.npy has been created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inference_output_dir = config.get(\"data_paths\")[\"inference_output_data_dir\"]\n",
    "target_shape = (300, 300, 100)\n",
    "\n",
    "# for each inference volume\n",
    "for cube_path in tqdm(inf_cube_paths):\n",
    "    pred_cube = np.zeros(target_shape)\n",
    "    vol = np.load(cube_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    n_slices = target_shape[0]\n",
    "    # for each slice in an inference volume\n",
    "    for idx in range(n_slices):\n",
    "        slice = vol[idx]\n",
    "        # preprocess\n",
    "        transformed = inference_transform(image=slice.T)\n",
    "        image = transformed[\"image\"]\n",
    "        image = torch.unsqueeze(image, 0)\n",
    "        image = image.cuda()\n",
    "        # get predictions\n",
    "        pred_image = model(image)\n",
    "        pred_image = pred_image.log_softmax(dim=1).exp()\n",
    "        # post process\n",
    "        pred_image = pred_image.squeeze(0).detach().cpu().numpy()\n",
    "        pr = np.array(pred_image, dtype=\"float32\")\n",
    "        new_image = np.argmax(pr, axis=0)  # shape 128x320\n",
    "        new_image = new_image.T  # shape 320 x 128\n",
    "\n",
    "        new_height = target_shape[1]\n",
    "        new_width = target_shape[2]\n",
    "        resized_image = ndimage.zoom(\n",
    "            new_image,\n",
    "            (new_height / new_image.shape[0], new_width / new_image.shape[1]),\n",
    "            order=1,\n",
    "        )\n",
    "        # assign predicted slice to prediction volume\n",
    "        pred_cube[idx] = resized_image\n",
    "\n",
    "    # save prediction volume\n",
    "    cube_basename = os.path.basename(cube_path)\n",
    "    pred_basename = cube_basename.replace(\"test\", \"sub\")\n",
    "    pred_path = os.path.join(inference_output_dir, pred_basename)\n",
    "    np.save(pred_path, pred_cube, allow_pickle=True)\n",
    "    print(f\"{pred_path} has been created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ec116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
